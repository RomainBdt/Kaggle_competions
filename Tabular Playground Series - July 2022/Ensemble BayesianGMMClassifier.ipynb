{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from joblib import dump, load\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98000, 14)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_data = pd.read_csv('../input/tabular-playground-series-jul-2022/data.csv')\n",
    "raw_data = pd.read_csv('data\\data.csv')\n",
    "df = raw_data.drop(columns = 'id')\n",
    "\n",
    "cols = [F'f_{i:02d}' for i in list(range(7,14))+list(range(22,29))]  # LGBM confirms that choice with feature_importance_\n",
    "df = df[cols]\n",
    "\n",
    "df_scaled = pd.DataFrame(PowerTransformer().fit_transform(df), columns=cols)\n",
    "df_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklego BayesianGMMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEGO BayesianGMMClassifier\n",
    "# This a a copy of the BayesianGMMClassifier from SKLEGO to avoid the need to install the package\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.utils.validation import check_is_fitted, check_array, FLOAT_DTYPES\n",
    "\n",
    "\n",
    "class BayesianGMMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_components=1,\n",
    "        covariance_type=\"full\",\n",
    "        tol=0.001,\n",
    "        reg_covar=1e-06,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        init_params=\"kmeans\",\n",
    "        weight_concentration_prior_type=\"dirichlet_process\",\n",
    "        weight_concentration_prior=None,\n",
    "        mean_precision_prior=None,\n",
    "        mean_prior=None,\n",
    "        degrees_of_freedom_prior=None,\n",
    "        covariance_prior=None,\n",
    "        random_state=None,\n",
    "        warm_start=False,\n",
    "        verbose=0,\n",
    "        verbose_interval=10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The BayesianGMMClassifier trains a Gaussian Mixture Model for each class in y on a dataset X. Once\n",
    "        a density is trained for each class we can evaluate the likelihood scores to see which class\n",
    "        is more likely. All parameters of the model are an exact copy of the parameters in scikit-learn.\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.covariance_type = covariance_type\n",
    "        self.tol = tol\n",
    "        self.reg_covar = reg_covar\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.init_params = init_params\n",
    "        self.weight_concentration_prior_type = weight_concentration_prior_type\n",
    "        self.weight_concentration_prior = weight_concentration_prior\n",
    "        self.mean_precision_prior = mean_precision_prior\n",
    "        self.mean_prior = mean_prior\n",
    "        self.degrees_of_freedom_prior = degrees_of_freedom_prior\n",
    "        self.covariance_prior = covariance_prior\n",
    "        self.random_state = random_state\n",
    "        self.warm_start = warm_start\n",
    "        self.verbose = verbose\n",
    "        self.verbose_interval = verbose_interval\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> \"BayesianGMMClassifier\":\n",
    "        \"\"\"\n",
    "        Fit the model using X, y as training data.\n",
    "\n",
    "        :param X: array-like, shape=(n_columns, n_samples, ) training data.\n",
    "        :param y: array-like, shape=(n_samples, ) training data.\n",
    "        :return: Returns an instance of self.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, estimator=self, dtype=FLOAT_DTYPES)\n",
    "        if X.ndim == 1:\n",
    "            X = np.expand_dims(X, 1)\n",
    "\n",
    "        self.gmms_ = {}\n",
    "        self.classes_ = unique_labels(y)\n",
    "        for c in self.classes_:\n",
    "            subset_x, subset_y = X[y == c], y[y == c]\n",
    "            mixture = BayesianGaussianMixture(\n",
    "                n_components=self.n_components,\n",
    "                covariance_type=self.covariance_type,\n",
    "                tol=self.tol,\n",
    "                reg_covar=self.reg_covar,\n",
    "                max_iter=self.max_iter,\n",
    "                n_init=self.n_init,\n",
    "                init_params=self.init_params,\n",
    "                weight_concentration_prior_type=self.weight_concentration_prior_type,\n",
    "                weight_concentration_prior=self.weight_concentration_prior,\n",
    "                mean_precision_prior=self.mean_precision_prior,\n",
    "                mean_prior=self.mean_prior,\n",
    "                degrees_of_freedom_prior=self.degrees_of_freedom_prior,\n",
    "                covariance_prior=self.covariance_prior,\n",
    "                random_state=self.random_state,\n",
    "                warm_start=self.warm_start,\n",
    "                verbose=self.verbose,\n",
    "                verbose_interval=self.verbose_interval,\n",
    "            )\n",
    "            self.gmms_[c] = mixture.fit(subset_x, subset_y)\n",
    "            if VERBOSE > 1:\n",
    "                print(f'Weights of model {c}: {np.sort(np.round(mixture.weights_, 2))}')\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, [\"gmms_\", \"classes_\"])\n",
    "        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n",
    "        return self.classes_[self.predict_proba(X).argmax(axis=1)]\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES)\n",
    "        check_is_fitted(self, [\"gmms_\", \"classes_\"])\n",
    "        res = np.zeros((X.shape[0], self.classes_.shape[0]))\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            res[:, idx] = self.gmms_[c].score_samples(X)\n",
    "        return softmax(res, axis=1)\n",
    "    \n",
    "    \n",
    "    def get_densities(self, X):\n",
    "        densities = np.zeros(np.shape(X)[0])\n",
    "        for c in self.classes_:\n",
    "            densities += self.gmms_[c].score_samples(X)\n",
    "        return densities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(X, seed=0):\n",
    "\n",
    "\n",
    "    X_wo_outliers, y = remove_outliers(X, seed)\n",
    "    \n",
    "    predict_proba, _ = update_predictions(X, y, X_wo_outliers, seed)\n",
    "    \n",
    "    # Final prediction with full dataset, more n_init and less tol\n",
    "    if RUN_FINAL_PRED:\n",
    "        y = np.argmax(predict_proba, axis=1)\n",
    "        y = pd.Series(y)\n",
    "        predict_proba = final_prediction(X, y, X_wo_outliers, seed)\n",
    "    \n",
    "    return predict_proba\n",
    "    \n",
    "    \n",
    "def update_predictions(X, y, X_wo_outliers=None, seed=0):\n",
    "    rand_score_prev = 0\n",
    "#     densities = np.zeros(np.shape(y))\n",
    "    value_counts = pd.DataFrame(data=np.zeros((len(set(y)), 2)), columns=['count', 'diff'])\n",
    "    for i in range(N_GMMC_ITERATION):\n",
    "        if X_wo_outliers is not None:\n",
    "            X_sample = X_wo_outliers.sample(n=N_SAMPLE, random_state = seed+i)  # None, seed + i\n",
    "        else:\n",
    "            X_sample = X.sample(n=N_SAMPLE, random_state = seed+i)  # None, seed + i\n",
    "        y_sample = y.loc[X_sample.index]\n",
    "        \n",
    "        bgmmC = BayesianGMMClassifier(\n",
    "            n_components = N_CLUSTER, # 1, 6 or N_CLUSTER  !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            random_state = seed + i,  # None, seed + i\n",
    "            tol = TOL,\n",
    "            covariance_type = 'full',\n",
    "            max_iter = GMM_MAX_ITER,\n",
    "            n_init=N_INIT,\n",
    "            init_params=INIT_PARAMS)\n",
    "\n",
    "        bgmmC.fit(X_sample, y_sample)        \n",
    "        pred_probs = bgmmC.predict_proba(np.array(X))        \n",
    "        rand_score = adjusted_rand_score(y, np.argmax(pred_probs, axis=1))\n",
    "        \n",
    "        if VERBOSE > 0:\n",
    "            pct_of_change = 1 - sum(y == np.argmax(pred_probs, axis=1)) / X.shape[0]            \n",
    "            print('Iter_{} - Pct of change from last iteration: {:.4f}, rand score: {:.4f}'.format(i, pct_of_change, rand_score))\n",
    "        y = pd.Series(np.argmax(pred_probs, axis=1))\n",
    "        \n",
    "        if VERBOSE > 1:\n",
    "            y_prev = value_counts['count'].copy()\n",
    "            value_counts['count'] = y.value_counts().sort_index()\n",
    "            value_counts['diff'] = value_counts['count'] - y_prev\n",
    "            print(value_counts)\n",
    "        \n",
    "        # stop iteration if the rand score decreases and return results of previous iteration\n",
    "        if rand_score_prev > rand_score and ACTIVATE_EARLY_STOPPING:\n",
    "            break\n",
    "        else:\n",
    "            pred_probs_final = pred_probs\n",
    "            rand_score_prev = rand_score\n",
    "#             densities += bgmmC.get_densities(np.array(X))\n",
    "            densities = bgmmC.get_densities(np.array(X))\n",
    "        \n",
    "    return pred_probs_final, densities\n",
    "            \n",
    "            \n",
    "def final_prediction(X, y, X_wo_outliers=None, seed=0):\n",
    "    \"\"\"Make the final prediction with the full dataset, low tol, high init\"\"\"\n",
    "    \n",
    "    if X_wo_outliers is not None:\n",
    "        X_sample = X_wo_outliers.copy()\n",
    "    else:\n",
    "        X_sample = X.copy()\n",
    "    y_sample = y.loc[X_sample.index]\n",
    "\n",
    "    bgmmC = BayesianGMMClassifier(\n",
    "        n_components=N_CLUSTER,\n",
    "        random_state = seed,\n",
    "        tol = 0.01,\n",
    "        covariance_type = 'full',\n",
    "        max_iter = GMM_MAX_ITER,\n",
    "        n_init=5,\n",
    "        init_params=INIT_PARAMS)\n",
    "\n",
    "    bgmmC.fit(X_sample, y_sample)        \n",
    "    pred_probs = bgmmC.predict_proba(X)\n",
    "\n",
    "    \n",
    "    if VERBOSE > 0:\n",
    "        pct_of_change = 1 - sum(y == np.argmax(pred_probs, axis=1)) / X.shape[0]\n",
    "        rand_score = adjusted_rand_score(y, np.argmax(pred_probs, axis=1))\n",
    "        print('Final pred - Pct of change from last iteration: {:.4f}, rand score: {:.4f}'.format(pct_of_change, rand_score))\n",
    "        \n",
    "    return pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "def soft_voting(proba_list):\n",
    "    # Type here the prediction you may want to keep for the ensembling\n",
    "    n_models = np.shape(proba_list)[0]  # nb of Models\n",
    "    \n",
    "    prediction_to_drop = []\n",
    "    prediction_to_keep = [i for i in range(n_models) if i not in prediction_to_drop]\n",
    "    \n",
    "\n",
    "    values = [i for i in range(N_CLUSTER)]\n",
    "\n",
    "    ensemble_predict_proba = pd.DataFrame(np.zeros((np.shape(proba_list)[1], N_CLUSTER)), columns = values)\n",
    "\n",
    "    temp = pd.DataFrame()  # temp dataframe for reshaping and cleaning of predict_proba_list\n",
    "    # change predict_proba_list from shape (ITERATION, 98000, N_CLUSTER) to (98000, ITERATION * N_CLUSTER)\n",
    "    for i in range(0, n_models):\n",
    "        if i in prediction_to_keep:\n",
    "            temp = pd.concat([temp, pd.DataFrame(proba_list[i])], axis=1)\n",
    "\n",
    "    #  keep only rows with high probability value\n",
    "    temp = temp[temp.max(axis=1)>PROBA_THRESHOLD]\n",
    "\n",
    "    # replace high probabilities by 1, others by 0\n",
    "    temp[temp > PROBA_THRESHOLD] = 1\n",
    "    temp[temp < 1] = 0\n",
    "\n",
    "    # Keep only rows that have more than 90% of the models with high probability\n",
    "    temp = temp[temp.sum(axis=1) >= n_models * 0.9]\n",
    "\n",
    "    # Groupy of temp on all columns to find the pattern in the predictions\n",
    "    temp.columns = [i for i in range(temp.shape[1])]  # rename columns\n",
    "    mapping = temp.groupby(temp.columns.tolist(),as_index=False).size()\n",
    "\n",
    "    # Keep the N_CLUSTER biggest group\n",
    "    mapping = mapping.nlargest(n=N_CLUSTER, columns='size')\n",
    "    mapping.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    j = 0\n",
    "    # Renaming of the clusters\n",
    "    for i in range(n_models):\n",
    "        if i in prediction_to_keep:        \n",
    "            clusters_names = np.argmax(np.array(mapping.iloc[:, j*N_CLUSTER:(j+1)*N_CLUSTER]), axis=1)\n",
    "\n",
    "            # if one cluster is not renamed, it is affected to the remaining cluster\n",
    "            remaining_clusters = {}\n",
    "            if len(set(clusters_names)) < N_CLUSTER:\n",
    "                remaining_clusters = set(values).difference(set(clusters_names))\n",
    "                clusters_names[-1] = list(remaining_clusters)[0]\n",
    "            pred_dict = dict(zip(clusters_names, values))\n",
    "            \n",
    "            if VERBOSE > 0:\n",
    "                print(f'{pred_dict}  clusters clusters not found in pattern: {remaining_clusters}')\n",
    "\n",
    "            pred_temp = pd.DataFrame(proba_list[i]).rename(columns = pred_dict)\n",
    "            pred_temp = pred_temp.reindex(sorted(pred_temp.columns), axis=1)\n",
    "            ensemble_predict_proba += pred_temp # Soft voting by probabiliy addition        \n",
    "            j += 1\n",
    "\n",
    "    ensemble_predict_proba = ensemble_predict_proba / len(prediction_to_keep)\n",
    "    index_trusted = ensemble_predict_proba.max(axis=1) > TRUST_THRESHOLD\n",
    "    ensemble_predict = np.argmax(np.array(ensemble_predict_proba), axis=1)\n",
    "    if VERBOSE > 0:\n",
    "        print()\n",
    "        print(pd.DataFrame(ensemble_predict).value_counts())\n",
    "    # print(temp.groupby(temp.columns.tolist(),as_index=False).size().sort_values(by='size', ascending=False).head(20))\n",
    "    return ensemble_predict, index_trusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(X, seed=0):\n",
    "    if VERBOSE > 0:\n",
    "        print('====== Remove Outliers ======')\n",
    "    if OUTLIERS_THRESHOLD > 0:\n",
    "        X_sample = X.sample(n=N_SAMPLE, random_state = seed)\n",
    "        gmm = BayesianGaussianMixture(\n",
    "            n_components=N_CLUSTER, \n",
    "            covariance_type='full',\n",
    "            init_params=INIT_PARAMS,  # 'kmeans', 'k-means++', 'random', 'random_from_data'}\n",
    "            random_state = seed,\n",
    "            tol = TOL,\n",
    "            max_iter=GMM_MAX_ITER, \n",
    "            verbose=0, \n",
    "            verbose_interval=50, \n",
    "            n_init = N_INIT\n",
    "        )\n",
    "        gmm.fit(X_sample)\n",
    "        y = pd.Series(gmm.predict(X))    \n",
    "    \n",
    "        _, densities = update_predictions(X, y, seed=seed)\n",
    "        density_threshold = np.percentile(densities, OUTLIERS_THRESHOLD*100)\n",
    "        X_wo_outliers = X[densities > density_threshold]\n",
    "    return X_wo_outliers, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For test: remove_outliers\n",
    "# %%time\n",
    "# N_CLUSTER = 7\n",
    "# N_GMMC_ITERATION = 30\n",
    "# N_MODELS = 100\n",
    "# GMM_MAX_ITER = 300\n",
    "# N_SAMPLE = 20000\n",
    "# ACTIVATE_EARLY_STOPPING = True\n",
    "# TOL = 0.01\n",
    "# N_INIT = 3\n",
    "# PROBA_THRESHOLD = 0.99  # Rows with probability lower than this threshold are dropped\n",
    "# RUN_FINAL_PRED = True  # Make the final prediction with the full dataset, low tol, high init\n",
    "# EARLY_STOPPING_ENSEMBLE_RAND = 0.99\n",
    "# TRUST_THRESHOLD = 0.7\n",
    "# OUTLIERS_THRESHOLD = 0.01\n",
    "# VERBOSE = 2\n",
    "# seed = 0\n",
    "\n",
    "# df1 = df_scaled[:20000]\n",
    "# df2 = remove_outliers(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]C:\\Users\\hzf04d\\Anaconda3\\envs\\MainEnv\\lib\\site-packages\\sklearn\\mixture\\_base.py:286: ConvergenceWarning: Initialization 3 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 1.0000, rand score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                             | 1/30 [05:48<2:48:30, 348.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.8113, rand score: 0.8220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▎                                                                          | 2/30 [11:09<2:35:04, 332.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.3632, rand score: 0.8775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                        | 3/30 [16:46<2:30:33, 334.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0067, rand score: 0.9848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▋                                                                     | 4/30 [24:20<2:45:22, 381.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0045, rand score: 0.9897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▎                                                                  | 5/30 [30:02<2:33:02, 367.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0026, rand score: 0.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████                                                                | 6/30 [36:51<2:32:36, 381.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0019, rand score: 0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▋                                                             | 7/30 [42:59<2:24:36, 377.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0016, rand score: 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▎                                                          | 8/30 [49:15<2:18:04, 376.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0016, rand score: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████                                                        | 9/30 [55:52<2:14:02, 382.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0013, rand score: 0.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████▋                                                   | 10/30 [1:02:12<2:07:24, 382.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0015, rand score: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▏                                                | 11/30 [1:08:27<2:00:20, 380.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Ensemble ======\n",
      "Pct of ensemble prediction change from last iteration: 0.0008, rand score: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▏                                                | 11/30 [1:13:47<2:07:28, 402.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    16375\n",
      "2    16223\n",
      "5    14834\n",
      "1    13844\n",
      "0    13162\n",
      "6    12223\n",
      "4    11339\n",
      "dtype: int64\n",
      "Wall time: 1h 13min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N_CLUSTER = 7\n",
    "N_GMMC_ITERATION = 20\n",
    "N_MODELS = 30\n",
    "GMM_MAX_ITER = 300\n",
    "N_SAMPLE = 50000\n",
    "ACTIVATE_EARLY_STOPPING = True\n",
    "TOL = 0.01\n",
    "N_INIT = 3\n",
    "PROBA_THRESHOLD = 0.99  # Rows with probability lower than this threshold are dropped\n",
    "RUN_FINAL_PRED = True  # Make the final prediction with the full dataset, low tol, high init\n",
    "EARLY_STOPPING_ENSEMBLE_RAND = 0.998\n",
    "TRUST_THRESHOLD = 0.7\n",
    "OUTLIERS_THRESHOLD = 0.01\n",
    "VERBOSE = 0\n",
    "INIT_PARAMS='k-means++'  # 'kmeans', 'k-means++', 'random', 'random_from_data'}\n",
    "seed = 0\n",
    "\n",
    "X = df_scaled\n",
    "ensemble_predict_prev = -np.ones(X.shape[0])\n",
    "\n",
    "proba_list = []\n",
    "for _ in trange(N_MODELS):\n",
    "    proba = pipeline(X, seed=seed)\n",
    "    proba_list.append(proba)\n",
    "    print('====== Ensemble ======')\n",
    "    ensemble_predict, index_trusted = soft_voting(proba_list)\n",
    "    pct_of_change = 1 - sum(ensemble_predict == ensemble_predict_prev) / X.shape[0]\n",
    "    rand_score = adjusted_rand_score(ensemble_predict, ensemble_predict_prev)    \n",
    "    print('Pct of ensemble prediction change from last iteration: {:.4f}, rand score: {:.4f}'.format(pct_of_change, rand_score))\n",
    "    ensemble_predict_prev = ensemble_predict\n",
    "    seed += N_GMMC_ITERATION\n",
    "    time.sleep(1)  # wait 1 second for correct printing of trange\n",
    "    if rand_score > EARLY_STOPPING_ENSEMBLE_RAND:\n",
    "        break\n",
    "    \n",
    "print(pd.DataFrame(ensemble_predict).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('../input/tabular-playground-series-jul-2022/sample_submission.csv')\n",
    "sub = pd.read_csv('data\\sample_submission.csv')\n",
    "sub['Predicted'] = ensemble_predict\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MainEnv",
   "language": "python",
   "name": "mainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
