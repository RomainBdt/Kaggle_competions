{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>62.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.03</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.06</td>\n",
       "      <td>8.12</td>\n",
       "      <td>5.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>61.2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>61.6</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.70</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>62.6</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7.65</td>\n",
       "      <td>7.61</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220535</th>\n",
       "      <td>1.11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>62.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.52</td>\n",
       "      <td>4.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220536</th>\n",
       "      <td>0.33</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>61.9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220537</th>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>61.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.12</td>\n",
       "      <td>5.15</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220538</th>\n",
       "      <td>0.27</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>61.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220539</th>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220540 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        carat  cut  color  clarity  depth  table     x     y     z\n",
       "0        1.52    3      4        3   62.2   58.0  7.27  7.33  4.55\n",
       "1        2.03    2      0        1   62.0   58.0  8.06  8.12  5.05\n",
       "2        0.70    4      3        4   61.2   57.0  5.69  5.73  3.50\n",
       "3        0.32    4      3        4   61.6   56.0  4.38  4.41  2.71\n",
       "4        1.70    3      3        3   62.6   59.0  7.65  7.61  4.77\n",
       "...       ...  ...    ...      ...    ...    ...   ...   ...   ...\n",
       "220535   1.11    3      3        2   62.3   58.0  6.61  6.52  4.09\n",
       "220536   0.33    4      2        7   61.9   55.0  4.44  4.42  2.74\n",
       "220537   0.51    3      5        3   61.7   58.0  5.12  5.15  3.17\n",
       "220538   0.27    2      4        5   61.8   56.0  4.19  4.20  2.60\n",
       "220539   1.25    3      0        2   62.0   58.0  6.90  6.88  4.27\n",
       "\n",
       "[220540 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch, MeanShift, SpectralClustering, AffinityPropagation, FeatureAgglomeration\n",
    "\n",
    "# import regressors\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, StackingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, PassiveAggressiveRegressor, Perceptron, RidgeClassifier, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Lars, BayesianRidge, ARDRegression, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor, RANSACRegressor\n",
    "from sklearn.linear_model import LassoLars, LassoLarsIC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV, LarsCV, OrthogonalMatchingPursuitCV, LassoLarsCV, BayesianRidge, LinearRegression\n",
    "\n",
    "# pandas deactivate future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "SUBMIT = True\n",
    "USE_ORIGINAL = True\n",
    "SEED = 15\n",
    "SAMPLE = 1\n",
    "\n",
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')\n",
    "orig = pd.read_csv('datasets/cubic_zirconia.csv')\n",
    "\n",
    "for i, df in enumerate([train, test, orig]):\n",
    "    df.drop(['id'], axis=1, inplace=True)\n",
    "    if not SUBMIT:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    # df['dataset'] = i\n",
    "\n",
    "# Define test set\n",
    "if not SUBMIT:\n",
    "    train, test = train_test_split(train, test_size=0.2, random_state=SEED) \n",
    "\n",
    "if USE_ORIGINAL:\n",
    "    train = pd.concat([train, orig], axis=0)\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Sampling for faster training\n",
    "if SAMPLE < 1:\n",
    "    train = train.sample(frac=SAMPLE, random_state=SEED)\n",
    "\n",
    "del orig\n",
    "\n",
    "# set training data\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('price')\n",
    "X_test = test.copy()\n",
    "\n",
    "if not SUBMIT:\n",
    "    y_test = X_test.pop('price')\n",
    "else:\n",
    "    y_test = None\n",
    "    \n",
    "base_cols = X_train.columns\n",
    "\n",
    "# transform categorical features\n",
    "def transform_categorical(df):\n",
    "    df['cut'] = df['cut'].map({'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4})\n",
    "    df['color'] = df['color'].map({'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D': 6})\n",
    "    df['clarity'] = df['clarity'].map({'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7})\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Drop extreme values\n",
    "    min = 2\n",
    "    max = 20\n",
    "    df = df[(df['x'] < max) & (df['y'] < max) & (df['z'] < max)]\n",
    "    df = df[(df['x'] > min) & (df['y'] > min) & (df['z'] > min)]\n",
    "    return df\n",
    "\n",
    "def add_volume_ratio(df):\n",
    "    # df['volume_ratio1'] = (df['x'] * df['y']) / (df['z'] * df['z'])\n",
    "    df['volume_ratio2'] = (df['x'] * df['z']) / (df['y'] * df['y'])\n",
    "    df['volume_ratio3'] = (df['y'] * df['z']) / (df['x'] * df['x'])\n",
    "    # df['volume_ratio4'] = (df['x']) / (df['z'])\n",
    "    # df['volume_ratio5'] = (df['y']) / (df['z'])\n",
    "    df['volume_ratio6'] = (df['x'] * df['z']) / (df['y'] * df['z'])  # will set nan if z is nan\n",
    "    # df['volume_ratio7'] = (df['x'] + df['y']) / df['z']\n",
    "    # df['volume_ratio8'] = (df['x'] + df['z']) / df['y']\n",
    "    # df['volume_ratio9'] = (df['y'] + df['z']) / df['x']\n",
    "    # df['volume_ratio10'] = (df['x'] * df['y'] * df['z']) / (df['x'].mean() * df['y'].mean() * df['z'].mean())\n",
    "    # df['volume_ratio11'] = (df['x'] * df['y'] * df['z']) / (df['x'].max() * df['y'].max() * df['z'].max())\n",
    "    # df['volume_ratio12'] = (df['x'] * df['y'] * df['z']) / (df['x'].min() * df['y'].min() * df['z'].min())\n",
    "    # df['volume_ratio13'] = (df['x'] * df['y'] * df['z']) / (df['x'].median() * df['y'].median() * df['z'].median())\n",
    "    # df['volume_ratio14'] = (df['x'] * df['y'] * df['z']) / (df['x'].std() * df['y'].std() * df['z'].std())\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df[\"volume\"] = df[\"x\"] * df[\"y\"] * df[\"z\"]\n",
    "    df[\"surface_area\"] = 2 * (df[\"x\"] * df[\"y\"] + df[\"y\"] * df[\"z\"] + df[\"z\"] * df[\"x\"])\n",
    "    df[\"aspect_ratio_xy\"] = df[\"x\"] / df[\"y\"]\n",
    "    df[\"aspect_ratio_yz\"] = df[\"y\"] / df[\"z\"]\n",
    "    df[\"aspect_ratio_zx\"] = df[\"z\"] / df[\"x\"]\n",
    "    df[\"diagonal_distance\"] = np.sqrt(df[\"x\"] ** 2 + df[\"y\"] ** 2 + df[\"z\"] ** 2)\n",
    "    # df[\"relative_height\"] = (df[\"z\"] - df[\"z\"].min()) / (df[\"z\"].max() - df[\"z\"].min())\n",
    "    # df[\"relative_position\"] = (df[\"x\"] + df[\"y\"] + df[\"z\"]) / (df[\"x\"] + df[\"y\"] + df[\"z\"]).sum()\n",
    "    # df[\"volume_ratio\"] = df[\"x\"] * df[\"y\"] * df[\"z\"] / (df[\"x\"].mean() * df[\"y\"].mean() * df[\"z\"].mean())\n",
    "    # df[\"length_ratio\"] = df[\"x\"] / df[\"x\"].mean()\n",
    "    # df[\"width_ratio\"] = df[\"y\"] / df[\"y\"].mean()\n",
    "    # df[\"height_ratio\"] = df[\"z\"] / df[\"z\"].mean()\n",
    "    df[\"sphericity\"] = 1.4641 * (6 * df[\"volume\"])**(2/3) / df[\"surface_area\"]\n",
    "    df[\"compactness\"] = df[\"volume\"]**(1/3) / df[\"x\"]\n",
    "    df['density'] = df['carat'] / df['volume']\n",
    "    df['table_percentage'] = (df['table'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['depth_percentage'] = (df['depth'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['symmetry'] = (abs(df['x'] - df['z']) + abs(df['y'] - df['z'])) / (df['x'] + df['y'] + df['z'])\n",
    "    df['surface_area'] = 2 * ((df['x'] * df['y']) + (df['x'] * df['z']) + (df['y'] * df['z']))\n",
    "    df['depth_to_table_ratio'] = df['depth'] / df['table']\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def target_transform(serie):\n",
    "    serie = np.log1p(serie)\n",
    "    return serie\n",
    "\n",
    "def inverse_target_transform(serie):\n",
    "    serie = np.expm1(serie)\n",
    "    return serie\n",
    "\n",
    "def set_categorical(df):\n",
    "    df['cut'] = df['cut'].astype('category')\n",
    "    df['color'] = df['color'].astype('category')\n",
    "    df['clarity'] = df['clarity'].astype('category')\n",
    "    return df\n",
    "\n",
    "def add_girdle_parameters(df):\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def impute_x_y_z(df):\n",
    "    df['is_imputed'] = df.isna().any(axis=1).astype(int)\n",
    "    df['girdle_diameter'].fillna((df['x'] + df['y']) / 2, inplace=True)\n",
    "    df['x'].fillna(2*df['girdle_diameter'] - df['y'], inplace=True)\n",
    "    df['y'].fillna(2*df['girdle_diameter'] - df['x'], inplace=True)\n",
    "    df['z'].fillna(df['girdle_diameter'] * df['depth'] / 100, inplace=True)\n",
    "    df = add_girdle_parameters(df)\n",
    "    return df\n",
    "\n",
    "def set_nan(df):\n",
    "    for col in ['x', 'y', 'z']:\n",
    "        df[col].replace(0, np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def drop_girdle_parameters(df):\n",
    "    df.drop(['girdle_diameter', 'girdle_thickness', 'girdle_ratio'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Make data preparation pipeline\n",
    "def data_prepation(X_train, X_test):\n",
    "    \n",
    "    for df in [X_train, X_test]:\n",
    "        # df = set_nan(df)\n",
    "        df = transform_categorical(df)\n",
    "        # df = set_categorical(df)\n",
    "        # df = add_girdle_parameters(df)\n",
    "        # df = impute_x_y_z(df)\n",
    "        # df = drop_girdle_parameters(df)\n",
    "        \n",
    "    \n",
    "    # imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    # imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    # imputer = KNNImputer(n_neighbors=1, weights=\"uniform\")\n",
    "    # X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    # X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # selected_cols = base_cols\n",
    "    # selected_cols = ['surface_area', 'clarity', 'color', 'cut', 'carat', 'depth_percentage', 'depth', 'compactness', 'depth_to_table_ratio']\n",
    "    \n",
    "    # for df in [X_train, X_test]:\n",
    "    #     df = add_volume_ratio(df)\n",
    "        # df = feature_engineering(df)\n",
    "        \n",
    "        # df.fillna(0, inplace=True)\n",
    "        # df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        # df.dropna(inplace=True)\n",
    "        # df.drop([col for col in df.columns if col not in selected_cols], axis=1, inplace=True)\n",
    "        \n",
    "    # Scaling\n",
    "    # scaler = PowerTransformer()\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    # Clustering features\n",
    "    # model = KMeans(n_clusters=20, random_state=42)\n",
    "    # X_train['cluster'] = model.fit_predict(X_train)\n",
    "    # X_test['cluster'] = model.predict(X_test)\n",
    "    \n",
    "    return X_train, X_test\n",
    "            \n",
    "data_prep_has_fit_method = False\n",
    "\n",
    "if not data_prep_has_fit_method:\n",
    "    X_train, X_test = data_prepation(X_train, X_test)\n",
    "    X_train_prep, X_test_prep = X_train.copy(), X_test.copy()\n",
    "else:\n",
    "    X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "\n",
    "   \n",
    "# X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "# pd.DataFrame(X_train_prep.isna().sum(), columns=['train']).join(pd.DataFrame(X_test_prep.isna().sum(), columns=['test']))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor1: 574.1378 ± 4.1339, Time: 8.29 seconds, RMSE: 569.3603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:20,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor2: 730.0553 ± 2.9693, Time: 25.58 seconds, RMSE: 727.7157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:08,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor3: 574.4216 ± 4.3921, Time: 10.78 seconds, RMSE: 569.7514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor4: 798.0266 ± 7.5034, Time: 8.80 seconds, RMSE: 786.8051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor5: 576.5568 ± 5.0255, Time: 7.89 seconds, RMSE: 570.2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor6: 574.1378 ± 4.1339, Time: 7.92 seconds, RMSE: 569.3603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:05,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor7: 573.1275 ± 4.3175, Time: 7.24 seconds, RMSE: 569.0639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:05,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor8: 573.1275 ± 4.3175, Time: 7.32 seconds, RMSE: 569.0639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:20,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor9: 744.2118 ± 3.4723, Time: 25.31 seconds, RMSE: 740.0248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:13,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor10: 573.5038 ± 5.3067, Time: 18.17 seconds, RMSE: 568.3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:11,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor11: 571.9705 ± 5.0895, Time: 15.92 seconds, RMSE: 568.5716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:51, 10.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor1: 582.1524 ± 3.5743, Time: 64.25 seconds, RMSE: 578.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:32, 46.41s/it]"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Set categorical features for catboost\n",
    "cat_features = [col for col in X_train_prep.columns if X_train_prep[col].dtype == 'category']\n",
    "\n",
    "regressors = {\n",
    "    'LGBMRegressor1': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt'),\n",
    "    # 'LGBMRegressor2': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart'),\n",
    "    'LGBMRegressor3': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='goss'),\n",
    "    # 'LGBMRegressor4': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1),\n",
    "    'LGBMRegressor5': LGBMRegressor(random_state=SEED, n_jobs=-1, class_weight='balanced'),\n",
    "    'LGBMRegressor6': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    'LGBMRegressor7': LGBMRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    'LGBMRegressor8': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor9': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart', colsample_bytree=0.7),\n",
    "    'LGBMRegressor10': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240),\n",
    "    'LGBMRegressor11': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6),\n",
    "    'XGBRegressor1': XGBRegressor(random_state=SEED, n_jobs=-1),\n",
    "    'XGBRegressor2': XGBRegressor(random_state=SEED, n_jobs=-1, booster='dart'),\n",
    "    'XGBRegressor3': XGBRegressor(random_state=SEED, n_jobs=-1, booster='gblinear'),\n",
    "    'XGBRegressor4': XGBRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    'XGBRegressor5': XGBRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    'XGBRegressor6': XGBRegressor(random_state=SEED, \n",
    "                                  n_jobs=-1, \n",
    "                                  learning_rate=0.055, \n",
    "                                  n_estimators=200, \n",
    "                                  max_depth=8, \n",
    "                                  min_child_weight=1, \n",
    "                                  gamma=0.07, \n",
    "                                  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, \n",
    "                                  colsample_bynode=0.8,\n",
    "                                  subsample=0.7, \n",
    "                                  objective='reg:squarederror'),\n",
    "    'XGBRFRegressor6': XGBRegressor(random_state=SEED, n_jobs=-1, objective='reg:squarederror'),\n",
    "    'XGBRandomForestRegressor': XGBRFRegressor(random_state=SEED, n_jobs=-1),\n",
    "    'CatBoostRegressor': CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features), # Promising but fails on the cv\n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(random_state=SEED),\n",
    "    'HistGradientBoostingRegressor2': HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "                                                                    max_depth=6, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "                                                                    min_samples_leaf=9, max_bins=255),\n",
    "    # 'RandomForestRegressor': RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'ExtraTreesRegressor': ExtraTreesRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'AdaBoostRegressor': AdaBoostRegressor(random_state=SEED),\n",
    "    # 'GradientBoostingRegressor': GradientBoostingRegressor(random_state=SEED),\n",
    "    # 'BaggingRegressor': BaggingRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    # 'DecisionTreeRegressor': DecisionTreeRegressor(random_state=SEED),\n",
    "    # 'GaussianProcessRegressor': GaussianProcessRegressor(random_state=SEED),\n",
    "    # 'MLPRegressor1': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='adam'),\n",
    "    # 'MLPRegressor2': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='lbfgs'), # promising but long to train\n",
    "    # 'MLPRegressor3': MLPRegressor(random_state=SEED, max_iter=5000, activation='tanh', solver='adam'),\n",
    "    # 'MLPRegressor4': MLPRegressor(random_state=SEED, max_iter=1000, activation='tanh', solver='lbfgs'),  # promising but long to train\n",
    "    # 'MLPRegressor5': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='adam'),\n",
    "    # 'MLPRegressor6': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='lbfgs'),\n",
    "    # 'MLPRegressor7': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='adam'),\n",
    "    # 'MLPRegressor8': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='lbfgs'),\n",
    "    # 'Ridge': Ridge(random_state=SEED),\n",
    "    # 'SGDRegressor': SGDRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'PassiveAggressiveRegressor': PassiveAggressiveRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'Perceptron': Perceptron(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'LinearRegression': LinearRegression(),\n",
    "    # 'Lasso': Lasso(random_state=SEED),\n",
    "    # 'ElasticNet': ElasticNet(random_state=SEED, max_iter=1e6),\n",
    "    # 'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    # 'BayesianRidge': BayesianRidge(),\n",
    "    # 'ARDRegression': ARDRegression(),\n",
    "    # 'TheilSenRegressor': TheilSenRegressor(random_state=SEED),\n",
    "    # 'RANSACRegressor': RANSACRegressor(random_state=SEED),\n",
    "    # 'OrthogonalMatchingPursuit': OrthogonalMatchingPursuit(normalize=False),\n",
    "    # 'Lars': Lars(),\n",
    "    # 'LassoLars': LassoLars(),\n",
    "    # 'LassoLarsIC': LassoLarsIC(normalize=False),\n",
    "    # 'StackingRegressor': StackingRegressor(\n",
    "    #         estimators=[\n",
    "    #             ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, \n",
    "    #                                             max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    #             ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "    #                                         max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "    #                                         colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "    #                                         objective='reg:squarederror')),\n",
    "    #             ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features)), # Promising but fails on the cv\n",
    "    #             # ('ExtraTreesRegressor', ExtraTreesRegressor(random_state=SEED, n_jobs=-1))\n",
    "    #             ], \n",
    "    #         final_estimator=Ridge(random_state=SEED),\n",
    "    #         cv=cv,\n",
    "    #         # n_jobs=-1,\n",
    "    #         verbose=1\n",
    "    #         )\n",
    "}\n",
    "\n",
    "for model_name, regressor in regressors.items():\n",
    "    t0 = time.time()\n",
    "    scores = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "    # ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "    ttr = regressor\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(cv.split(X_train))):\n",
    "        \n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index].copy(), X_train.iloc[test_index].copy()\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "        \n",
    "        if data_prep_has_fit_method:\n",
    "            X_train_cv, X_test_cv = data_prepation(X_train_cv, X_test_cv)\n",
    "\n",
    "        # Models that need scaling and no missing value\n",
    "        if model_name in ['MLPRegressor1', 'MLPRegressor2', 'MLPRegressor3', 'MLPRegressor4', 'MLPRegressor5', 'MLPRegressor6', 'MLPRegressor7', 'MLPRegressor8', 'SGDRegressor', 'PassiveAggressiveRegressor', 'Perceptron', 'Ridge', 'Lasso', 'ElasticNet', 'HuberRegressor', 'BayesianRidge', 'ARDRegression', 'TheilSenRegressor', 'RANSACRegressor', 'OrthogonalMatchingPursuit', 'Lars', 'LassoLars', 'LassoLarsIC']:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_cv = pd.DataFrame(scaler.fit_transform(X_train_cv), columns=X_train_cv.columns)\n",
    "            X_test_cv = pd.DataFrame(scaler.transform(X_test_cv), columns=X_test_cv.columns)\n",
    "\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            X_train_cv = pd.DataFrame(imputer.fit_transform(X_train_cv), columns=X_train_cv.columns)\n",
    "            X_test_cv = pd.DataFrame(imputer.transform(X_test_cv), columns=X_test_cv.columns)\n",
    "\n",
    "            \n",
    "        ttr.fit(X_train_cv, y_train_cv)        \n",
    "        y_pred = ttr.predict(X_test_cv)\n",
    "        score_eval = mean_squared_error(y_test_cv, y_pred, squared=False)\n",
    "        scores.append(score_eval)\n",
    "        \n",
    "        try:\n",
    "            feature_importance = pd.Series(ttr.regressor_.feature_importances_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "        except:\n",
    "            try:\n",
    "                feature_importance = pd.Series(ttr.regressor_.coef_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "            except:\n",
    "                feature_importance = pd.Series(np.zeros(X_train_cv.shape[1]), index=X_train_cv.columns, name=f'fold{i}')\n",
    "        feature_importances = pd.concat([feature_importances, feature_importance], axis=1)\n",
    "    \n",
    "    feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "    \n",
    "    ttr.fit(X_train_prep, y_train)\n",
    "    y_pred = ttr.predict(X_test_prep)\n",
    "    \n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(f'{model_name}: {np.mean(scores):.4f} ± {np.std(scores):.4f}, Time: {time.time() - t0:.2f} seconds, RMSE: {score_eval:.4f}')\n",
    "    # print(feature_importances.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-568.0778 ± 5.3919\n"
     ]
    }
   ],
   "source": [
    "# Simple stacking with sklearn with target transformation\n",
    "regressors = [\n",
    "    ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                  max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                  objective='reg:squarederror')),\n",
    "    ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True)),\n",
    "    # ('HistGradientBoostingRegressor', HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "    #                                                                 max_depth=6, learning_rate=0.1, \n",
    "    #                                                                 l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "    #                                                                 min_samples_leaf=9, max_bins=255)),\n",
    "    ('HistGradientBoostingRegressor2', HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "                                                                    max_depth=10, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.1, max_leaf_nodes=100, \n",
    "                                                                    min_samples_leaf=20, max_bins=255)),\n",
    "    \n",
    "]\n",
    "\n",
    "model = StackingRegressor(\n",
    "    estimators=regressors,\n",
    "    final_estimator=LarsCV(cv=cv, max_iter=10000, n_jobs=-1),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    )\n",
    "\n",
    "y_train_transformed = target_transform(y_train)\n",
    "\n",
    "def scorer(estimator, X, y):\n",
    "    y_pred = inverse_target_transform(estimator.predict(X))\n",
    "    return -mean_squared_error(inverse_target_transform(y), y_pred, squared=False)\n",
    "\n",
    "scores = cross_val_score(model, X_train_prep, y_train_transformed, cv=cv, scoring=scorer, n_jobs=-1)\n",
    "print(f'{np.mean(scores):.4f} ± {np.std(scores):.4f}')\n",
    "\n",
    "model.fit(X_train_prep, y_train_transformed)\n",
    "y_pred_StackingRegressor = inverse_target_transform(model.predict(X_test_prep))\n",
    "\n",
    "# Save predictions\n",
    "sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "sub['price'] = y_pred_StackingRegressor\n",
    "now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "sub.to_csv(f'submissions/submission_StackingRegressor{now}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple stacking with sklearn without target transformation\n",
    "regressors = [\n",
    "    ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                  max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                  objective='reg:squarederror')),\n",
    "    ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True)),\n",
    "    # ('HistGradientBoostingRegressor', HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "    #                                                                 max_depth=6, learning_rate=0.1, \n",
    "    #                                                                 l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "    #                                                                 min_samples_leaf=9, max_bins=255)),\n",
    "    ('HistGradientBoostingRegressor2', HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "                                                                    max_depth=10, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.1, max_leaf_nodes=100, \n",
    "                                                                    min_samples_leaf=20, max_bins=255)),\n",
    "    \n",
    "]\n",
    "\n",
    "model = StackingRegressor(\n",
    "    estimators=regressors,\n",
    "    final_estimator=LarsCV(cv=cv, max_iter=10000, n_jobs=-1),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    )\n",
    "\n",
    "# y_train_transformed = target_transform(y_train)\n",
    "\n",
    "# def scorer(estimator, X, y):\n",
    "#     y_pred = inverse_target_transform(estimator.predict(X))\n",
    "#     return -mean_squared_error(inverse_target_transform(y), y_pred, squared=False)\n",
    "\n",
    "# scores = cross_val_score(model, X_train_prep, y_train, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "# print(f'{np.mean(scores):.4f} ± {np.std(scores):.4f}')\n",
    "\n",
    "model.fit(X_train_prep, y_train)\n",
    "y_pred_StackingRegressor = model.predict(X_test_prep)\n",
    "\n",
    "# Save predictions\n",
    "sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "sub['price'] = y_pred_StackingRegressor\n",
    "now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "sub.to_csv(f'submissions/submission_StackingRegressor{now}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "def plot_predictions_errors(y, y_pred, title):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        subsample=100,\n",
    "        ax=axs[0],\n",
    "        random_state=0,\n",
    "    )\n",
    "    axs[0].set_title(\"Actual vs. Predicted values\")\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"residual_vs_predicted\",\n",
    "        subsample=1000,\n",
    "        ax=axs[1],\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    axs[1].set_title(\"Residuals vs. Predicted Values\")\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 of 5\n",
      "Meta regressor: LinearRegression, RMSE hold out: 571.5068289823369, RMSE test: 563.9245, fit time: 0.08 s, Coefficients: [0.30345239 0.00375857 0.35595301 0.33712766], intercept: -0.0023031654882750274, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: RidgeCV, RMSE hold out: 571.4978771210448, RMSE test: 563.9050, fit time: 0.93 s, Coefficients: [0.30054188 0.01200578 0.35284242 0.33490373], intercept: -0.0023195370128066273, l1_ratio: nan, alpha: 1.0\n",
      "Meta regressor: ElasticNetCV, RMSE hold out: 571.7696412870171, RMSE test: 564.2040, fit time: 3.22 s, Coefficients: [0.33440853 0.1029093  0.35528347 0.2068054 ], intercept: 0.004606603356261729, l1_ratio: 0.99, alpha: 0.001\n",
      "Meta regressor: LassoCV, RMSE hold out: 571.7859091414609, RMSE test: 564.2231, fit time: 4.60 s, Coefficients: [0.34461589 0.0924156  0.35723719 0.20512615], intercept: 0.004698374180884812, l1_ratio: nan, alpha: 0.001\n",
      "Meta regressor: LarsCV, RMSE hold out: 571.5070290026416, RMSE test: 563.9247, fit time: 7.31 s, Coefficients: [0.30345249 0.00375756 0.35595328 0.33712781], intercept: -0.0022992788171158907, l1_ratio: nan, alpha: 5.11948959413458e-07\n",
      "Meta regressor: OrthogonalMatchingPursuitCV, RMSE hold out: 571.5127083006048, RMSE test: 563.9333, fit time: 8.14 s, Coefficients: [0.30535358 0.         0.35699406 0.33794207], intercept: -0.0022884419296360647, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: LassoLarsCV, RMSE hold out: 571.5070290026416, RMSE test: 563.9247, fit time: 8.89 s, Coefficients: [0.30345249 0.00375756 0.35595328 0.33712781], intercept: -0.0022992788171158907, l1_ratio: nan, alpha: 5.11948959413458e-07\n",
      "Meta regressor: BayesianRidge, RMSE hold out: 571.505625143445, RMSE test: 563.9220, fit time: 8.92 s, Coefficients: [0.30307365 0.00481561 0.35555666 0.336846  ], intercept: -0.0023053095466787354, l1_ratio: nan, alpha: 96.67147064499518\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 2 of 5\n",
      "Meta regressor: LinearRegression, RMSE hold out: 567.1820298073953, RMSE test: 562.7672, fit time: 0.09 s, Coefficients: [ 0.29088878 -0.02501941  0.3895361   0.34484827], intercept: -0.001968622083382776, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: RidgeCV, RMSE hold out: 567.1542246666902, RMSE test: 562.7415, fit time: 0.89 s, Coefficients: [ 0.28864785 -0.01624036  0.38550703  0.34234152], intercept: -0.0019857716140796455, l1_ratio: nan, alpha: 1.0\n",
      "Meta regressor: ElasticNetCV, RMSE hold out: 567.3825477992959, RMSE test: 563.2064, fit time: 3.30 s, Coefficients: [0.33410608 0.07382927 0.39188071 0.19954506], intercept: 0.004994767042133752, l1_ratio: 1.0, alpha: 0.001\n",
      "Meta regressor: LassoCV, RMSE hold out: 567.3825477992959, RMSE test: 563.2064, fit time: 4.70 s, Coefficients: [0.33410608 0.07382927 0.39188071 0.19954506], intercept: 0.004994767042133752, l1_ratio: nan, alpha: 0.001\n",
      "Meta regressor: LarsCV, RMSE hold out: 567.1007611714025, RMSE test: 562.7096, fit time: 4.98 s, Coefficients: [0.27844925 0.         0.38251475 0.33929395], intercept: -0.001999287913913683, l1_ratio: nan, alpha: 8.501007020866102e-06\n",
      "Meta regressor: OrthogonalMatchingPursuitCV, RMSE hold out: 567.097355037177, RMSE test: 562.7056, fit time: 5.22 s, Coefficients: [0.2784546  0.         0.38251549 0.3392961 ], intercept: -0.0020635748786608232, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: LassoLarsCV, RMSE hold out: 567.1007611714025, RMSE test: 562.7096, fit time: 5.50 s, Coefficients: [0.27844925 0.         0.38251475 0.33929395], intercept: -0.001999287913913683, l1_ratio: nan, alpha: 8.501007020866102e-06\n",
      "Meta regressor: BayesianRidge, RMSE hold out: 567.1786617799875, RMSE test: 562.7641, fit time: 5.53 s, Coefficients: [ 0.29061712 -0.02397238  0.38905756  0.34455173], intercept: -0.0019707098965984216, l1_ratio: nan, alpha: 96.95324939142074\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 3 of 5\n",
      "Meta regressor: LinearRegression, RMSE hold out: 569.7237697840658, RMSE test: 563.4681, fit time: 0.08 s, Coefficients: [0.28882517 0.00397716 0.37558983 0.33194636], intercept: -0.002663449963423048, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: RidgeCV, RMSE hold out: 569.695706465462, RMSE test: 563.4406, fit time: 0.91 s, Coefficients: [0.28678201 0.01187684 0.37194702 0.32973456], intercept: -0.0026778093178405626, l1_ratio: nan, alpha: 1.0\n",
      "Meta regressor: ElasticNetCV, RMSE hold out: 569.7497999613358, RMSE test: 563.5297, fit time: 3.18 s, Coefficients: [0.31996722 0.10989155 0.37330457 0.19628715], intercept: 0.004267095164649959, l1_ratio: 0.99, alpha: 0.001\n",
      "Meta regressor: LassoCV, RMSE hold out: 569.7901209237805, RMSE test: 563.5495, fit time: 4.59 s, Coefficients: [0.32874797 0.10038006 0.37588326 0.19442734], intercept: 0.004358973986375858, l1_ratio: nan, alpha: 0.001\n",
      "Meta regressor: LarsCV, RMSE hold out: 569.7245569578857, RMSE test: 563.4691, fit time: 4.86 s, Coefficients: [0.28882617 0.00397332 0.37559055 0.33194652], intercept: -0.0026481192239993234, l1_ratio: nan, alpha: 2.022165253535954e-06\n",
      "Meta regressor: OrthogonalMatchingPursuitCV, RMSE hold out: 569.7410955656692, RMSE test: 563.4807, fit time: 5.09 s, Coefficients: [0.29074422 0.         0.3767262  0.33286608], intercept: -0.002647893603898943, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: LassoLarsCV, RMSE hold out: 569.7245569578857, RMSE test: 563.4691, fit time: 5.36 s, Coefficients: [0.28882617 0.00397332 0.37559055 0.33194652], intercept: -0.0026481192239993234, l1_ratio: nan, alpha: 2.022165253535954e-06\n",
      "Meta regressor: BayesianRidge, RMSE hold out: 569.7201502006872, RMSE test: 563.4646, fit time: 5.39 s, Coefficients: [0.288562   0.00497978 0.37512931 0.33166768], intercept: -0.0026653192979146922, l1_ratio: nan, alpha: 96.82848873398251\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 4 of 5\n",
      "Meta regressor: LinearRegression, RMSE hold out: 571.1277449703501, RMSE test: 563.6487, fit time: 0.08 s, Coefficients: [0.28895904 0.00421076 0.40079122 0.30633965], intercept: -0.002335393774798078, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: RidgeCV, RMSE hold out: 571.0988853801271, RMSE test: 563.6190, fit time: 1.00 s, Coefficients: [0.28693091 0.01205496 0.39653604 0.30478072], intercept: -0.0023501661907827653, l1_ratio: nan, alpha: 1.0\n",
      "Meta regressor: ElasticNetCV, RMSE hold out: 571.538413464156, RMSE test: 563.8991, fit time: 3.24 s, Coefficients: [0.33935283 0.10095607 0.39233642 0.16675421], intercept: 0.004696875743326778, l1_ratio: 1.0, alpha: 0.001\n",
      "Meta regressor: LassoCV, RMSE hold out: 571.538413464156, RMSE test: 563.8991, fit time: 4.60 s, Coefficients: [0.33935283 0.10095607 0.39233642 0.16675421], intercept: 0.004696875743326778, l1_ratio: nan, alpha: 0.001\n",
      "Meta regressor: LarsCV, RMSE hold out: 571.1288423832465, RMSE test: 563.6499, fit time: 4.87 s, Coefficients: [0.28896    0.00420611 0.40079254 0.30633957], intercept: -0.0023163098486786993, l1_ratio: nan, alpha: 2.5217624017303025e-06\n",
      "Meta regressor: OrthogonalMatchingPursuitCV, RMSE hold out: 571.1424997669068, RMSE test: 563.6618, fit time: 5.10 s, Coefficients: [0.29101436 0.         0.40197096 0.30731325], intercept: -0.0023192834403102935, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: LassoLarsCV, RMSE hold out: 571.1288423832465, RMSE test: 563.6499, fit time: 5.36 s, Coefficients: [0.28896    0.00420611 0.40079254 0.30633957], intercept: -0.0023163098486786993, l1_ratio: nan, alpha: 2.5217624017303025e-06\n",
      "Meta regressor: BayesianRidge, RMSE hold out: 571.1240982597111, RMSE test: 563.6450, fit time: 5.38 s, Coefficients: [0.28870217 0.00518947 0.40026268 0.3061466 ], intercept: -0.0023372815033413374, l1_ratio: nan, alpha: 97.52704171106394\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 5 of 5\n",
      "Meta regressor: LinearRegression, RMSE hold out: 568.1949862774909, RMSE test: 563.0226, fit time: 0.07 s, Coefficients: [ 0.31462324 -0.02557612  0.37820732  0.33300705], intercept: -0.002010571052093013, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: RidgeCV, RMSE hold out: 568.1883201455158, RMSE test: 563.0148, fit time: 0.86 s, Coefficients: [ 0.3135667  -0.02277971  0.3771043   0.33237103], intercept: -0.0020168801470941133, l1_ratio: nan, alpha: 0.31622776601683794\n",
      "Meta regressor: ElasticNetCV, RMSE hold out: 568.6783608700187, RMSE test: 563.4561, fit time: 3.11 s, Coefficients: [0.35697401 0.06583688 0.3796974  0.19686759], intercept: 0.004901446638879925, l1_ratio: 1.0, alpha: 0.001\n",
      "Meta regressor: LassoCV, RMSE hold out: 568.6783608700187, RMSE test: 563.4561, fit time: 4.43 s, Coefficients: [0.35697401 0.06583688 0.3796974  0.19686759], intercept: 0.004901446638879925, l1_ratio: nan, alpha: 0.001\n",
      "Meta regressor: LarsCV, RMSE hold out: 568.1262720203695, RMSE test: 562.9579, fit time: 4.69 s, Coefficients: [0.30163991 0.         0.37132849 0.32730123], intercept: -0.0020723233828157106, l1_ratio: nan, alpha: 6.037334875239142e-06\n",
      "Meta regressor: OrthogonalMatchingPursuitCV, RMSE hold out: 568.1234521437061, RMSE test: 562.9549, fit time: 4.90 s, Coefficients: [0.3016439  0.         0.37133006 0.32730154], intercept: -0.002118011319629076, l1_ratio: nan, alpha: nan\n",
      "Meta regressor: LassoLarsCV, RMSE hold out: 568.1262720203695, RMSE test: 562.9579, fit time: 5.20 s, Coefficients: [0.30163991 0.         0.37132849 0.32730123], intercept: -0.0020723233828157106, l1_ratio: nan, alpha: 6.037334875239142e-06\n",
      "Meta regressor: BayesianRidge, RMSE hold out: 568.1924699446297, RMSE test: 563.0197, fit time: 5.22 s, Coefficients: [ 0.31422529 -0.02452631  0.37779385  0.33276896], intercept: -0.0020129489497762165, l1_ratio: nan, alpha: 95.84299753016982\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CV stacking: \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "regressors = [\n",
    "    ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                  max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                  objective='reg:squarederror')),\n",
    "    ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True)),\n",
    "    # ('HistGradientBoostingRegressor', HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "    #                                                                 max_depth=6, learning_rate=0.1, \n",
    "    #                                                                 l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "    #                                                                 min_samples_leaf=9, max_bins=255)),\n",
    "    ('HistGradientBoostingRegressor2', HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "                                                                    max_depth=10, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.1, max_leaf_nodes=100, \n",
    "                                                                    min_samples_leaf=20, max_bins=255)),\n",
    "    \n",
    "]\n",
    "\n",
    "meta_regressors = [\n",
    "    ('LinearRegression', LinearRegression()),\n",
    "    ('RidgeCV', RidgeCV(alphas=np.logspace(-3, 3, 13), cv=cv)),\n",
    "    ('ElasticNetCV', ElasticNetCV(alphas=np.logspace(-3, 3, 13), cv=cv, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], max_iter=10000)),\n",
    "    ('LassoCV', LassoCV(alphas=np.logspace(-3, 3, 13), cv=cv, max_iter=10000)),  # = ElasticNetCV with l1_ratio=1\n",
    "    ('LarsCV', LarsCV(cv=cv, max_iter=10000, n_jobs=-1)),\n",
    "    ('OrthogonalMatchingPursuitCV', OrthogonalMatchingPursuitCV(cv=cv, n_jobs=-1)),\n",
    "    ('LassoLarsCV', LassoLarsCV(cv=cv, max_iter=10000, n_jobs=-1)),\n",
    "    ('BayesianRidge', BayesianRidge()),\n",
    "]\n",
    "\n",
    "FIT_REGRESSORS = False\n",
    "DISPLAY_REGRESSOR_RESULTS = True\n",
    "PLOT_ERRORS = False\n",
    "\n",
    "if FIT_REGRESSORS:\n",
    "    # Store out of fold predictions for meta learner\n",
    "    X_meta_trains = {}  # Dict of datasets used for meta learner training\n",
    "    X_meta_hold_outs = {}  # Dict of datasets used for meta learner validation\n",
    "    X_meta_tests = {}  # Dict of datasets used for meta learner testing\n",
    "\n",
    "for i, (train_index, hold_out_index) in enumerate(cv.split(X_train_prep)):\n",
    "    t0 = time.time()\n",
    "    print(f'Fold {i+1} of {cv.get_n_splits()}')\n",
    "    X_train_cv, X_hold_out = X_train_prep.iloc[train_index].copy(), X_train_prep.iloc[hold_out_index].copy()\n",
    "    y_train_cv, y_hold_out = y_train.iloc[train_index].copy(), y_train.iloc[hold_out_index].copy()\n",
    "    \n",
    "    X_meta_train = pd.DataFrame(index=train_index, columns=[name for name, _ in regressors])\n",
    "    X_meta_hold_out = pd.DataFrame(index=hold_out_index, columns=[name for name, _ in regressors])\n",
    "    X_meta_test = pd.DataFrame(index=X_test_prep.index, columns=[name for name, _ in regressors])\n",
    "    \n",
    "    if FIT_REGRESSORS:\n",
    "        for name, regressor in regressors:\n",
    "            print(f'Fitting {name} ...')\n",
    "            ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "            \n",
    "            if name == 'CatBoostRegressor':\n",
    "                X_meta_train[name] = cross_val_predict(ttr, X_train_cv, y_train_cv, cv=cv, verbose=0)  # CatBoostRegressor fails on n_jobs=-1\n",
    "            else:\n",
    "                X_meta_train[name] = cross_val_predict(ttr, X_train_cv, y_train_cv, cv=cv, n_jobs=-1, verbose=0)\n",
    "            \n",
    "            # fit the model on the full cv training set\n",
    "            ttr.fit(X_train_cv, y_train_cv)\n",
    "            X_meta_hold_out[name] = ttr.predict(X_hold_out)\n",
    "            X_meta_test[name] = ttr.predict(X_test_prep)\n",
    "\n",
    "            if DISPLAY_REGRESSOR_RESULTS:\n",
    "                print(f'Hold out score of {name}: {mean_squared_error(y_hold_out, X_meta_hold_out[name], squared=False):.4f}')\n",
    "                if not SUBMIT:\n",
    "                    print(f'Test score of {name}: {mean_squared_error(y_test, X_meta_test[name], squared=False):.4f}')\n",
    "\n",
    "        # Store datasets for meta learner\n",
    "        X_meta_trains[i] = X_meta_train.copy()\n",
    "        X_meta_hold_outs[i] = X_meta_hold_out.copy()\n",
    "        X_meta_tests[i] = X_meta_test.copy()\n",
    "        \n",
    "    # Transform the predictions of regressors with target transform        \n",
    "    X_meta_train = target_transform(X_meta_trains[i])\n",
    "    X_meta_hold_out = target_transform(X_meta_hold_outs[i])\n",
    "    X_meta_test = target_transform(X_meta_tests[i])\n",
    "\n",
    "    for name, meta_regressor in meta_regressors:\n",
    "        # Fit the final estimator on the hold out predictions\n",
    "        meta_ttr = TransformedTargetRegressor(\n",
    "            meta_regressor,\n",
    "            func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "        \n",
    "        meta_ttr.fit(X_meta_train, y_train_cv)\n",
    "        y_hold_out_pred = meta_ttr.predict(X_meta_hold_out)\n",
    "        y_test_pred = meta_ttr.predict(X_meta_test)\n",
    "        \n",
    "        if not SUBMIT:\n",
    "            score_eval = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "        else:\n",
    "            score_eval = np.nan\n",
    "            \n",
    "        l1_ratio = getattr(meta_ttr.regressor_, 'l1_ratio_', np.nan)\n",
    "        alpha = getattr(meta_ttr.regressor_, 'alpha_', np.nan)\n",
    "        coef = getattr(meta_ttr.regressor_, 'coef_', np.nan)\n",
    "        intercept = getattr(meta_ttr.regressor_, 'intercept_', np.nan)\n",
    "        print(f'Meta regressor: {name}, RMSE hold out: {mean_squared_error(y_hold_out, y_hold_out_pred, squared=False)},',\n",
    "              f'RMSE test: {score_eval:.4f}, fit time: {time.time() - t0:.2f} s,',\n",
    "              f'Coefficients: {coef}, intercept: {intercept}, l1_ratio: {l1_ratio}, alpha: {alpha}', end='\\n'\n",
    "        )\n",
    "        if PLOT_ERRORS:\n",
    "            plot_predictions_errors(y_hold_out, y_hold_out_pred, 'Hold out')\n",
    "            if not SUBMIT:\n",
    "                plot_predictions_errors(y_test, y_test_pred, 'Test')\n",
    "\n",
    "    print('-'*80, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all dataframes of X_meta_trains and X_meta_hold_outs to csv seprarately\n",
    "for i in range(5):\n",
    "    X_meta_trains[i].to_csv(f'datasets/X_meta_trains_{i}.csv')\n",
    "    X_meta_hold_outs[i].to_csv(f'datasets/X_meta_hold_outs_{i}.csv')\n",
    "    X_meta_tests[i].to_csv(f'datasets/X_meta_tests_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dataframes of X_meta_trains and X_meta_hold_outs from csv seprarately\n",
    "X_meta_trains_2, X_meta_hold_outs_2 = {}, {}\n",
    "for i in range(5):\n",
    "    X_meta_trains_2[i] = pd.read_csv(f'datasets/X_meta_trains_{i}.csv', index_col=0)\n",
    "    X_meta_hold_outs_2[i] = pd.read_csv(f'datasets/X_meta_hold_outs_{i}.csv', index_col=0)\n",
    "    X_meta_tests_2[i] = pd.read_csv(f'datasets/X_meta_tests_{i}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LGBMRegressor11 ...\n",
      "Fitting XGBRegressor6 ...\n",
      "Fitting CatBoostRegressor ...\n",
      "Fitting HistGradientBoostingRegressor2 ...\n"
     ]
    }
   ],
   "source": [
    "# Make final predictions\n",
    "\n",
    "# CV stacking: \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "regressors = [\n",
    "    ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                  max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                  objective='reg:squarederror')),\n",
    "    ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True)),\n",
    "    # ('HistGradientBoostingRegressor', HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "    #                                                                 max_depth=6, learning_rate=0.1, \n",
    "    #                                                                 l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "    #                                                                 min_samples_leaf=9, max_bins=255)),\n",
    "    ('HistGradientBoostingRegressor2', HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "                                                                    max_depth=10, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.1, max_leaf_nodes=100, \n",
    "                                                                    min_samples_leaf=20, max_bins=255)),\n",
    "    \n",
    "]\n",
    "\n",
    "if SUBMIT:\n",
    "    # Store out of fold predictions for meta learner\n",
    "    X_meta_train = pd.DataFrame(index=X_train_prep.index, columns=[name for name, _ in regressors])\n",
    "    X_meta_test = pd.DataFrame(index=X_test_prep.index, columns=[name for name, _ in regressors])\n",
    "\n",
    "    # Create datasets for meta learner\n",
    "    for name, regressor in regressors:\n",
    "        print(f'Fitting {name} ...')\n",
    "        ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "        \n",
    "        if name == 'CatBoostRegressor':\n",
    "            X_meta_train[name] = cross_val_predict(ttr, X_train_prep, y_train, cv=cv, verbose=0)  # CatBoostRegressor fails on n_jobs=-1\n",
    "        else:\n",
    "            X_meta_train[name] = cross_val_predict(ttr, X_train_prep, y_train, cv=cv, n_jobs=-1, verbose=0)\n",
    "        \n",
    "        # fit the model on the full cv training set\n",
    "        ttr.fit(X_train_prep, y_train)\n",
    "        X_meta_test[name] = ttr.predict(X_test_prep)\n",
    "\n",
    "    # Transform the predictions of regressors with target transform\n",
    "    X_meta_train = target_transform(X_meta_train)\n",
    "    X_meta_test = target_transform(X_meta_test)\n",
    "\n",
    "    # Fit the final estimator on the hold out predictions\n",
    "    meta_regressor = LarsCV(cv=cv, max_iter=10000, n_jobs=-1)\n",
    "    meta_ttr = TransformedTargetRegressor(\n",
    "        meta_regressor,\n",
    "        func=target_transform, \n",
    "        inverse_func=inverse_target_transform, \n",
    "        check_inverse=False)\n",
    "    y_pred_test = meta_ttr.fit(X_meta_train, y_train).predict(X_meta_test)\n",
    "\n",
    "    # Save predictions\n",
    "    sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "    sub['price'] = y_pred_test\n",
    "    now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "    sub.to_csv(f'submissions/submission{now}.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f90af0c099b2a3322334c0593a59f872710278483b6e7b3217af559be1bbf34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
