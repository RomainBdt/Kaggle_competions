{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>62.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.03</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.06</td>\n",
       "      <td>8.12</td>\n",
       "      <td>5.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>61.2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>61.6</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.70</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>62.6</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7.65</td>\n",
       "      <td>7.61</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220535</th>\n",
       "      <td>1.11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>62.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.52</td>\n",
       "      <td>4.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220536</th>\n",
       "      <td>0.33</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>61.9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220537</th>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>61.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.12</td>\n",
       "      <td>5.15</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220538</th>\n",
       "      <td>0.27</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>61.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220539</th>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220540 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        carat  cut  color  clarity  depth  table     x     y     z\n",
       "0        1.52    3      4        3   62.2   58.0  7.27  7.33  4.55\n",
       "1        2.03    2      0        1   62.0   58.0  8.06  8.12  5.05\n",
       "2        0.70    4      3        4   61.2   57.0  5.69  5.73  3.50\n",
       "3        0.32    4      3        4   61.6   56.0  4.38  4.41  2.71\n",
       "4        1.70    3      3        3   62.6   59.0  7.65  7.61  4.77\n",
       "...       ...  ...    ...      ...    ...    ...   ...   ...   ...\n",
       "220535   1.11    3      3        2   62.3   58.0  6.61  6.52  4.09\n",
       "220536   0.33    4      2        7   61.9   55.0  4.44  4.42  2.74\n",
       "220537   0.51    3      5        3   61.7   58.0  5.12  5.15  3.17\n",
       "220538   0.27    2      4        5   61.8   56.0  4.19  4.20  2.60\n",
       "220539   1.25    3      0        2   62.0   58.0  6.90  6.88  4.27\n",
       "\n",
       "[220540 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch, MeanShift, SpectralClustering, AffinityPropagation, FeatureAgglomeration\n",
    "\n",
    "# import regressors\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, StackingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, PassiveAggressiveRegressor, Perceptron, RidgeClassifier, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Lars, BayesianRidge, ARDRegression, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor, RANSACRegressor\n",
    "from sklearn.linear_model import LassoLars, LassoLarsIC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# pandas deactivate future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "SUBMIT = True\n",
    "USE_ORIGINAL = True\n",
    "SEED = 15\n",
    "SAMPLE = 1\n",
    "\n",
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')\n",
    "orig = pd.read_csv('datasets/cubic_zirconia.csv')\n",
    "\n",
    "for i, df in enumerate([train, test, orig]):\n",
    "    df.drop(['id'], axis=1, inplace=True)\n",
    "    if not SUBMIT:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    # df['dataset'] = i\n",
    "\n",
    "# Define test set\n",
    "if not SUBMIT:\n",
    "    train, test = train_test_split(train, test_size=0.2, random_state=SEED) \n",
    "\n",
    "if USE_ORIGINAL:\n",
    "    train = pd.concat([train, orig], axis=0)\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Sampling for faster training\n",
    "if SAMPLE < 1:\n",
    "    train = train.sample(frac=SAMPLE, random_state=SEED)\n",
    "\n",
    "del orig\n",
    "\n",
    "# set training data\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('price')\n",
    "X_test = test.copy()\n",
    "\n",
    "if not SUBMIT:\n",
    "    y_test = X_test.pop('price')\n",
    "else:\n",
    "    y_test = None\n",
    "    \n",
    "base_cols = X_train.columns\n",
    "\n",
    "# transform categorical features\n",
    "def transform_categorical(df):\n",
    "    df['cut'] = df['cut'].map({'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4})\n",
    "    df['color'] = df['color'].map({'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D': 6})\n",
    "    df['clarity'] = df['clarity'].map({'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7})\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Drop extreme values\n",
    "    min = 2\n",
    "    max = 20\n",
    "    df = df[(df['x'] < max) & (df['y'] < max) & (df['z'] < max)]\n",
    "    df = df[(df['x'] > min) & (df['y'] > min) & (df['z'] > min)]\n",
    "    return df\n",
    "\n",
    "def add_volume_ratio(df):\n",
    "    # df['volume_ratio1'] = (df['x'] * df['y']) / (df['z'] * df['z'])\n",
    "    df['volume_ratio2'] = (df['x'] * df['z']) / (df['y'] * df['y'])\n",
    "    df['volume_ratio3'] = (df['y'] * df['z']) / (df['x'] * df['x'])\n",
    "    # df['volume_ratio4'] = (df['x']) / (df['z'])\n",
    "    # df['volume_ratio5'] = (df['y']) / (df['z'])\n",
    "    df['volume_ratio6'] = (df['x'] * df['z']) / (df['y'] * df['z'])  # will set nan if z is nan\n",
    "    # df['volume_ratio7'] = (df['x'] + df['y']) / df['z']\n",
    "    # df['volume_ratio8'] = (df['x'] + df['z']) / df['y']\n",
    "    # df['volume_ratio9'] = (df['y'] + df['z']) / df['x']\n",
    "    # df['volume_ratio10'] = (df['x'] * df['y'] * df['z']) / (df['x'].mean() * df['y'].mean() * df['z'].mean())\n",
    "    # df['volume_ratio11'] = (df['x'] * df['y'] * df['z']) / (df['x'].max() * df['y'].max() * df['z'].max())\n",
    "    # df['volume_ratio12'] = (df['x'] * df['y'] * df['z']) / (df['x'].min() * df['y'].min() * df['z'].min())\n",
    "    # df['volume_ratio13'] = (df['x'] * df['y'] * df['z']) / (df['x'].median() * df['y'].median() * df['z'].median())\n",
    "    # df['volume_ratio14'] = (df['x'] * df['y'] * df['z']) / (df['x'].std() * df['y'].std() * df['z'].std())\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df[\"volume\"] = df[\"x\"] * df[\"y\"] * df[\"z\"]\n",
    "    df[\"surface_area\"] = 2 * (df[\"x\"] * df[\"y\"] + df[\"y\"] * df[\"z\"] + df[\"z\"] * df[\"x\"])\n",
    "    df[\"aspect_ratio_xy\"] = df[\"x\"] / df[\"y\"]\n",
    "    df[\"aspect_ratio_yz\"] = df[\"y\"] / df[\"z\"]\n",
    "    df[\"aspect_ratio_zx\"] = df[\"z\"] / df[\"x\"]\n",
    "    df[\"diagonal_distance\"] = np.sqrt(df[\"x\"] ** 2 + df[\"y\"] ** 2 + df[\"z\"] ** 2)\n",
    "    # df[\"relative_height\"] = (df[\"z\"] - df[\"z\"].min()) / (df[\"z\"].max() - df[\"z\"].min())\n",
    "    # df[\"relative_position\"] = (df[\"x\"] + df[\"y\"] + df[\"z\"]) / (df[\"x\"] + df[\"y\"] + df[\"z\"]).sum()\n",
    "    # df[\"volume_ratio\"] = df[\"x\"] * df[\"y\"] * df[\"z\"] / (df[\"x\"].mean() * df[\"y\"].mean() * df[\"z\"].mean())\n",
    "    # df[\"length_ratio\"] = df[\"x\"] / df[\"x\"].mean()\n",
    "    # df[\"width_ratio\"] = df[\"y\"] / df[\"y\"].mean()\n",
    "    # df[\"height_ratio\"] = df[\"z\"] / df[\"z\"].mean()\n",
    "    df[\"sphericity\"] = 1.4641 * (6 * df[\"volume\"])**(2/3) / df[\"surface_area\"]\n",
    "    df[\"compactness\"] = df[\"volume\"]**(1/3) / df[\"x\"]\n",
    "    df['density'] = df['carat'] / df['volume']\n",
    "    df['table_percentage'] = (df['table'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['depth_percentage'] = (df['depth'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['symmetry'] = (abs(df['x'] - df['z']) + abs(df['y'] - df['z'])) / (df['x'] + df['y'] + df['z'])\n",
    "    df['surface_area'] = 2 * ((df['x'] * df['y']) + (df['x'] * df['z']) + (df['y'] * df['z']))\n",
    "    df['depth_to_table_ratio'] = df['depth'] / df['table']\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def target_transform(serie):\n",
    "    serie = np.log1p(serie)\n",
    "    return serie\n",
    "\n",
    "def inverse_target_transform(serie):\n",
    "    serie = np.expm1(serie)\n",
    "    return serie\n",
    "\n",
    "def set_categorical(df):\n",
    "    df['cut'] = df['cut'].astype('category')\n",
    "    df['color'] = df['color'].astype('category')\n",
    "    df['clarity'] = df['clarity'].astype('category')\n",
    "    return df\n",
    "\n",
    "def add_girdle_parameters(df):\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def impute_x_y_z(df):\n",
    "    df['is_imputed'] = df.isna().any(axis=1).astype(int)\n",
    "    df['girdle_diameter'].fillna((df['x'] + df['y']) / 2, inplace=True)\n",
    "    df['x'].fillna(2*df['girdle_diameter'] - df['y'], inplace=True)\n",
    "    df['y'].fillna(2*df['girdle_diameter'] - df['x'], inplace=True)\n",
    "    df['z'].fillna(df['girdle_diameter'] * df['depth'] / 100, inplace=True)\n",
    "    df = add_girdle_parameters(df)\n",
    "    return df\n",
    "\n",
    "def set_nan(df):\n",
    "    for col in ['x', 'y', 'z']:\n",
    "        df[col].replace(0, np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def drop_girdle_parameters(df):\n",
    "    df.drop(['girdle_diameter', 'girdle_thickness', 'girdle_ratio'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Make data preparation pipeline\n",
    "def data_prepation(X_train, X_test):\n",
    "    \n",
    "    for df in [X_train, X_test]:\n",
    "        # df = set_nan(df)\n",
    "        df = transform_categorical(df)\n",
    "        # df = set_categorical(df)\n",
    "        # df = add_girdle_parameters(df)\n",
    "        # df = impute_x_y_z(df)\n",
    "        # df = drop_girdle_parameters(df)\n",
    "        \n",
    "    \n",
    "    # imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    # imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    # imputer = KNNImputer(n_neighbors=1, weights=\"uniform\")\n",
    "    # X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    # X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # selected_cols = base_cols\n",
    "    # selected_cols = ['surface_area', 'clarity', 'color', 'cut', 'carat', 'depth_percentage', 'depth', 'compactness', 'depth_to_table_ratio']\n",
    "    \n",
    "    # for df in [X_train, X_test]:\n",
    "    #     df = add_volume_ratio(df)\n",
    "        # df = feature_engineering(df)\n",
    "        \n",
    "        # df.fillna(0, inplace=True)\n",
    "        # df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        # df.dropna(inplace=True)\n",
    "        # df.drop([col for col in df.columns if col not in selected_cols], axis=1, inplace=True)\n",
    "        \n",
    "    # Scaling\n",
    "    # scaler = PowerTransformer()\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    # Clustering features\n",
    "    # model = KMeans(n_clusters=20, random_state=42)\n",
    "    # X_train['cluster'] = model.fit_predict(X_train)\n",
    "    # X_test['cluster'] = model.predict(X_test)\n",
    "    \n",
    "    return X_train, X_test\n",
    "            \n",
    "data_prep_has_fit_method = False\n",
    "\n",
    "if not data_prep_has_fit_method:\n",
    "    X_train, X_test = data_prepation(X_train, X_test)\n",
    "    X_train_prep, X_test_prep = X_train.copy(), X_test.copy()\n",
    "else:\n",
    "    X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "\n",
    "   \n",
    "# X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "# pd.DataFrame(X_train_prep.isna().sum(), columns=['train']).join(pd.DataFrame(X_test_prep.isna().sum(), columns=['test']))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:09,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor1: 579.2315 ± 4.4920, Time: 12.80 seconds, RMSE: 576.7363\n",
      "         fold0  fold1  fold2  fold3  fold4   mean\n",
      "clarity    691    724    702    707    719  708.6\n",
      "color      598    572    563    568    576  575.4\n",
      "y          455    445    475    466    471  462.4\n",
      "x          389    376    372    391    372  380.0\n",
      "carat      347    335    371    331    347  346.2\n",
      "z          179    174    164    171    168  171.2\n",
      "depth      153    162    157    161    160  158.6\n",
      "cut        112    131    114    118    106  116.2\n",
      "table       76     81     82     87     81   81.4\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Set categorical features for catboost\n",
    "cat_features = [col for col in X_train_prep.columns if X_train_prep[col].dtype == 'category']\n",
    "\n",
    "regressors = {\n",
    "    'LGBMRegressor1': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt'),\n",
    "    # 'LGBMRegressor2': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart'),\n",
    "    # 'LGBMRegressor3': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='goss'),\n",
    "    # 'LGBMRegressor4': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1),\n",
    "    # 'LGBMRegressor5': LGBMRegressor(random_state=SEED, n_jobs=-1, class_weight='balanced'),\n",
    "    # 'LGBMRegressor6': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'LGBMRegressor7': LGBMRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor8': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor9': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart', colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor10': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240),\n",
    "    # 'LGBMRegressor11': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6),\n",
    "    # 'XGBRegressor1': XGBRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'XGBRegressor2': XGBRegressor(random_state=SEED, n_jobs=-1, booster='dart'),\n",
    "    # 'XGBRegressor3': XGBRegressor(random_state=SEED, n_jobs=-1, booster='gblinear'),\n",
    "    # 'XGBRegressor4': XGBRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'XGBRegressor5': XGBRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'XGBRegressor6': XGBRegressor(random_state=SEED, \n",
    "    #                               n_jobs=-1, \n",
    "    #                               learning_rate=0.055, \n",
    "    #                               n_estimators=200, \n",
    "    #                               max_depth=8, \n",
    "    #                               min_child_weight=1, \n",
    "    #                               gamma=0.07, \n",
    "    #                               colsample_bytree=0.67, \n",
    "    #                               colsample_bylevel=0.67, \n",
    "    #                               colsample_bynode=0.8,\n",
    "    #                               subsample=0.7, \n",
    "    #                               objective='reg:squarederror'),\n",
    "    # 'XGBRFRegressor6': XGBRegressor(random_state=SEED, n_jobs=-1, objective='reg:squarederror'),\n",
    "    # 'XGBRandomForestRegressor': XGBRFRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'CatBoostRegressor': CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features), # Promising but fails on the cv\n",
    "    # 'HistGradientBoostingRegressor': HistGradientBoostingRegressor(random_state=SEED),\n",
    "    # 'HistGradientBoostingRegressor2': HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "    #                                                                 max_depth=6, learning_rate=0.1, \n",
    "    #                                                                 l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "    #                                                                 min_samples_leaf=9, max_bins=255),\n",
    "    # 'RandomForestRegressor': RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'ExtraTreesRegressor': ExtraTreesRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'AdaBoostRegressor': AdaBoostRegressor(random_state=SEED),\n",
    "    # 'GradientBoostingRegressor': GradientBoostingRegressor(random_state=SEED),\n",
    "    # 'BaggingRegressor': BaggingRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    # 'DecisionTreeRegressor': DecisionTreeRegressor(random_state=SEED),\n",
    "    # 'GaussianProcessRegressor': GaussianProcessRegressor(random_state=SEED),\n",
    "    # 'MLPRegressor1': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='adam'),\n",
    "    # 'MLPRegressor2': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='lbfgs'), # promising but long to train\n",
    "    # 'MLPRegressor3': MLPRegressor(random_state=SEED, max_iter=5000, activation='tanh', solver='adam'),\n",
    "    # 'MLPRegressor4': MLPRegressor(random_state=SEED, max_iter=1000, activation='tanh', solver='lbfgs'),  # promising but long to train\n",
    "    # 'MLPRegressor5': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='adam'),\n",
    "    # 'MLPRegressor6': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='lbfgs'),\n",
    "    # 'MLPRegressor7': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='adam'),\n",
    "    # 'MLPRegressor8': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='lbfgs'),\n",
    "    # 'Ridge': Ridge(random_state=SEED),\n",
    "    # 'SGDRegressor': SGDRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'PassiveAggressiveRegressor': PassiveAggressiveRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'Perceptron': Perceptron(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'LinearRegression': LinearRegression(),\n",
    "    # 'Lasso': Lasso(random_state=SEED),\n",
    "    # 'ElasticNet': ElasticNet(random_state=SEED, max_iter=1e6),\n",
    "    # 'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    # 'BayesianRidge': BayesianRidge(),\n",
    "    # 'ARDRegression': ARDRegression(),\n",
    "    # 'TheilSenRegressor': TheilSenRegressor(random_state=SEED),\n",
    "    # 'RANSACRegressor': RANSACRegressor(random_state=SEED),\n",
    "    # 'OrthogonalMatchingPursuit': OrthogonalMatchingPursuit(normalize=False),\n",
    "    # 'Lars': Lars(),\n",
    "    # 'LassoLars': LassoLars(),\n",
    "    # 'LassoLarsIC': LassoLarsIC(normalize=False),\n",
    "    # 'StackingRegressor': StackingRegressor(\n",
    "    #         estimators=[\n",
    "    #             ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, \n",
    "    #                                             max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    #             ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "    #                                         max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "    #                                         colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "    #                                         objective='reg:squarederror')),\n",
    "    #             ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features)), # Promising but fails on the cv\n",
    "    #             # ('ExtraTreesRegressor', ExtraTreesRegressor(random_state=SEED, n_jobs=-1))\n",
    "    #             ], \n",
    "    #         final_estimator=Ridge(random_state=SEED),\n",
    "    #         cv=cv,\n",
    "    #         # n_jobs=-1,\n",
    "    #         verbose=1\n",
    "    #         )\n",
    "}\n",
    "\n",
    "for model_name, regressor in regressors.items():\n",
    "    t0 = time.time()\n",
    "    scores = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "    ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(cv.split(X_train))):\n",
    "        \n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index].copy(), X_train.iloc[test_index].copy()\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "        \n",
    "        if data_prep_has_fit_method:\n",
    "            X_train_cv, X_test_cv = data_prepation(X_train_cv, X_test_cv)\n",
    "        \n",
    "        ttr.fit(X_train_cv, y_train_cv)        \n",
    "        y_pred = ttr.predict(X_test_cv)\n",
    "        score_eval = mean_squared_error(y_test_cv, y_pred, squared=False)\n",
    "        scores.append(score_eval)\n",
    "        \n",
    "        try:\n",
    "            feature_importance = pd.Series(ttr.regressor_.feature_importances_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "        except:\n",
    "            try:\n",
    "                feature_importance = pd.Series(ttr.regressor_.coef_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "            except:\n",
    "                feature_importance = pd.Series(np.zeros(X_train_cv.shape[1]), index=X_train_cv.columns, name=f'fold{i}')\n",
    "        feature_importances = pd.concat([feature_importances, feature_importance], axis=1)\n",
    "    \n",
    "    feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "    \n",
    "    ttr.fit(X_train_prep, y_train)\n",
    "    y_pred = ttr.predict(X_test_prep)\n",
    "    \n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(f'{model_name}: {np.mean(scores):.4f} ± {np.std(scores):.4f}, Time: {time.time() - t0:.2f} seconds, RMSE: {score_eval:.4f}')\n",
    "    print(feature_importances.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = y_pred\n",
    "\n",
    "sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "sub['price'] = y_pred_test\n",
    "now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "sub.to_csv(f'submissions/submission{now}.csv', index=False)\n",
    "# Copy the leaked values from the original dataset before submitting\n",
    "# Transform the price column back to the original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_val_score(estimator, X, y, cv):\n",
    "    scores = []\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X)):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "        estimator.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = estimator.predict(X_test_cv)\n",
    "        score_eval = mean_squared_error(y_test_cv, y_pred, squared=False)\n",
    "        scores.append(score_eval)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressors = [\n",
    "#     ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, \n",
    "#                                      max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "#     ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "#                                   max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "#                                   colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "#                                   objective='reg:squarederror')),\n",
    "#     ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features)), # Promising but fails on the cv\n",
    "# ]\n",
    "\n",
    "# model = StackingRegressor(\n",
    "#     estimators=regressors,\n",
    "#     final_estimator=Ridge(random_state=SEED),\n",
    "#     # cv=cv,\n",
    "#     # n_jobs=-1,\n",
    "#     verbose=1,\n",
    "#     )\n",
    "# scores = my_cross_val_score(model, X_train_prep, y_train, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_meta_trains' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m     X_meta_tests[i] \u001b[39m=\u001b[39m X_meta_test\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     63\u001b[0m \u001b[39m# Transform the predictions of regressors with target transform        \u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m X_meta_train \u001b[39m=\u001b[39m target_transform(X_meta_trains[i])\n\u001b[0;32m     65\u001b[0m X_meta_hold_out \u001b[39m=\u001b[39m target_transform(X_meta_hold_outs[i])\n\u001b[0;32m     66\u001b[0m X_meta_test \u001b[39m=\u001b[39m target_transform(X_meta_tests[i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_meta_trains' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, cross_val_predict, cross_validate\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV\n",
    "\n",
    "\n",
    "# CV stacking: \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "regressors = [\n",
    "    ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                  max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                  objective='reg:squarederror')),\n",
    "    ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True)),\n",
    "    ('HistGradientBoostingRegressor', HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "                                                                    max_depth=6, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "                                                                    min_samples_leaf=9, max_bins=255)),\n",
    "    # ('HistGradientBoostingRegressor2', HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "    #                                                                 max_depth=10, learning_rate=0.1, \n",
    "    #                                                                 l2_regularization=0.1, max_leaf_nodes=100, \n",
    "    #                                                                 min_samples_leaf=20, max_bins=255)),\n",
    "    \n",
    "]\n",
    "\n",
    "FIT_REGRESSORS = True\n",
    "\n",
    "if FIT_REGRESSORS:\n",
    "    # Store hold out predictions\n",
    "    X_meta_trains = {}  # Dict of datasets used for meta learner training\n",
    "    X_meta_hold_outs = {}  # Dict of datasets used for meta learner validation\n",
    "    X_meta_tests = {}  # Dict of datasets used for meta learner testing\n",
    "\n",
    "for i, (train_index, hold_out_index) in enumerate(cv.split(X_train)):\n",
    "    t0 = time.time()\n",
    "    X_train_cv, X_hold_out = X_train.iloc[train_index].copy(), X_train.iloc[hold_out_index].copy()\n",
    "    y_train_cv, y_hold_out = y_train.iloc[train_index].copy(), y_train.iloc[hold_out_index].copy()\n",
    "    \n",
    "    X_meta_train = pd.DataFrame(index=train_index, columns=[name for name, _ in regressors])\n",
    "    X_meta_hold_out = pd.DataFrame(index=hold_out_index, columns=[name for name, _ in regressors])\n",
    "    X_meta_test = pd.DataFrame(index=X_test_prep.index, columns=[name for name, _ in regressors])\n",
    "    \n",
    "    if FIT_REGRESSORS:\n",
    "        for name, regressor in regressors:\n",
    "            print(f'Fitting {name} on fold {i+1} of {cv.get_n_splits()}')\n",
    "            ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "            \n",
    "            if name == 'CatBoostRegressor':\n",
    "                cv_predict = cross_val_predict(ttr, X_train_cv, y_train_cv, cv=cv, verbose=0)  # CatBoostRegressor fails on n_jobs=-1\n",
    "            else:\n",
    "                cv_predict = cross_val_predict(ttr, X_train_cv, y_train_cv, cv=cv, n_jobs=-1, verbose=0)\n",
    "            X_meta_train[name] = cv_predict  # Out of fold predictions\n",
    "            \n",
    "            #   fit the model on the full training set\n",
    "            ttr.fit(X_train_cv, y_train_cv)\n",
    "            X_meta_hold_out[name] = ttr.predict(X_hold_out)\n",
    "            X_meta_test[name] = ttr.predict(X_test)\n",
    "        X_meta_trains[i] = X_meta_train.copy()\n",
    "        X_meta_hold_outs[i] = X_meta_hold_out.copy()\n",
    "        X_meta_tests[i] = X_meta_test.copy()\n",
    "        \n",
    "    # Transform the predictions of regressors with target transform        \n",
    "    X_meta_train = target_transform(X_meta_trains[i])\n",
    "    X_meta_hold_out = target_transform(X_meta_hold_outs[i])\n",
    "    X_meta_test = target_transform(X_meta_tests[i])\n",
    "  \n",
    "    # Fit the final estimator on the hold out predictions\n",
    "    meta_regressor = TransformedTargetRegressor(\n",
    "        RidgeCV(cv=cv, scoring='neg_root_mean_squared_error', alphas=np.logspace(-3, 3, 19)),\n",
    "        func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "    \n",
    "    meta_regressor.fit(X_meta_train, y_train_cv)\n",
    "    y_hold_out_pred = meta_regressor.predict(X_meta_hold_out)\n",
    "    y_pred = meta_regressor.predict(X_meta_test)\n",
    "    \n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        \n",
    "    print(f'Fold {i}: {mean_squared_error(y_hold_out, y_hold_out_pred, squared=False)}, {time.time() - t0:.2f} s, RMSE test: {score_eval:.4f}')\n",
    "    print('meta_regressor.coef_:', meta_regressor.regressor_.coef_, 'meta_regressor.alpha_:', meta_regressor.regressor_.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all dataframes of X_meta_trains and X_meta_hold_outs to csv seprarately\n",
    "\n",
    "for i in range(5):\n",
    "    X_meta_trains[i].to_csv(f'datasets/X_meta_trains_{i}.csv')\n",
    "    X_meta_hold_outs[i].to_csv(f'datasets/X_meta_hold_outs_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dataframes of X_meta_trains and X_meta_hold_outs from csv seprarately\n",
    "X_meta_trains_2, X_meta_hold_outs_2 = {}, {}\n",
    "for i in range(5):\n",
    "    X_meta_trains_2[i] = pd.read_csv(f'datasets/X_meta_trains_{i}.csv', index_col=0)\n",
    "    X_meta_hold_outs_2[i] = pd.read_csv(f'datasets/X_meta_hold_outs_{i}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f90af0c099b2a3322334c0593a59f872710278483b6e7b3217af559be1bbf34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
