{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>volume_ratio1</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_ratio4</th>\n",
       "      <th>volume_ratio5</th>\n",
       "      <th>volume_ratio6</th>\n",
       "      <th>volume_ratio7</th>\n",
       "      <th>volume_ratio8</th>\n",
       "      <th>volume_ratio9</th>\n",
       "      <th>volume_ratio10</th>\n",
       "      <th>volume_ratio11</th>\n",
       "      <th>volume_ratio13</th>\n",
       "      <th>volume_ratio14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.38</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>61.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>7.11</td>\n",
       "      <td>7.17</td>\n",
       "      <td>4.40</td>\n",
       "      <td>2.633197</td>\n",
       "      <td>...</td>\n",
       "      <td>1.615909</td>\n",
       "      <td>1.629545</td>\n",
       "      <td>0.991632</td>\n",
       "      <td>3.245455</td>\n",
       "      <td>1.605300</td>\n",
       "      <td>1.627286</td>\n",
       "      <td>1.938986</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>1.948928</td>\n",
       "      <td>261.179152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.56</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>61.5</td>\n",
       "      <td>56.0</td>\n",
       "      <td>7.51</td>\n",
       "      <td>7.49</td>\n",
       "      <td>4.61</td>\n",
       "      <td>2.646793</td>\n",
       "      <td>...</td>\n",
       "      <td>1.629067</td>\n",
       "      <td>1.624729</td>\n",
       "      <td>1.002670</td>\n",
       "      <td>3.253796</td>\n",
       "      <td>1.618158</td>\n",
       "      <td>1.611185</td>\n",
       "      <td>2.241589</td>\n",
       "      <td>0.013533</td>\n",
       "      <td>2.253082</td>\n",
       "      <td>301.939377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.36</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60.9</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.59</td>\n",
       "      <td>4.61</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.679791</td>\n",
       "      <td>...</td>\n",
       "      <td>1.633452</td>\n",
       "      <td>1.640569</td>\n",
       "      <td>0.995662</td>\n",
       "      <td>3.274021</td>\n",
       "      <td>1.605206</td>\n",
       "      <td>1.616558</td>\n",
       "      <td>0.513988</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>0.516624</td>\n",
       "      <td>69.233614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.35</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.595650</td>\n",
       "      <td>...</td>\n",
       "      <td>1.616487</td>\n",
       "      <td>1.605735</td>\n",
       "      <td>1.006696</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>1.629464</td>\n",
       "      <td>1.611973</td>\n",
       "      <td>0.487295</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.489794</td>\n",
       "      <td>65.638075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>61.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.74</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.667353</td>\n",
       "      <td>...</td>\n",
       "      <td>1.626298</td>\n",
       "      <td>1.640138</td>\n",
       "      <td>0.991561</td>\n",
       "      <td>3.266436</td>\n",
       "      <td>1.601266</td>\n",
       "      <td>1.623404</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.559408</td>\n",
       "      <td>74.967170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181786</th>\n",
       "      <td>1.11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>62.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.52</td>\n",
       "      <td>4.09</td>\n",
       "      <td>2.576336</td>\n",
       "      <td>...</td>\n",
       "      <td>1.616137</td>\n",
       "      <td>1.594132</td>\n",
       "      <td>1.013804</td>\n",
       "      <td>3.210269</td>\n",
       "      <td>1.641104</td>\n",
       "      <td>1.605144</td>\n",
       "      <td>1.523722</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>1.531534</td>\n",
       "      <td>205.243512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181787</th>\n",
       "      <td>0.33</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>61.9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.613991</td>\n",
       "      <td>...</td>\n",
       "      <td>1.620438</td>\n",
       "      <td>1.613139</td>\n",
       "      <td>1.004525</td>\n",
       "      <td>3.233577</td>\n",
       "      <td>1.624434</td>\n",
       "      <td>1.612613</td>\n",
       "      <td>0.464825</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.467208</td>\n",
       "      <td>62.611322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181788</th>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>61.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.12</td>\n",
       "      <td>5.15</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.623969</td>\n",
       "      <td>...</td>\n",
       "      <td>1.615142</td>\n",
       "      <td>1.624606</td>\n",
       "      <td>0.994175</td>\n",
       "      <td>3.239748</td>\n",
       "      <td>1.609709</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.722553</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.726258</td>\n",
       "      <td>97.327042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181789</th>\n",
       "      <td>0.27</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>61.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.603254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.611538</td>\n",
       "      <td>1.615385</td>\n",
       "      <td>0.997619</td>\n",
       "      <td>3.226923</td>\n",
       "      <td>1.616667</td>\n",
       "      <td>1.622912</td>\n",
       "      <td>0.395521</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.397549</td>\n",
       "      <td>53.276261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181790</th>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.27</td>\n",
       "      <td>2.603645</td>\n",
       "      <td>...</td>\n",
       "      <td>1.615925</td>\n",
       "      <td>1.611241</td>\n",
       "      <td>1.002907</td>\n",
       "      <td>3.227166</td>\n",
       "      <td>1.623547</td>\n",
       "      <td>1.615942</td>\n",
       "      <td>1.752260</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>1.761245</td>\n",
       "      <td>236.027430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181791 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        carat  cut  color  clarity  depth  table     x     y     z  \\\n",
       "0        1.38    4      5        1   61.7   56.0  7.11  7.17  4.40   \n",
       "1        1.56    4      2        4   61.5   56.0  7.51  7.49  4.61   \n",
       "2        0.36    4      6        1   60.9   56.0  4.59  4.61  2.81   \n",
       "3        0.35    3      5        2   62.2   59.0  4.51  4.48  2.79   \n",
       "4        0.39    2      6        2   61.3   56.0  4.70  4.74  2.89   \n",
       "...       ...  ...    ...      ...    ...    ...   ...   ...   ...   \n",
       "181786   1.11    3      3        2   62.3   58.0  6.61  6.52  4.09   \n",
       "181787   0.33    4      2        7   61.9   55.0  4.44  4.42  2.74   \n",
       "181788   0.51    3      5        3   61.7   58.0  5.12  5.15  3.17   \n",
       "181789   0.27    2      4        5   61.8   56.0  4.19  4.20  2.60   \n",
       "181790   1.25    3      0        2   62.0   58.0  6.90  6.88  4.27   \n",
       "\n",
       "        volume_ratio1  ...  volume_ratio4  volume_ratio5  volume_ratio6  \\\n",
       "0            2.633197  ...       1.615909       1.629545       0.991632   \n",
       "1            2.646793  ...       1.629067       1.624729       1.002670   \n",
       "2            2.679791  ...       1.633452       1.640569       0.995662   \n",
       "3            2.595650  ...       1.616487       1.605735       1.006696   \n",
       "4            2.667353  ...       1.626298       1.640138       0.991561   \n",
       "...               ...  ...            ...            ...            ...   \n",
       "181786       2.576336  ...       1.616137       1.594132       1.013804   \n",
       "181787       2.613991  ...       1.620438       1.613139       1.004525   \n",
       "181788       2.623969  ...       1.615142       1.624606       0.994175   \n",
       "181789       2.603254  ...       1.611538       1.615385       0.997619   \n",
       "181790       2.603645  ...       1.615925       1.611241       1.002907   \n",
       "\n",
       "        volume_ratio7  volume_ratio8  volume_ratio9  volume_ratio10  \\\n",
       "0            3.245455       1.605300       1.627286        1.938986   \n",
       "1            3.253796       1.618158       1.611185        2.241589   \n",
       "2            3.274021       1.605206       1.616558        0.513988   \n",
       "3            3.222222       1.629464       1.611973        0.487295   \n",
       "4            3.266436       1.601266       1.623404        0.556554   \n",
       "...               ...            ...            ...             ...   \n",
       "181786       3.210269       1.641104       1.605144        1.523722   \n",
       "181787       3.233577       1.624434       1.612613        0.464825   \n",
       "181788       3.239748       1.609709       1.625000        0.722553   \n",
       "181789       3.226923       1.616667       1.622912        0.395521   \n",
       "181790       3.227166       1.623547       1.615942        1.752260   \n",
       "\n",
       "        volume_ratio11  volume_ratio13  volume_ratio14  \n",
       "0             0.011706        1.948928      261.179152  \n",
       "1             0.013533        2.253082      301.939377  \n",
       "2             0.003103        0.516624       69.233614  \n",
       "3             0.002942        0.489794       65.638075  \n",
       "4             0.003360        0.559408       74.967170  \n",
       "...                ...             ...             ...  \n",
       "181786        0.009199        1.531534      205.243512  \n",
       "181787        0.002806        0.467208       62.611322  \n",
       "181788        0.004362        0.726258       97.327042  \n",
       "181789        0.002388        0.397549       53.276261  \n",
       "181790        0.010579        1.761245      236.027430  \n",
       "\n",
       "[181791 rows x 22 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch, MeanShift, SpectralClustering, AffinityPropagation, FeatureAgglomeration\n",
    "\n",
    "# import regressors\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, PassiveAggressiveRegressor, Perceptron, RidgeClassifier, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Lars, BayesianRidge, ARDRegression, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor, RANSACRegressor\n",
    "from sklearn.linear_model import LassoLars, LassoLarsIC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# pandas deactivate future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "SUBMIT = False\n",
    "USE_ORIGINAL = True\n",
    "SEED = 15\n",
    "SAMPLE = 1\n",
    "\n",
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')\n",
    "orig = pd.read_csv('datasets/cubic_zirconia.csv')\n",
    "\n",
    "for i, df in enumerate([train, test, orig]):\n",
    "    df.drop(['id'], axis=1, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # df['dataset'] = i\n",
    "\n",
    "# Define test set\n",
    "if not SUBMIT:\n",
    "    train, test = train_test_split(train, test_size=0.2, random_state=SEED) \n",
    "\n",
    "if USE_ORIGINAL:\n",
    "    train = pd.concat([train, orig], axis=0)\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Sampling for faster training\n",
    "if SAMPLE < 1:\n",
    "    train = train.sample(frac=SAMPLE, random_state=SEED)\n",
    "\n",
    "del orig\n",
    "\n",
    "# set training data\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('price')\n",
    "X_test = test.copy()\n",
    "\n",
    "if not SUBMIT:\n",
    "    y_test = X_test.pop('price')\n",
    "else:\n",
    "    y_test = None\n",
    "    \n",
    "base_cols = X_train.columns\n",
    "\n",
    "# transform categorical features\n",
    "def transform_categorical(df):\n",
    "    df['cut'] = df['cut'].map({'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4})\n",
    "    df['color'] = df['color'].map({'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D': 6})\n",
    "    df['clarity'] = df['clarity'].map({'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7})\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Drop extreme values\n",
    "    min = 2\n",
    "    max = 20\n",
    "    df = df[(df['x'] < max) & (df['y'] < max) & (df['z'] < max)]\n",
    "    df = df[(df['x'] > min) & (df['y'] > min) & (df['z'] > min)]\n",
    "    return df\n",
    "\n",
    "def add_volume_ratio(df):\n",
    "    df['volume_ratio1'] = (df['x'] * df['y']) / (df['z'] * df['z'])\n",
    "    df['volume_ratio2'] = (df['x'] * df['z']) / (df['y'] * df['y'])\n",
    "    df['volume_ratio3'] = (df['y'] * df['z']) / (df['x'] * df['x'])\n",
    "    df['volume_ratio4'] = (df['x']) / (df['z'])\n",
    "    df['volume_ratio5'] = (df['y']) / (df['z'])\n",
    "    df['volume_ratio6'] = (df['x']) / (df['y'])\n",
    "    df['volume_ratio7'] = (df['x'] + df['y']) / df['z']\n",
    "    df['volume_ratio8'] = (df['x'] + df['z']) / df['y']\n",
    "    df['volume_ratio9'] = (df['y'] + df['z']) / df['x']\n",
    "    df['volume_ratio10'] = (df['x'] * df['y'] * df['z']) / (df['x'].mean() * df['y'].mean() * df['z'].mean())\n",
    "    df['volume_ratio11'] = (df['x'] * df['y'] * df['z']) / (df['x'].max() * df['y'].max() * df['z'].max())\n",
    "    # df['volume_ratio12'] = (df['x'] * df['y'] * df['z']) / (df['x'].min() * df['y'].min() * df['z'].min())\n",
    "    df['volume_ratio13'] = (df['x'] * df['y'] * df['z']) / (df['x'].median() * df['y'].median() * df['z'].median())\n",
    "    df['volume_ratio14'] = (df['x'] * df['y'] * df['z']) / (df['x'].std() * df['y'].std() * df['z'].std())\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df[\"volume\"] = df[\"x\"] * df[\"y\"] * df[\"z\"]\n",
    "    df[\"surface_area\"] = 2 * (df[\"x\"] * df[\"y\"] + df[\"y\"] * df[\"z\"] + df[\"z\"] * df[\"x\"])\n",
    "    df[\"aspect_ratio_xy\"] = df[\"x\"] / df[\"y\"]\n",
    "    df[\"aspect_ratio_yz\"] = df[\"y\"] / df[\"z\"]\n",
    "    df[\"aspect_ratio_zx\"] = df[\"z\"] / df[\"x\"]\n",
    "    df[\"diagonal_distance\"] = np.sqrt(df[\"x\"] ** 2 + df[\"y\"] ** 2 + df[\"z\"] ** 2)\n",
    "    # df[\"relative_height\"] = (df[\"z\"] - df[\"z\"].min()) / (df[\"z\"].max() - df[\"z\"].min())\n",
    "    # df[\"relative_position\"] = (df[\"x\"] + df[\"y\"] + df[\"z\"]) / (df[\"x\"] + df[\"y\"] + df[\"z\"]).sum()\n",
    "    # df[\"volume_ratio\"] = df[\"x\"] * df[\"y\"] * df[\"z\"] / (df[\"x\"].mean() * df[\"y\"].mean() * df[\"z\"].mean())\n",
    "    # df[\"length_ratio\"] = df[\"x\"] / df[\"x\"].mean()\n",
    "    # df[\"width_ratio\"] = df[\"y\"] / df[\"y\"].mean()\n",
    "    # df[\"height_ratio\"] = df[\"z\"] / df[\"z\"].mean()\n",
    "    df[\"sphericity\"] = 1.4641 * (6 * df[\"volume\"])**(2/3) / df[\"surface_area\"]\n",
    "    df[\"compactness\"] = df[\"volume\"]**(1/3) / df[\"x\"]\n",
    "    df['density'] = df['carat'] / df['volume']\n",
    "    df['table_percentage'] = (df['table'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['depth_percentage'] = (df['depth'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['symmetry'] = (abs(df['x'] - df['z']) + abs(df['y'] - df['z'])) / (df['x'] + df['y'] + df['z'])\n",
    "    df['surface_area'] = 2 * ((df['x'] * df['y']) + (df['x'] * df['z']) + (df['y'] * df['z']))\n",
    "    df['depth_to_table_ratio'] = df['depth'] / df['table']\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def target_transform(serie):\n",
    "    serie = np.log1p(serie)\n",
    "    return serie\n",
    "\n",
    "def inverse_target_transform(serie):\n",
    "    serie = np.expm1(serie)\n",
    "    return serie\n",
    "\n",
    "def set_categorical(df):\n",
    "    df['cut'] = df['cut'].astype('category')\n",
    "    df['color'] = df['color'].astype('category')\n",
    "    df['clarity'] = df['clarity'].astype('category')\n",
    "    return df\n",
    "\n",
    "def add_girdle_parameters(df):\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def impute_x_y_z(df):\n",
    "    df['is_imputed'] = df.isna().any(axis=1).astype(int)\n",
    "    df['girdle_diameter'].fillna((df['x'] + df['y']) / 2, inplace=True)\n",
    "    df['x'].fillna(2*df['girdle_diameter'] - df['y'], inplace=True)\n",
    "    df['y'].fillna(2*df['girdle_diameter'] - df['x'], inplace=True)\n",
    "    df['z'].fillna(df['girdle_diameter'] * df['depth'] / 100, inplace=True)\n",
    "    df = add_girdle_parameters(df)\n",
    "    return df\n",
    "\n",
    "def set_nan(df):\n",
    "    for col in ['x', 'y', 'z']:\n",
    "        df[col].replace(0, np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def drop_girdle_parameters(df):\n",
    "    df.drop(['girdle_diameter', 'girdle_thickness', 'girdle_ratio'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Make data preparation pipeline\n",
    "def data_prepation(X_train, X_test):\n",
    "    \n",
    "    for df in [X_train, X_test]:\n",
    "        # df = set_nan(df)\n",
    "        df = transform_categorical(df)\n",
    "        # df = set_categorical(df)\n",
    "        # df = add_girdle_parameters(df)\n",
    "        # df = impute_x_y_z(df)\n",
    "        # df = drop_girdle_parameters(df)\n",
    "        \n",
    "    \n",
    "    # imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    # imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    # imputer = KNNImputer(n_neighbors=1, weights=\"uniform\")\n",
    "    # X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    # X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # selected_cols = base_cols\n",
    "    # selected_cols = ['surface_area', 'clarity', 'color', 'cut', 'carat', 'depth_percentage', 'depth', 'compactness', 'depth_to_table_ratio']\n",
    "    \n",
    "    # for df in [X_train, X_test]:\n",
    "        df = add_volume_ratio(df)\n",
    "        # df = feature_engineering(df)\n",
    "        \n",
    "        # df.fillna(0, inplace=True)\n",
    "        # df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        # df.dropna(inplace=True)\n",
    "        # df.drop([col for col in df.columns if col not in selected_cols], axis=1, inplace=True)\n",
    "        \n",
    "    # Scaling\n",
    "    # scaler = PowerTransformer()\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    # Clustering features\n",
    "    # model = KMeans(n_clusters=20, random_state=42)\n",
    "    # X_train['cluster'] = model.fit_predict(X_train)\n",
    "    # X_test['cluster'] = model.predict(X_test)\n",
    "    \n",
    "    return X_train, X_test\n",
    "            \n",
    "data_prep_has_fit_method = False\n",
    "\n",
    "if not data_prep_has_fit_method:\n",
    "    X_train, X_test = data_prepation(X_train, X_test)\n",
    "    X_train_prep, X_test_prep = X_train.copy(), X_test.copy()\n",
    "else:\n",
    "    X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "\n",
    "   \n",
    "# X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "# pd.DataFrame(X_train_prep.isna().sum(), columns=['train']).join(pd.DataFrame(X_test_prep.isna().sum(), columns=['test']))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:13,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor1: 581.4480 ± 0.9158, Time: 16.62 seconds, RMSE: 569.9828\n",
      "                fold0  fold1  fold2  fold3  fold4   mean\n",
      "clarity           724    727    724    721    715  722.2\n",
      "color             575    573    578    595    584  581.0\n",
      "carat             321    353    328    335    331  333.6\n",
      "y                 237    211    228    228    218  224.4\n",
      "volume_ratio10    165    171    168    168    148  164.0\n",
      "z                 169    174    159    145    154  160.2\n",
      "x                 153    136    149    145    163  149.2\n",
      "volume_ratio6     107    105    103    104    121  108.0\n",
      "cut               100    108    100     96    103  101.4\n",
      "depth             102     91     91     91     81   91.2\n",
      "table              81     85     88     80     75   81.8\n",
      "volume_ratio8      58     62     55     52     62   57.8\n",
      "volume_ratio9      48     57     49     51     48   50.6\n",
      "volume_ratio5      31     38     37     40     33   35.8\n",
      "volume_ratio4      37     29     35     35     41   35.4\n",
      "volume_ratio3      36     27     31     33     48   35.0\n",
      "volume_ratio2      32     24     32     35     33   31.2\n",
      "volume_ratio1      18     14     30     39     22   24.6\n",
      "volume_ratio7       6     15     15      7     20   12.6\n",
      "volume_ratio11      0      0      0      0      0    0.0\n",
      "volume_ratio13      0      0      0      0      0    0.0\n",
      "volume_ratio14      0      0      0      0      0    0.0\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Set categorical features for catboost\n",
    "cat_features = [col for col in X_train_prep.columns if X_train_prep[col].dtype == 'category']\n",
    "\n",
    "regressors = {\n",
    "    'LGBMRegressor1': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt'),\n",
    "    # 'LGBMRegressor2': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart'),\n",
    "    # 'LGBMRegressor3': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='goss'),\n",
    "    # 'LGBMRegressor4': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1),\n",
    "    # 'LGBMRegressor5': LGBMRegressor(random_state=SEED, n_jobs=-1, class_weight='balanced'),\n",
    "    # 'LGBMRegressor6': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'LGBMRegressor7': LGBMRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor8': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor9': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart', colsample_bytree=0.7),\n",
    "    # 'XGBRegressor1': XGBRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'XGBRegressor2': XGBRegressor(random_state=SEED, n_jobs=-1, booster='dart'),\n",
    "    # 'XGBRegressor3': XGBRegressor(random_state=SEED, n_jobs=-1, booster='gblinear'),\n",
    "    # 'XGBRegressor4': XGBRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'XGBRegressor5': XGBRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'XGBRFRegressor6': XGBRegressor(random_state=SEED, n_jobs=-1, objective='reg:squarederror'),\n",
    "    # 'XGBRandomForestRegressor': XGBRFRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'CatBoostRegressor': CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features), # Promising but fails on the cv\n",
    "    # 'RandomForestRegressor': RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'ExtraTreesRegressor': ExtraTreesRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'AdaBoostRegressor': AdaBoostRegressor(random_state=SEED),\n",
    "    # 'GradientBoostingRegressor': GradientBoostingRegressor(random_state=SEED),\n",
    "    # 'BaggingRegressor': BaggingRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    # 'DecisionTreeRegressor': DecisionTreeRegressor(random_state=SEED),\n",
    "    # 'GaussianProcessRegressor': GaussianProcessRegressor(random_state=SEED),\n",
    "    # 'MLPRegressor1': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='adam'),\n",
    "    # 'MLPRegressor2': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='lbfgs'), # promising but long to train\n",
    "    # 'MLPRegressor3': MLPRegressor(random_state=SEED, max_iter=5000, activation='tanh', solver='adam'),\n",
    "    # 'MLPRegressor4': MLPRegressor(random_state=SEED, max_iter=1000, activation='tanh', solver='lbfgs'),  # promising but long to train\n",
    "    # 'MLPRegressor5': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='adam'),\n",
    "    # 'MLPRegressor6': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='lbfgs'),\n",
    "    # 'MLPRegressor7': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='adam'),\n",
    "    # 'MLPRegressor8': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='lbfgs'),\n",
    "    # 'Ridge': Ridge(random_state=SEED),\n",
    "    # 'SGDRegressor': SGDRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'PassiveAggressiveRegressor': PassiveAggressiveRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'Perceptron': Perceptron(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'LinearRegression': LinearRegression(),\n",
    "    # 'Lasso': Lasso(random_state=SEED),\n",
    "    # 'ElasticNet': ElasticNet(random_state=SEED, max_iter=1e6),\n",
    "    # 'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    # 'BayesianRidge': BayesianRidge(),\n",
    "    # 'ARDRegression': ARDRegression(),\n",
    "    # 'TheilSenRegressor': TheilSenRegressor(random_state=SEED),\n",
    "    # 'RANSACRegressor': RANSACRegressor(random_state=SEED),\n",
    "    # 'OrthogonalMatchingPursuit': OrthogonalMatchingPursuit(normalize=False),\n",
    "    # 'Lars': Lars(),\n",
    "    # 'LassoLars': LassoLars(),\n",
    "    # 'LassoLarsIC': LassoLarsIC(normalize=False),\n",
    "    # 'StackingRegressor': StackingRegressor(\n",
    "    #         estimators=[\n",
    "    #             ('LGBMRandomForestRegressor', LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1)),\n",
    "    #             ('XGBRandomForestRegressor', XGBRFRegressor(random_state=SEED, n_jobs=-1)),\n",
    "    #             ('RandomForestRegressor', RandomForestRegressor(random_state=SEED, n_jobs=-1)),\n",
    "    #             # ('ExtraTreesRegressor', ExtraTreesRegressor(random_state=SEED, n_jobs=-1))\n",
    "    #             ], \n",
    "    #         final_estimator=Ridge(random_state=SEED),\n",
    "    #         # cv=cv,\n",
    "    #         # n_jobs=-1,\n",
    "    #         )\n",
    "}\n",
    "\n",
    "for model_name, regressor in regressors.items():\n",
    "    t0 = time.time()\n",
    "    scores = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "    ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(cv.split(X_train))):\n",
    "        \n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index].copy(), X_train.iloc[test_index].copy()\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "        \n",
    "        if data_prep_has_fit_method:\n",
    "            X_train_cv, X_test_cv = data_prepation(X_train_cv, X_test_cv)\n",
    "        \n",
    "        ttr.fit(X_train_cv, y_train_cv)        \n",
    "        y_pred = ttr.predict(X_test_cv)\n",
    "        score_eval = mean_squared_error(y_test_cv, y_pred, squared=False)\n",
    "        scores.append(score_eval)\n",
    "        \n",
    "        try:\n",
    "            feature_importance = pd.Series(ttr.regressor_.feature_importances_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "        except:\n",
    "            feature_importance = pd.Series(ttr.regressor_.coef_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "        feature_importances = pd.concat([feature_importances, feature_importance], axis=1)\n",
    "    \n",
    "    feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "    \n",
    "    ttr.fit(X_train_prep, y_train)\n",
    "    y_pred = ttr.predict(X_test_prep)\n",
    "    \n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(f'{model_name}: {np.mean(scores):.4f} ± {np.std(scores):.4f}, Time: {time.time() - t0:.2f} seconds, RMSE: {score_eval:.4f}')\n",
    "    print(feature_importances.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 2/13 [00:43<04:14, 23.12s/it]"
     ]
    }
   ],
   "source": [
    "# Keep base columns and add one feature engineering at a time\n",
    "regressor = LGBMRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=.632, subsample=.632, subsample_freq=1)\n",
    "ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "volume_ratio_cols = [col for col in X_train.columns if 'volume_ratio' in col]\n",
    "\n",
    "selected_cols = list(base_cols)\n",
    "len_cols = len(volume_ratio_cols)\n",
    "\n",
    "# drop one feature at a time based on feature importance\n",
    "for _ in range(len_cols):\n",
    "    t0 = time.time()\n",
    "    best_scores = np.inf\n",
    "    best_col = None\n",
    "    for col in tqdm(volume_ratio_cols):\n",
    "        scores = []        \n",
    "        for i, (train_index, test_index) in enumerate(cv.split(X_train)):\n",
    "\n",
    "            X_train_cv, X_test_cv = X_train.iloc[train_index][selected_cols].copy(), X_train.iloc[test_index][selected_cols].copy()\n",
    "            y_train_cv, y_test_cv = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "            \n",
    "            if data_prep_has_fit_method:\n",
    "                X_train_cv, X_test_cv = data_prepation(X_train_cv, X_test_cv)\n",
    "            \n",
    "            ttr.fit(X_train_cv, y_train_cv)        \n",
    "            y_pred = ttr.predict(X_test_cv)\n",
    "            scores.append(mean_squared_error(y_test_cv, y_pred, squared=False))\n",
    "\n",
    "        if np.mean(scores) < np.mean(best_scores):\n",
    "            best_scores = scores\n",
    "            best_col = col\n",
    "    \n",
    "    selected_cols.append(best_col)\n",
    "    volume_ratio_cols.remove(best_col)\n",
    "            \n",
    "    ttr.fit(X_train_prep[selected_cols], y_train)\n",
    "    y_pred = ttr.predict(X_test_prep[selected_cols])\n",
    "\n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(f'{model_name}: {np.mean(best_scores):.4f} ± {np.std(best_scores):.4f}, Time: {time.time() - t0:.2f} seconds, RMSE: {score_eval:.4f}, Added: {best_col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_test = test_predictions.mean(axis=1).round().astype(int)\n",
    "\n",
    "# sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "# sub['quality'] = y_pred_test\n",
    "# now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "# sub.to_csv(f'submissions/submission{now}.csv', index=False)\n",
    "# # Copy the leaked values from the original dataset before submitting\n",
    "# # Transform the price column back to the original scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f90af0c099b2a3322334c0593a59f872710278483b6e7b3217af559be1bbf34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
