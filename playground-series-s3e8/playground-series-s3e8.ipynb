{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>62.2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>7.27</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.03</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.06</td>\n",
       "      <td>8.12</td>\n",
       "      <td>5.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>61.2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>61.6</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>4.41</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.70</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>62.6</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7.65</td>\n",
       "      <td>7.61</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220535</th>\n",
       "      <td>1.11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>62.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.52</td>\n",
       "      <td>4.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220536</th>\n",
       "      <td>0.33</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>61.9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220537</th>\n",
       "      <td>0.51</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>61.7</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.12</td>\n",
       "      <td>5.15</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220538</th>\n",
       "      <td>0.27</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>61.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220539</th>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.90</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220540 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        carat  cut  color  clarity  depth  table     x     y     z\n",
       "0        1.52    3      4        3   62.2   58.0  7.27  7.33  4.55\n",
       "1        2.03    2      0        1   62.0   58.0  8.06  8.12  5.05\n",
       "2        0.70    4      3        4   61.2   57.0  5.69  5.73  3.50\n",
       "3        0.32    4      3        4   61.6   56.0  4.38  4.41  2.71\n",
       "4        1.70    3      3        3   62.6   59.0  7.65  7.61  4.77\n",
       "...       ...  ...    ...      ...    ...    ...   ...   ...   ...\n",
       "220535   1.11    3      3        2   62.3   58.0  6.61  6.52  4.09\n",
       "220536   0.33    4      2        7   61.9   55.0  4.44  4.42  2.74\n",
       "220537   0.51    3      5        3   61.7   58.0  5.12  5.15  3.17\n",
       "220538   0.27    2      4        5   61.8   56.0  4.19  4.20  2.60\n",
       "220539   1.25    3      0        2   62.0   58.0  6.90  6.88  4.27\n",
       "\n",
       "[220540 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch, MeanShift, SpectralClustering, AffinityPropagation, FeatureAgglomeration\n",
    "\n",
    "# import regressors\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, PassiveAggressiveRegressor, Perceptron, RidgeClassifier, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Lars, BayesianRidge, ARDRegression, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor, RANSACRegressor\n",
    "from sklearn.linear_model import LassoLars, LassoLarsIC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# pandas deactivate future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "SUBMIT = True\n",
    "USE_ORIGINAL = True\n",
    "SEED = 15\n",
    "SAMPLE = 1\n",
    "\n",
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')\n",
    "orig = pd.read_csv('datasets/cubic_zirconia.csv')\n",
    "\n",
    "for i, df in enumerate([train, test, orig]):\n",
    "    df.drop(['id'], axis=1, inplace=True)\n",
    "    if not SUBMIT:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    # df['dataset'] = i\n",
    "\n",
    "# Define test set\n",
    "if not SUBMIT:\n",
    "    train, test = train_test_split(train, test_size=0.2, random_state=SEED) \n",
    "\n",
    "if USE_ORIGINAL:\n",
    "    train = pd.concat([train, orig], axis=0)\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Sampling for faster training\n",
    "if SAMPLE < 1:\n",
    "    train = train.sample(frac=SAMPLE, random_state=SEED)\n",
    "\n",
    "del orig\n",
    "\n",
    "# set training data\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('price')\n",
    "X_test = test.copy()\n",
    "\n",
    "if not SUBMIT:\n",
    "    y_test = X_test.pop('price')\n",
    "else:\n",
    "    y_test = None\n",
    "    \n",
    "base_cols = X_train.columns\n",
    "\n",
    "# transform categorical features\n",
    "def transform_categorical(df):\n",
    "    df['cut'] = df['cut'].map({'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4})\n",
    "    df['color'] = df['color'].map({'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D': 6})\n",
    "    df['clarity'] = df['clarity'].map({'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7})\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Drop extreme values\n",
    "    min = 2\n",
    "    max = 20\n",
    "    df = df[(df['x'] < max) & (df['y'] < max) & (df['z'] < max)]\n",
    "    df = df[(df['x'] > min) & (df['y'] > min) & (df['z'] > min)]\n",
    "    return df\n",
    "\n",
    "def add_volume_ratio(df):\n",
    "    # df['volume_ratio1'] = (df['x'] * df['y']) / (df['z'] * df['z'])\n",
    "    df['volume_ratio2'] = (df['x'] * df['z']) / (df['y'] * df['y'])\n",
    "    df['volume_ratio3'] = (df['y'] * df['z']) / (df['x'] * df['x'])\n",
    "    # df['volume_ratio4'] = (df['x']) / (df['z'])\n",
    "    # df['volume_ratio5'] = (df['y']) / (df['z'])\n",
    "    df['volume_ratio6'] = (df['x'] * df['z']) / (df['y'] * df['z'])  # will set nan if z is nan\n",
    "    # df['volume_ratio7'] = (df['x'] + df['y']) / df['z']\n",
    "    # df['volume_ratio8'] = (df['x'] + df['z']) / df['y']\n",
    "    # df['volume_ratio9'] = (df['y'] + df['z']) / df['x']\n",
    "    # df['volume_ratio10'] = (df['x'] * df['y'] * df['z']) / (df['x'].mean() * df['y'].mean() * df['z'].mean())\n",
    "    # df['volume_ratio11'] = (df['x'] * df['y'] * df['z']) / (df['x'].max() * df['y'].max() * df['z'].max())\n",
    "    # df['volume_ratio12'] = (df['x'] * df['y'] * df['z']) / (df['x'].min() * df['y'].min() * df['z'].min())\n",
    "    # df['volume_ratio13'] = (df['x'] * df['y'] * df['z']) / (df['x'].median() * df['y'].median() * df['z'].median())\n",
    "    # df['volume_ratio14'] = (df['x'] * df['y'] * df['z']) / (df['x'].std() * df['y'].std() * df['z'].std())\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df[\"volume\"] = df[\"x\"] * df[\"y\"] * df[\"z\"]\n",
    "    df[\"surface_area\"] = 2 * (df[\"x\"] * df[\"y\"] + df[\"y\"] * df[\"z\"] + df[\"z\"] * df[\"x\"])\n",
    "    df[\"aspect_ratio_xy\"] = df[\"x\"] / df[\"y\"]\n",
    "    df[\"aspect_ratio_yz\"] = df[\"y\"] / df[\"z\"]\n",
    "    df[\"aspect_ratio_zx\"] = df[\"z\"] / df[\"x\"]\n",
    "    df[\"diagonal_distance\"] = np.sqrt(df[\"x\"] ** 2 + df[\"y\"] ** 2 + df[\"z\"] ** 2)\n",
    "    # df[\"relative_height\"] = (df[\"z\"] - df[\"z\"].min()) / (df[\"z\"].max() - df[\"z\"].min())\n",
    "    # df[\"relative_position\"] = (df[\"x\"] + df[\"y\"] + df[\"z\"]) / (df[\"x\"] + df[\"y\"] + df[\"z\"]).sum()\n",
    "    # df[\"volume_ratio\"] = df[\"x\"] * df[\"y\"] * df[\"z\"] / (df[\"x\"].mean() * df[\"y\"].mean() * df[\"z\"].mean())\n",
    "    # df[\"length_ratio\"] = df[\"x\"] / df[\"x\"].mean()\n",
    "    # df[\"width_ratio\"] = df[\"y\"] / df[\"y\"].mean()\n",
    "    # df[\"height_ratio\"] = df[\"z\"] / df[\"z\"].mean()\n",
    "    df[\"sphericity\"] = 1.4641 * (6 * df[\"volume\"])**(2/3) / df[\"surface_area\"]\n",
    "    df[\"compactness\"] = df[\"volume\"]**(1/3) / df[\"x\"]\n",
    "    df['density'] = df['carat'] / df['volume']\n",
    "    df['table_percentage'] = (df['table'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['depth_percentage'] = (df['depth'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['symmetry'] = (abs(df['x'] - df['z']) + abs(df['y'] - df['z'])) / (df['x'] + df['y'] + df['z'])\n",
    "    df['surface_area'] = 2 * ((df['x'] * df['y']) + (df['x'] * df['z']) + (df['y'] * df['z']))\n",
    "    df['depth_to_table_ratio'] = df['depth'] / df['table']\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def target_transform(serie):\n",
    "    serie = np.log1p(serie)\n",
    "    return serie\n",
    "\n",
    "def inverse_target_transform(serie):\n",
    "    serie = np.expm1(serie)\n",
    "    return serie\n",
    "\n",
    "def set_categorical(df):\n",
    "    df['cut'] = df['cut'].astype('category')\n",
    "    df['color'] = df['color'].astype('category')\n",
    "    df['clarity'] = df['clarity'].astype('category')\n",
    "    return df\n",
    "\n",
    "def add_girdle_parameters(df):\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def impute_x_y_z(df):\n",
    "    df['is_imputed'] = df.isna().any(axis=1).astype(int)\n",
    "    df['girdle_diameter'].fillna((df['x'] + df['y']) / 2, inplace=True)\n",
    "    df['x'].fillna(2*df['girdle_diameter'] - df['y'], inplace=True)\n",
    "    df['y'].fillna(2*df['girdle_diameter'] - df['x'], inplace=True)\n",
    "    df['z'].fillna(df['girdle_diameter'] * df['depth'] / 100, inplace=True)\n",
    "    df = add_girdle_parameters(df)\n",
    "    return df\n",
    "\n",
    "def set_nan(df):\n",
    "    for col in ['x', 'y', 'z']:\n",
    "        df[col].replace(0, np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def drop_girdle_parameters(df):\n",
    "    df.drop(['girdle_diameter', 'girdle_thickness', 'girdle_ratio'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Make data preparation pipeline\n",
    "def data_prepation(X_train, X_test):\n",
    "    \n",
    "    for df in [X_train, X_test]:\n",
    "        # df = set_nan(df)\n",
    "        df = transform_categorical(df)\n",
    "        # df = set_categorical(df)\n",
    "        # df = add_girdle_parameters(df)\n",
    "        # df = impute_x_y_z(df)\n",
    "        # df = drop_girdle_parameters(df)\n",
    "        \n",
    "    \n",
    "    # imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    # imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    # imputer = KNNImputer(n_neighbors=1, weights=\"uniform\")\n",
    "    # X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    # X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # selected_cols = base_cols\n",
    "    # selected_cols = ['surface_area', 'clarity', 'color', 'cut', 'carat', 'depth_percentage', 'depth', 'compactness', 'depth_to_table_ratio']\n",
    "    \n",
    "    # for df in [X_train, X_test]:\n",
    "    #     df = add_volume_ratio(df)\n",
    "        # df = feature_engineering(df)\n",
    "        \n",
    "        # df.fillna(0, inplace=True)\n",
    "        # df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        # df.dropna(inplace=True)\n",
    "        # df.drop([col for col in df.columns if col not in selected_cols], axis=1, inplace=True)\n",
    "        \n",
    "    # Scaling\n",
    "    # scaler = PowerTransformer()\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    # Clustering features\n",
    "    # model = KMeans(n_clusters=20, random_state=42)\n",
    "    # X_train['cluster'] = model.fit_predict(X_train)\n",
    "    # X_test['cluster'] = model.predict(X_test)\n",
    "    \n",
    "    return X_train, X_test\n",
    "            \n",
    "data_prep_has_fit_method = False\n",
    "\n",
    "if not data_prep_has_fit_method:\n",
    "    X_train, X_test = data_prepation(X_train, X_test)\n",
    "    X_train_prep, X_test_prep = X_train.copy(), X_test.copy()\n",
    "else:\n",
    "    X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "\n",
    "   \n",
    "# X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "# pd.DataFrame(X_train_prep.isna().sum(), columns=['train']).join(pd.DataFrame(X_test_prep.isna().sum(), columns=['test']))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   18.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.0min finished\n",
      "c:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "c:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\numpy\\core\\_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SUBMIT:\n\u001b[0;32m    119\u001b[0m     score_eval \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred, squared\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(scores)\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m ± \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mstd(scores)\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Time: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds, RMSE: \u001b[39m\u001b[39m{\u001b[39;00mscore_eval\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[39mprint\u001b[39m(feature_importances\u001b[39m.\u001b[39msort_values(\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'score_eval' is not defined"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Set categorical features for catboost\n",
    "cat_features = [col for col in X_train_prep.columns if X_train_prep[col].dtype == 'category']\n",
    "\n",
    "regressors = {\n",
    "    # 'LGBMRegressor1': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt'),\n",
    "    # 'LGBMRegressor2': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart'),\n",
    "    # 'LGBMRegressor3': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='goss'),\n",
    "    # 'LGBMRegressor4': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1),\n",
    "    # 'LGBMRegressor5': LGBMRegressor(random_state=SEED, n_jobs=-1, class_weight='balanced'),\n",
    "    # 'LGBMRegressor6': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'LGBMRegressor7': LGBMRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor8': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor9': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart', colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor10': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240),\n",
    "    # 'LGBMRegressor11': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6),\n",
    "    # 'XGBRegressor1': XGBRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'XGBRegressor2': XGBRegressor(random_state=SEED, n_jobs=-1, booster='dart'),\n",
    "    # 'XGBRegressor3': XGBRegressor(random_state=SEED, n_jobs=-1, booster='gblinear'),\n",
    "    # 'XGBRegressor4': XGBRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'XGBRegressor5': XGBRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'XGBRegressor6': XGBRegressor(random_state=SEED, \n",
    "    #                               n_jobs=-1, \n",
    "    #                               learning_rate=0.055, \n",
    "    #                               n_estimators=200, \n",
    "    #                               max_depth=8, \n",
    "    #                               min_child_weight=1, \n",
    "    #                               gamma=0.07, \n",
    "    #                               colsample_bytree=0.67, \n",
    "    #                               colsample_bylevel=0.67, \n",
    "    #                               colsample_bynode=0.8,\n",
    "    #                               subsample=0.7, \n",
    "    #                               objective='reg:squarederror'),\n",
    "    # 'XGBRFRegressor6': XGBRegressor(random_state=SEED, n_jobs=-1, objective='reg:squarederror'),\n",
    "    # 'XGBRandomForestRegressor': XGBRFRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'CatBoostRegressor': CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features), # Promising but fails on the cv\n",
    "    # 'RandomForestRegressor': RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'ExtraTreesRegressor': ExtraTreesRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'AdaBoostRegressor': AdaBoostRegressor(random_state=SEED),\n",
    "    # 'GradientBoostingRegressor': GradientBoostingRegressor(random_state=SEED),\n",
    "    # 'BaggingRegressor': BaggingRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    # 'DecisionTreeRegressor': DecisionTreeRegressor(random_state=SEED),\n",
    "    # 'GaussianProcessRegressor': GaussianProcessRegressor(random_state=SEED),\n",
    "    # 'MLPRegressor1': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='adam'),\n",
    "    # 'MLPRegressor2': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='lbfgs'), # promising but long to train\n",
    "    # 'MLPRegressor3': MLPRegressor(random_state=SEED, max_iter=5000, activation='tanh', solver='adam'),\n",
    "    # 'MLPRegressor4': MLPRegressor(random_state=SEED, max_iter=1000, activation='tanh', solver='lbfgs'),  # promising but long to train\n",
    "    # 'MLPRegressor5': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='adam'),\n",
    "    # 'MLPRegressor6': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='lbfgs'),\n",
    "    # 'MLPRegressor7': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='adam'),\n",
    "    # 'MLPRegressor8': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='lbfgs'),\n",
    "    # 'Ridge': Ridge(random_state=SEED),\n",
    "    # 'SGDRegressor': SGDRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'PassiveAggressiveRegressor': PassiveAggressiveRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'Perceptron': Perceptron(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'LinearRegression': LinearRegression(),\n",
    "    # 'Lasso': Lasso(random_state=SEED),\n",
    "    # 'ElasticNet': ElasticNet(random_state=SEED, max_iter=1e6),\n",
    "    # 'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    # 'BayesianRidge': BayesianRidge(),\n",
    "    # 'ARDRegression': ARDRegression(),\n",
    "    # 'TheilSenRegressor': TheilSenRegressor(random_state=SEED),\n",
    "    # 'RANSACRegressor': RANSACRegressor(random_state=SEED),\n",
    "    # 'OrthogonalMatchingPursuit': OrthogonalMatchingPursuit(normalize=False),\n",
    "    # 'Lars': Lars(),\n",
    "    # 'LassoLars': LassoLars(),\n",
    "    # 'LassoLarsIC': LassoLarsIC(normalize=False),\n",
    "    'StackingRegressor': StackingRegressor(\n",
    "            estimators=[\n",
    "                ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, \n",
    "                                                max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "                ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                            max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                            colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                            objective='reg:squarederror')),\n",
    "                ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features)), # Promising but fails on the cv\n",
    "                # ('ExtraTreesRegressor', ExtraTreesRegressor(random_state=SEED, n_jobs=-1))\n",
    "                ], \n",
    "            final_estimator=Ridge(random_state=SEED),\n",
    "            cv=cv,\n",
    "            # n_jobs=-1,\n",
    "            verbose=1\n",
    "            )\n",
    "}\n",
    "\n",
    "for model_name, regressor in regressors.items():\n",
    "    t0 = time.time()\n",
    "    scores = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "    ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "    \n",
    "    # for i, (train_index, test_index) in tqdm(enumerate(cv.split(X_train))):\n",
    "        \n",
    "    #     X_train_cv, X_test_cv = X_train.iloc[train_index].copy(), X_train.iloc[test_index].copy()\n",
    "    #     y_train_cv, y_test_cv = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "        \n",
    "    #     if data_prep_has_fit_method:\n",
    "    #         X_train_cv, X_test_cv = data_prepation(X_train_cv, X_test_cv)\n",
    "        \n",
    "    #     ttr.fit(X_train_cv, y_train_cv)        \n",
    "    #     y_pred = ttr.predict(X_test_cv)\n",
    "    #     score_eval = mean_squared_error(y_test_cv, y_pred, squared=False)\n",
    "    #     scores.append(score_eval)\n",
    "        \n",
    "    #     try:\n",
    "    #         feature_importance = pd.Series(ttr.regressor_.feature_importances_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "    #     except:\n",
    "    #         feature_importance = pd.Series(ttr.regressor_.coef_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "    #     feature_importances = pd.concat([feature_importances, feature_importance], axis=1)\n",
    "    \n",
    "    feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "    \n",
    "    ttr.fit(X_train_prep, y_train)\n",
    "    y_pred = ttr.predict(X_test_prep)\n",
    "    \n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(f'{model_name}: {np.mean(scores):.4f} ± {np.std(scores):.4f}, Time: {time.time() - t0:.2f} seconds, RMSE: {score_eval:.4f}')\n",
    "    print(feature_importances.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.35</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.3</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.54</td>\n",
       "      <td>2.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.77</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.71</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>61.9</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.77</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.33</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>61.6</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.20</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129045</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.78</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129046</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Premium</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.6</td>\n",
       "      <td>62.0</td>\n",
       "      <td>5.77</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129047</th>\n",
       "      <td>1.01</td>\n",
       "      <td>Premium</td>\n",
       "      <td>G</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>62.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>6.41</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129048</th>\n",
       "      <td>1.35</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>I1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>7.05</td>\n",
       "      <td>7.08</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129049</th>\n",
       "      <td>1.07</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128515 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        carat        cut color clarity  depth  table     x     y     z\n",
       "0        0.35      Ideal     D     VS2   62.3   56.0  4.51  4.54  2.82\n",
       "1        0.77  Very Good     F     SI2   62.8   56.0  5.83  5.87  3.68\n",
       "2        0.71      Ideal     I     VS2   61.9   53.0  5.77  5.74  3.55\n",
       "3        0.33      Ideal     G    VVS2   61.6   55.0  4.44  4.42  2.73\n",
       "4        1.20  Very Good     I     VS2   62.7   56.0  6.75  6.79  4.24\n",
       "...       ...        ...   ...     ...    ...    ...   ...   ...   ...\n",
       "129045   0.72      Ideal     D    VVS2   62.0   56.0  5.75  5.78  3.57\n",
       "129046   0.70    Premium     D     SI1   59.6   62.0  5.77  5.74  3.43\n",
       "129047   1.01    Premium     G    VVS2   62.3   58.0  6.44  6.41  4.01\n",
       "129048   1.35      Ideal     D      I1   62.0   56.0  7.05  7.08  4.38\n",
       "129049   1.07    Premium     H     SI2   62.6   60.0  6.49  6.45  4.06\n",
       "\n",
       "[128515 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (128515) does not match length of index (129050)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m y_pred_test \u001b[39m=\u001b[39m y_pred\n\u001b[0;32m      3\u001b[0m sub \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39msubmissions/sample_submission.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sub[\u001b[39m'\u001b[39m\u001b[39mquality\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m y_pred_test\n\u001b[0;32m      5\u001b[0m now \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m sub\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msubmissions/submission\u001b[39m\u001b[39m{\u001b[39;00mnow\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4163\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4164\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4170\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   4174\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   4175\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   4177\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   4178\u001b[0m     ):\n\u001b[0;32m   4179\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\frame.py:4912\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4909\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m   4911\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4912\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4913\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (128515) does not match length of index (129050)"
     ]
    }
   ],
   "source": [
    "y_pred_test = y_pred\n",
    "\n",
    "sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "sub['quality'] = y_pred_test\n",
    "now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "sub.to_csv(f'submissions/submission{now}.csv', index=False)\n",
    "# Copy the leaked values from the original dataset before submitting\n",
    "# Transform the price column back to the original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cross_val_score(estimator, X, y, cv):\n",
    "    scores = []\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X)):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "        estimator.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = estimator.predict(X_test_cv)\n",
    "        score_eval = mean_squared_error(y_test_cv, y_pred, squared=False)\n",
    "        scores.append(score_eval)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   22.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   12.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   12.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   26.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   22.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "regressors = [\n",
    "    ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                  max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                  objective='reg:squarederror')),\n",
    "    ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features)), # Promising but fails on the cv\n",
    "]\n",
    "\n",
    "model = StackingRegressor(\n",
    "    estimators=regressors,\n",
    "    final_estimator=Ridge(random_state=SEED),\n",
    "    # cv=cv,\n",
    "    # n_jobs=-1,\n",
    "    verbose=1,\n",
    "    )\n",
    "scores = my_cross_val_score(model, X_train_prep, y_train, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[572.5012925802354,\n",
       " 566.5213828457635,\n",
       " 571.0245441375674,\n",
       " 572.0435810182444,\n",
       " 566.4953768648442]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569.717235489331"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f90af0c099b2a3322334c0593a59f872710278483b6e7b3217af559be1bbf34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
