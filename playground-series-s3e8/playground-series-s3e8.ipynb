{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch, MeanShift, SpectralClustering, AffinityPropagation, FeatureAgglomeration\n",
    "\n",
    "# import regressors\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, StackingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, PassiveAggressiveRegressor, Perceptron, RidgeClassifier, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Lars, BayesianRidge, ARDRegression, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor, RANSACRegressor\n",
    "from sklearn.linear_model import LassoLars, LassoLarsIC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV, LarsCV, OrthogonalMatchingPursuitCV, LassoLarsCV, BayesianRidge, LinearRegression\n",
    "\n",
    "# pandas deactivate future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "SUBMIT = True\n",
    "USE_ORIGINAL = True\n",
    "SEED = 15\n",
    "SAMPLE = 1\n",
    "\n",
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')\n",
    "orig = pd.read_csv('datasets/cubic_zirconia.csv')\n",
    "\n",
    "for i, df in enumerate([train, test, orig]):\n",
    "    df.drop(['id'], axis=1, inplace=True)\n",
    "    if not SUBMIT:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    # df['dataset'] = i\n",
    "\n",
    "# Define test set\n",
    "if not SUBMIT:\n",
    "    train, test = train_test_split(train, test_size=0.2, random_state=SEED) \n",
    "\n",
    "if USE_ORIGINAL:\n",
    "    train = pd.concat([train, orig], axis=0)\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Sampling for faster training\n",
    "if SAMPLE < 1:\n",
    "    train = train.sample(frac=SAMPLE, random_state=SEED)\n",
    "\n",
    "del orig\n",
    "\n",
    "# set training data\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('price')\n",
    "X_test = test.copy()\n",
    "\n",
    "if not SUBMIT:\n",
    "    y_test = X_test.pop('price')\n",
    "else:\n",
    "    y_test = None\n",
    "    \n",
    "base_cols = X_train.columns\n",
    "\n",
    "# transform categorical features\n",
    "def transform_categorical(df):\n",
    "    df['cut'] = df['cut'].map({'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4})\n",
    "    df['color'] = df['color'].map({'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D': 6})\n",
    "    df['clarity'] = df['clarity'].map({'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7})\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Drop extreme values\n",
    "    min = 2\n",
    "    max = 20\n",
    "    df = df[(df['x'] < max) & (df['y'] < max) & (df['z'] < max)]\n",
    "    df = df[(df['x'] > min) & (df['y'] > min) & (df['z'] > min)]\n",
    "    return df\n",
    "\n",
    "def add_volume_ratio(df):\n",
    "    # df['volume_ratio1'] = (df['x'] * df['y']) / (df['z'] * df['z'])\n",
    "    df['volume_ratio2'] = (df['x'] * df['z']) / (df['y'] * df['y'])\n",
    "    df['volume_ratio3'] = (df['y'] * df['z']) / (df['x'] * df['x'])\n",
    "    # df['volume_ratio4'] = (df['x']) / (df['z'])\n",
    "    # df['volume_ratio5'] = (df['y']) / (df['z'])\n",
    "    df['volume_ratio6'] = (df['x'] * df['z']) / (df['y'] * df['z'])  # will set nan if z is nan\n",
    "    # df['volume_ratio7'] = (df['x'] + df['y']) / df['z']\n",
    "    # df['volume_ratio8'] = (df['x'] + df['z']) / df['y']\n",
    "    # df['volume_ratio9'] = (df['y'] + df['z']) / df['x']\n",
    "    # df['volume_ratio10'] = (df['x'] * df['y'] * df['z']) / (df['x'].mean() * df['y'].mean() * df['z'].mean())\n",
    "    # df['volume_ratio11'] = (df['x'] * df['y'] * df['z']) / (df['x'].max() * df['y'].max() * df['z'].max())\n",
    "    # df['volume_ratio12'] = (df['x'] * df['y'] * df['z']) / (df['x'].min() * df['y'].min() * df['z'].min())\n",
    "    # df['volume_ratio13'] = (df['x'] * df['y'] * df['z']) / (df['x'].median() * df['y'].median() * df['z'].median())\n",
    "    # df['volume_ratio14'] = (df['x'] * df['y'] * df['z']) / (df['x'].std() * df['y'].std() * df['z'].std())\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df[\"volume\"] = df[\"x\"] * df[\"y\"] * df[\"z\"]\n",
    "    df[\"surface_area\"] = 2 * (df[\"x\"] * df[\"y\"] + df[\"y\"] * df[\"z\"] + df[\"z\"] * df[\"x\"])\n",
    "    df[\"aspect_ratio_xy\"] = df[\"x\"] / df[\"y\"]\n",
    "    df[\"aspect_ratio_yz\"] = df[\"y\"] / df[\"z\"]\n",
    "    df[\"aspect_ratio_zx\"] = df[\"z\"] / df[\"x\"]\n",
    "    df[\"diagonal_distance\"] = np.sqrt(df[\"x\"] ** 2 + df[\"y\"] ** 2 + df[\"z\"] ** 2)\n",
    "    # df[\"relative_height\"] = (df[\"z\"] - df[\"z\"].min()) / (df[\"z\"].max() - df[\"z\"].min())\n",
    "    # df[\"relative_position\"] = (df[\"x\"] + df[\"y\"] + df[\"z\"]) / (df[\"x\"] + df[\"y\"] + df[\"z\"]).sum()\n",
    "    # df[\"volume_ratio\"] = df[\"x\"] * df[\"y\"] * df[\"z\"] / (df[\"x\"].mean() * df[\"y\"].mean() * df[\"z\"].mean())\n",
    "    # df[\"length_ratio\"] = df[\"x\"] / df[\"x\"].mean()\n",
    "    # df[\"width_ratio\"] = df[\"y\"] / df[\"y\"].mean()\n",
    "    # df[\"height_ratio\"] = df[\"z\"] / df[\"z\"].mean()\n",
    "    df[\"sphericity\"] = 1.4641 * (6 * df[\"volume\"])**(2/3) / df[\"surface_area\"]\n",
    "    df[\"compactness\"] = df[\"volume\"]**(1/3) / df[\"x\"]\n",
    "    df['density'] = df['carat'] / df['volume']\n",
    "    df['table_percentage'] = (df['table'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['depth_percentage'] = (df['depth'] / ((df['x'] + df['y']) / 2)) * 100\n",
    "    df['symmetry'] = (abs(df['x'] - df['z']) + abs(df['y'] - df['z'])) / (df['x'] + df['y'] + df['z'])\n",
    "    df['surface_area'] = 2 * ((df['x'] * df['y']) + (df['x'] * df['z']) + (df['y'] * df['z']))\n",
    "    df['depth_to_table_ratio'] = df['depth'] / df['table']\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def target_transform(serie):\n",
    "    serie = np.log1p(serie)\n",
    "    return serie\n",
    "\n",
    "def inverse_target_transform(serie):\n",
    "    serie = np.expm1(serie)\n",
    "    return serie\n",
    "\n",
    "def set_categorical(df):\n",
    "    df['cut'] = df['cut'].astype('category')\n",
    "    df['color'] = df['color'].astype('category')\n",
    "    df['clarity'] = df['clarity'].astype('category')\n",
    "    return df\n",
    "\n",
    "def add_girdle_parameters(df):\n",
    "    df['girdle_diameter'] = 100 * df['z'] / df['depth']\n",
    "    df['girdle_thickness'] = 100 * df['z'] / df['table']\n",
    "    df['girdle_ratio'] = df['girdle_diameter'] / df['girdle_thickness']\n",
    "    return df\n",
    "\n",
    "def impute_x_y_z(df):\n",
    "    df['is_imputed'] = df.isna().any(axis=1).astype(int)\n",
    "    df['girdle_diameter'].fillna((df['x'] + df['y']) / 2, inplace=True)\n",
    "    df['x'].fillna(2*df['girdle_diameter'] - df['y'], inplace=True)\n",
    "    df['y'].fillna(2*df['girdle_diameter'] - df['x'], inplace=True)\n",
    "    df['z'].fillna(df['girdle_diameter'] * df['depth'] / 100, inplace=True)\n",
    "    df = add_girdle_parameters(df)\n",
    "    return df\n",
    "\n",
    "def set_nan(df):\n",
    "    for col in ['x', 'y', 'z']:\n",
    "        df[col].replace(0, np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def drop_girdle_parameters(df):\n",
    "    df.drop(['girdle_diameter', 'girdle_thickness', 'girdle_ratio'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Make data preparation pipeline\n",
    "def data_prepation(X_train, X_test):\n",
    "    \n",
    "    for df in [X_train, X_test]:\n",
    "        # df = set_nan(df)\n",
    "        df = transform_categorical(df)\n",
    "        # df = set_categorical(df)\n",
    "        # df = add_girdle_parameters(df)\n",
    "        # df = impute_x_y_z(df)\n",
    "        # df = drop_girdle_parameters(df)\n",
    "        \n",
    "    \n",
    "    # imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    # imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    # imputer = KNNImputer(n_neighbors=1, weights=\"uniform\")\n",
    "    # X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    # X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # selected_cols = base_cols\n",
    "    # selected_cols = ['surface_area', 'clarity', 'color', 'cut', 'carat', 'depth_percentage', 'depth', 'compactness', 'depth_to_table_ratio']\n",
    "    \n",
    "    # for df in [X_train, X_test]:\n",
    "    #     df = add_volume_ratio(df)\n",
    "        # df = feature_engineering(df)\n",
    "        \n",
    "        # df.fillna(0, inplace=True)\n",
    "        # df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        # df.dropna(inplace=True)\n",
    "        # df.drop([col for col in df.columns if col not in selected_cols], axis=1, inplace=True)\n",
    "        \n",
    "    # Scaling\n",
    "    # scaler = PowerTransformer()\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    # X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    # Clustering features\n",
    "    # model = KMeans(n_clusters=20, random_state=42)\n",
    "    # X_train['cluster'] = model.fit_predict(X_train)\n",
    "    # X_test['cluster'] = model.predict(X_test)\n",
    "    \n",
    "    return X_train, X_test\n",
    "            \n",
    "data_prep_has_fit_method = False\n",
    "\n",
    "if not data_prep_has_fit_method:\n",
    "    X_train, X_test = data_prepation(X_train, X_test)\n",
    "    X_train_prep, X_test_prep = X_train.copy(), X_test.copy()\n",
    "else:\n",
    "    X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "\n",
    "   \n",
    "# X_train_prep, X_test_prep = data_prepation(X_train.copy(), X_test.copy())\n",
    "# pd.DataFrame(X_train_prep.isna().sum(), columns=['train']).join(pd.DataFrame(X_test_prep.isna().sum(), columns=['test']))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Set categorical features for catboost\n",
    "cat_features = [col for col in X_train_prep.columns if X_train_prep[col].dtype == 'category']\n",
    "\n",
    "regressors = {\n",
    "    'LGBMRegressor1': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt'),\n",
    "    # 'LGBMRegressor2': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart'),\n",
    "    'LGBMRegressor3': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='goss'),\n",
    "    # 'LGBMRegressor4': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1),\n",
    "    'LGBMRegressor5': LGBMRegressor(random_state=SEED, n_jobs=-1, class_weight='balanced'),\n",
    "    'LGBMRegressor6': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    'LGBMRegressor7': LGBMRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    'LGBMRegressor8': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor9': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart', colsample_bytree=0.7),\n",
    "    'LGBMRegressor10': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240),\n",
    "    'LGBMRegressor11': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6),\n",
    "    'XGBRegressor1': XGBRegressor(random_state=SEED, n_jobs=-1),\n",
    "    'XGBRegressor2': XGBRegressor(random_state=SEED, n_jobs=-1, booster='dart'),\n",
    "    'XGBRegressor3': XGBRegressor(random_state=SEED, n_jobs=-1, booster='gblinear'),\n",
    "    'XGBRegressor4': XGBRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    'XGBRegressor5': XGBRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    'XGBRegressor6': XGBRegressor(random_state=SEED, \n",
    "                                  n_jobs=-1, \n",
    "                                  learning_rate=0.055, \n",
    "                                  n_estimators=200, \n",
    "                                  max_depth=8, \n",
    "                                  min_child_weight=1, \n",
    "                                  gamma=0.07, \n",
    "                                  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, \n",
    "                                  colsample_bynode=0.8,\n",
    "                                  subsample=0.7, \n",
    "                                  objective='reg:squarederror'),\n",
    "    'XGBRegressor7': XGBRegressor(random_state=SEED, n_jobs=-1, objective='reg:squarederror'),\n",
    "    'XGBRandomForestRegressor': XGBRFRegressor(random_state=SEED, n_jobs=-1),\n",
    "    'CatBoostRegressor': CatBoostRegressor(random_state=SEED, silent=True), # Promising but fails on the cv\n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(random_state=SEED),\n",
    "    'HistGradientBoostingRegressor2': HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "                                                                    max_depth=6, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "                                                                    min_samples_leaf=9, max_bins=255),\n",
    "    'HistGradientBoostingRegressor3': HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "                                                                    max_depth=10, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.1, max_leaf_nodes=100, \n",
    "                                                                    min_samples_leaf=20, max_bins=255),\n",
    "    # 'RandomForestRegressor': RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'ExtraTreesRegressor': ExtraTreesRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'AdaBoostRegressor': AdaBoostRegressor(random_state=SEED),\n",
    "    # 'GradientBoostingRegressor': GradientBoostingRegressor(random_state=SEED),\n",
    "    # 'BaggingRegressor': BaggingRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    # 'DecisionTreeRegressor': DecisionTreeRegressor(random_state=SEED),\n",
    "    # 'GaussianProcessRegressor': GaussianProcessRegressor(random_state=SEED),\n",
    "    # 'MLPRegressor1': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='adam'),\n",
    "    # 'MLPRegressor2': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='lbfgs'), # promising but long to train\n",
    "    # 'MLPRegressor3': MLPRegressor(random_state=SEED, max_iter=5000, activation='tanh', solver='adam'),\n",
    "    # 'MLPRegressor4': MLPRegressor(random_state=SEED, max_iter=1000, activation='tanh', solver='lbfgs'),  # promising but long to train\n",
    "    # 'MLPRegressor5': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='adam'),\n",
    "    # 'MLPRegressor6': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='lbfgs'),\n",
    "    # 'MLPRegressor7': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='adam'),\n",
    "    # 'MLPRegressor8': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='lbfgs'),\n",
    "    # 'Ridge': Ridge(random_state=SEED),\n",
    "    # 'SGDRegressor': SGDRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'PassiveAggressiveRegressor': PassiveAggressiveRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'Perceptron': Perceptron(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'LinearRegression': LinearRegression(),\n",
    "    # 'Lasso': Lasso(random_state=SEED),\n",
    "    # 'ElasticNet': ElasticNet(random_state=SEED, max_iter=1e6),\n",
    "    # 'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    # 'BayesianRidge': BayesianRidge(),\n",
    "    # 'ARDRegression': ARDRegression(),\n",
    "    # 'TheilSenRegressor': TheilSenRegressor(random_state=SEED),\n",
    "    # 'RANSACRegressor': RANSACRegressor(random_state=SEED),\n",
    "    # 'OrthogonalMatchingPursuit': OrthogonalMatchingPursuit(normalize=False),\n",
    "    # 'Lars': Lars(),\n",
    "    # 'LassoLars': LassoLars(),\n",
    "    # 'LassoLarsIC': LassoLarsIC(normalize=False),\n",
    "    # 'StackingRegressor': StackingRegressor(\n",
    "    #         estimators=[\n",
    "    #             ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, \n",
    "    #                                             max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    #             ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "    #                                         max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "    #                                         colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "    #                                         objective='reg:squarederror')),\n",
    "    #             ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True, cat_features=cat_features)), # Promising but fails on the cv\n",
    "    #             # ('ExtraTreesRegressor', ExtraTreesRegressor(random_state=SEED, n_jobs=-1))\n",
    "    #             ], \n",
    "    #         final_estimator=Ridge(random_state=SEED),\n",
    "    #         cv=cv,\n",
    "    #         # n_jobs=-1,\n",
    "    #         verbose=1\n",
    "    #         )\n",
    "}\n",
    "\n",
    "for model_name, regressor in regressors.items():\n",
    "    t0 = time.time()\n",
    "    scores = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "    # ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "    ttr = regressor\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(cv.split(X_train))):\n",
    "        \n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index].copy(), X_train.iloc[test_index].copy()\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "        \n",
    "        if data_prep_has_fit_method:\n",
    "            X_train_cv, X_test_cv = data_prepation(X_train_cv, X_test_cv)\n",
    "\n",
    "        # Models that need scaling and no missing value\n",
    "        if model_name in ['MLPRegressor1', 'MLPRegressor2', 'MLPRegressor3', 'MLPRegressor4', 'MLPRegressor5', 'MLPRegressor6', 'MLPRegressor7', 'MLPRegressor8', 'SGDRegressor', 'PassiveAggressiveRegressor', 'Perceptron', 'Ridge', 'Lasso', 'ElasticNet', 'HuberRegressor', 'BayesianRidge', 'ARDRegression', 'TheilSenRegressor', 'RANSACRegressor', 'OrthogonalMatchingPursuit', 'Lars', 'LassoLars', 'LassoLarsIC']:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_cv = pd.DataFrame(scaler.fit_transform(X_train_cv), columns=X_train_cv.columns)\n",
    "            X_test_cv = pd.DataFrame(scaler.transform(X_test_cv), columns=X_test_cv.columns)\n",
    "\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            X_train_cv = pd.DataFrame(imputer.fit_transform(X_train_cv), columns=X_train_cv.columns)\n",
    "            X_test_cv = pd.DataFrame(imputer.transform(X_test_cv), columns=X_test_cv.columns)\n",
    "\n",
    "            \n",
    "        ttr.fit(X_train_cv, y_train_cv)        \n",
    "        y_pred = ttr.predict(X_test_cv)\n",
    "        score_eval = mean_squared_error(y_test_cv, y_pred, squared=False)\n",
    "        scores.append(score_eval)\n",
    "        \n",
    "        try:\n",
    "            feature_importance = pd.Series(ttr.regressor_.feature_importances_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "        except:\n",
    "            try:\n",
    "                feature_importance = pd.Series(ttr.regressor_.coef_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "            except:\n",
    "                feature_importance = pd.Series(np.zeros(X_train_cv.shape[1]), index=X_train_cv.columns, name=f'fold{i}')\n",
    "        feature_importances = pd.concat([feature_importances, feature_importance], axis=1)\n",
    "    \n",
    "    feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "    \n",
    "    ttr.fit(X_train_prep, y_train)\n",
    "    y_pred = ttr.predict(X_test_prep)\n",
    "    \n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(f'{model_name}: {np.mean(scores):.4f} ± {np.std(scores):.4f}, Time: {time.time() - t0:.2f} seconds, RMSE: {score_eval:.4f}')\n",
    "    # print(feature_importances.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple stacking with sklearn with target transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple stacking with sklearn with target transformation\n",
    "regressors = [\n",
    "    ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                  max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                  objective='reg:squarederror')),\n",
    "    ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True)),\n",
    "    ('HistGradientBoostingRegressor2', HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "                                                                    max_depth=10, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.1, max_leaf_nodes=100, \n",
    "                                                                    min_samples_leaf=20, max_bins=255)),\n",
    "    \n",
    "]\n",
    "\n",
    "model = StackingRegressor(\n",
    "    estimators=regressors,\n",
    "    final_estimator=LarsCV(cv=cv, max_iter=10000, n_jobs=-1),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "y_train_transformed = target_transform(y_train)\n",
    "\n",
    "def scorer(estimator, X, y):\n",
    "    y_pred = inverse_target_transform(estimator.predict(X))\n",
    "    return -mean_squared_error(inverse_target_transform(y), y_pred, squared=False)\n",
    "\n",
    "scores = cross_val_score(model, X_train_prep, y_train_transformed, cv=cv, scoring=scorer, n_jobs=-1)\n",
    "print(f'{np.mean(scores):.4f} ± {np.std(scores):.4f}')\n",
    "\n",
    "model.fit(X_train_prep, y_train_transformed)\n",
    "y_pred_StackingRegressor = inverse_target_transform(model.predict(X_test_prep))\n",
    "\n",
    "# Save predictions\n",
    "sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "sub['price'] = y_pred_StackingRegressor\n",
    "now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "sub.to_csv(f'submissions/submission_StackingRegressor{now}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple stacking with sklearn without target transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple stacking with sklearn without target transformation\n",
    "regressors = [\n",
    "    ('LGBMRegressor11', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6)),\n",
    "    ('XGBRegressor6', XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200,  \n",
    "                                  max_depth=8,  min_child_weight=1, gamma=0.07,  colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, \n",
    "                                  objective='reg:squarederror')),\n",
    "    ('CatBoostRegressor', CatBoostRegressor(random_state=SEED, silent=True)),\n",
    "    ('HistGradientBoostingRegressor2', HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "                                                                    max_depth=10, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.1, max_leaf_nodes=100, \n",
    "                                                                    min_samples_leaf=20, max_bins=255)),\n",
    "    \n",
    "]\n",
    "\n",
    "model = StackingRegressor(\n",
    "    estimators=regressors,\n",
    "    final_estimator=LarsCV(cv=cv, max_iter=10000, n_jobs=-1),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "model.fit(X_train_prep, y_train)\n",
    "y_pred_StackingRegressor = model.predict(X_test_prep)\n",
    "\n",
    "# Save predictions\n",
    "sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "sub['price'] = y_pred_StackingRegressor\n",
    "now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "sub.to_csv(f'submissions/submission_StackingRegressor{now}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "def plot_predictions_errors(y, y_pred, title):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        subsample=100,\n",
    "        ax=axs[0],\n",
    "        random_state=0,\n",
    "    )\n",
    "    axs[0].set_title(\"Actual vs. Predicted values\")\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"residual_vs_predicted\",\n",
    "        subsample=1000,\n",
    "        ax=axs[1],\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    axs[1].set_title(\"Residuals vs. Predicted Values\")\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom stacking for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV stacking: \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "regressors = {\n",
    "    'LGBMRegressor11': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, \n",
    "                                     max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6),\n",
    "    'XGBRegressor6': XGBRegressor(random_state=SEED, n_jobs=-1, learning_rate=0.055, n_estimators=200, \n",
    "                                  max_depth=8, min_child_weight=1, gamma=0.07, colsample_bytree=0.67, \n",
    "                                  colsample_bylevel=0.67, colsample_bynode=0.8, subsample=0.7, objective='reg:squarederror'),\n",
    "    'CatBoostRegressor': CatBoostRegressor(random_state=SEED, silent=True), # Promising but fails on the cv\n",
    "    'HistGradientBoostingRegressor3': HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "                                                                    max_depth=10, learning_rate=0.1, \n",
    "                                                                    l2_regularization=0.1, max_leaf_nodes=100, \n",
    "                                                                    min_samples_leaf=20, max_bins=255),\n",
    "}\n",
    "\n",
    "meta_regressors = [\n",
    "    ('LinearRegression', LinearRegression()),\n",
    "    ('RidgeCV', RidgeCV(alphas=np.logspace(-3, 3, 13), cv=cv)),\n",
    "    ('ElasticNetCV', ElasticNetCV(alphas=np.logspace(-3, 3, 13), cv=cv, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], max_iter=100000)),\n",
    "    # ('LassoCV', LassoCV(alphas=np.logspace(-3, 3, 13), cv=cv, max_iter=100000)),  # = ElasticNetCV with l1_ratio=1\n",
    "    # ('LarsCV', LarsCV(cv=cv, max_iter=100000, n_jobs=-1)),\n",
    "    # ('OrthogonalMatchingPursuitCV', OrthogonalMatchingPursuitCV(cv=cv, n_jobs=-1)),\n",
    "    # ('LassoLarsCV', LassoLarsCV(cv=cv, max_iter=10000, n_jobs=-1)),\n",
    "    # ('BayesianRidge', BayesianRidge()),\n",
    "]\n",
    "\n",
    "FIT_REGRESSORS = False\n",
    "DISPLAY_REGRESSOR_RESULTS = True\n",
    "PLOT_ERRORS = False\n",
    "TARGET_TRANSFORMATION = False\n",
    "\n",
    "if FIT_REGRESSORS:\n",
    "    # Store out of fold predictions for meta learner\n",
    "    X_meta_trains = {}  # Dict of datasets used for meta learner training\n",
    "    X_meta_hold_outs = {}  # Dict of datasets used for meta learner validation\n",
    "    X_meta_tests = {}  # Dict of datasets used for meta learner testing\n",
    "\n",
    "for i, (train_index, hold_out_index) in enumerate(cv.split(X_train_prep)):\n",
    "    t0 = time.time()\n",
    "    print(f'Fold {i+1} of {cv.get_n_splits()}')\n",
    "    X_train_cv, X_hold_out = X_train_prep.iloc[train_index].copy(), X_train_prep.iloc[hold_out_index].copy()\n",
    "    y_train_cv, y_hold_out = y_train.iloc[train_index].copy(), y_train.iloc[hold_out_index].copy()\n",
    "    \n",
    "    X_meta_train = pd.DataFrame(index=train_index, columns=[name for name, _ in regressors.items()])\n",
    "    X_meta_hold_out = pd.DataFrame(index=hold_out_index, columns=[name for name, _ in regressors.items()])\n",
    "    X_meta_test = pd.DataFrame(index=X_test_prep.index, columns=[name for name, _ in regressors.items()])\n",
    "    \n",
    "    if FIT_REGRESSORS:\n",
    "        # for name, regressor in regressors:\n",
    "        for name, regressor in regressors.items():\n",
    "            print(f'Fitting {name} ...')\n",
    "            if TARGET_TRANSFORMATION:\n",
    "                ttr = TransformedTargetRegressor(regressor=regressor, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "            else:\n",
    "                ttr = regressor\n",
    "            \n",
    "            if name == 'CatBoostRegressor':\n",
    "                X_meta_train[name] = cross_val_predict(ttr, X_train_cv, y_train_cv, cv=cv, verbose=0)  # CatBoostRegressor fails on n_jobs=-1\n",
    "            else:\n",
    "                X_meta_train[name] = cross_val_predict(ttr, X_train_cv, y_train_cv, cv=cv, n_jobs=-1, verbose=0)\n",
    "            \n",
    "            # fit the model on the full cv training set\n",
    "            ttr.fit(X_train_cv, y_train_cv)\n",
    "            X_meta_hold_out[name] = ttr.predict(X_hold_out)\n",
    "            X_meta_test[name] = ttr.predict(X_test_prep)\n",
    "\n",
    "            if DISPLAY_REGRESSOR_RESULTS:\n",
    "                print(f'Hold out score of {name}: {mean_squared_error(y_hold_out, X_meta_hold_out[name], squared=False):.4f}')\n",
    "                if not SUBMIT:\n",
    "                    print(f'Test score of {name}: {mean_squared_error(y_test, X_meta_test[name], squared=False):.4f}')\n",
    "\n",
    "        # Store datasets for meta learner\n",
    "        X_meta_trains[i] = X_meta_train.copy()\n",
    "        X_meta_hold_outs[i] = X_meta_hold_out.copy()\n",
    "        X_meta_tests[i] = X_meta_test.copy()\n",
    "        \n",
    "    # Transform the predictions of regressors with target transform\n",
    "    if TARGET_TRANSFORMATION:  \n",
    "        X_meta_train = target_transform(X_meta_trains[i])\n",
    "        X_meta_hold_out = target_transform(X_meta_hold_outs[i])\n",
    "        X_meta_test = target_transform(X_meta_tests[i])\n",
    "    else:\n",
    "        X_meta_train = X_meta_trains[i]\n",
    "        X_meta_hold_out = X_meta_hold_outs[i]\n",
    "        X_meta_test = X_meta_tests[i]\n",
    "\n",
    "    for name, meta_regressor in meta_regressors:\n",
    "        # Fit the final estimator on the hold out predictions\n",
    "        if TARGET_TRANSFORMATION:\n",
    "            meta_ttr = TransformedTargetRegressor(\n",
    "                meta_regressor,\n",
    "                func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "        else:\n",
    "            meta_ttr = meta_regressor\n",
    "        \n",
    "        meta_ttr.fit(X_meta_train, y_train_cv)\n",
    "        y_hold_out_pred = meta_ttr.predict(X_meta_hold_out)\n",
    "        y_test_pred = meta_ttr.predict(X_meta_test)\n",
    "        \n",
    "        if not SUBMIT:\n",
    "            score_eval = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "        else:\n",
    "            score_eval = np.nan\n",
    "        \n",
    "        l1_ratio = getattr(meta_ttr.regressor_, 'l1_ratio_', np.nan) if TARGET_TRANSFORMATION else getattr(meta_ttr, 'l1_ratio_', np.nan)\n",
    "        alpha = getattr(meta_ttr.regressor_, 'alpha_', np.nan) if TARGET_TRANSFORMATION else getattr(meta_ttr, 'alpha_', np.nan)\n",
    "        coef = getattr(meta_ttr.regressor_, 'coef_', np.nan) if TARGET_TRANSFORMATION else getattr(meta_ttr, 'coef_', np.nan)\n",
    "        intercept = getattr(meta_ttr.regressor_, 'intercept_', np.nan) if TARGET_TRANSFORMATION else getattr(meta_ttr, 'intercept_', np.nan)\n",
    "        \n",
    "        print(f'Meta regressor: {name}, RMSE hold out: {mean_squared_error(y_hold_out, y_hold_out_pred, squared=False)},',\n",
    "              f'RMSE test: {score_eval:.4f}, fit time: {time.time() - t0:.2f} s,',\n",
    "              f'Coefficients: {coef}, intercept: {intercept}, l1_ratio: {l1_ratio}, alpha: {alpha}', end='\\n'\n",
    "        )\n",
    "        if PLOT_ERRORS:\n",
    "            plot_predictions_errors(y_hold_out, y_hold_out_pred, 'Hold out')\n",
    "            if not SUBMIT:\n",
    "                plot_predictions_errors(y_test, y_test_pred, 'Test')\n",
    "\n",
    "    print('-'*80, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all dataframes of X_meta_trains and X_meta_hold_outs to csv seprarately\n",
    "# for i in range(5):\n",
    "#     X_meta_trains[i].to_csv(f'datasets/X_meta_trains_{i}.csv')\n",
    "#     X_meta_hold_outs[i].to_csv(f'datasets/X_meta_hold_outs_{i}.csv')\n",
    "#     X_meta_tests[i].to_csv(f'datasets/X_meta_tests_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all dataframes of X_meta_trains and X_meta_hold_outs from csv seprarately\n",
    "# X_meta_trains_2, X_meta_hold_outs_2 = {}, {}\n",
    "# for i in range(5):\n",
    "#     X_meta_trains_2[i] = pd.read_csv(f'datasets/X_meta_trains_{i}.csv', index_col=0)\n",
    "#     X_meta_hold_outs_2[i] = pd.read_csv(f'datasets/X_meta_hold_outs_{i}.csv', index_col=0)\n",
    "#     X_meta_tests_2[i] = pd.read_csv(f'datasets/X_meta_tests_{i}.csv', index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f90af0c099b2a3322334c0593a59f872710278483b6e7b3217af559be1bbf34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
