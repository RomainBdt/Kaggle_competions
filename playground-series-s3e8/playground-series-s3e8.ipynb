{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# import regressors\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, PassiveAggressiveRegressor, Perceptron, RidgeClassifier, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Lars, BayesianRidge, ARDRegression, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor, RANSACRegressor\n",
    "from sklearn.linear_model import LassoLars, LassoLarsIC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# pandas deactivate future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "SUBMIT = False\n",
    "USE_ORIGINAL = False\n",
    "SEED = 15\n",
    "SAMPLE = 0.2\n",
    "\n",
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')\n",
    "orig = pd.read_csv('datasets/cubic_zirconia.csv')\n",
    "\n",
    "for i, df in enumerate([train, test, orig]):\n",
    "    df.drop(['id'], axis=1, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # df['dataset'] = i\n",
    "\n",
    "# Define test set\n",
    "if not SUBMIT:\n",
    "    train, test = train_test_split(train, test_size=0.2, random_state=SEED) \n",
    "\n",
    "if USE_ORIGINAL:\n",
    "    train = pd.concat([train, orig], axis=0)\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Sampling for faster training\n",
    "if SAMPLE < 1:\n",
    "    train = train.sample(frac=SAMPLE, random_state=SEED)\n",
    "\n",
    "del orig\n",
    "\n",
    "# set training data\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop('price')\n",
    "X_test = test.copy()\n",
    "\n",
    "if not SUBMIT:\n",
    "    y_test = X_test.pop('price')\n",
    "else:\n",
    "    y_test = None\n",
    "\n",
    "# transform categorical features\n",
    "def transform_categorical(df):\n",
    "    df['cut'] = df['cut'].map({'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4})\n",
    "    df['color'] = df['color'].map({'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D': 6})\n",
    "    df['clarity'] = df['clarity'].map({'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7})\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Drop extreme values\n",
    "    min = 2\n",
    "    max = 20\n",
    "    df = df[(df['x'] < max) & (df['y'] < max) & (df['z'] < max)]\n",
    "    df = df[(df['x'] > min) & (df['y'] > min) & (df['z'] > min)]\n",
    "    return df\n",
    "\n",
    "def add_volume(df):\n",
    "    df['volume'] = df['x'] * df['y'] * df['z']\n",
    "    return df\n",
    "\n",
    "def add_volume_ratio(df):\n",
    "    df['volume_ratio1'] = (df['x'] * df['y']) / (df['z'] * df['z'])\n",
    "    df['volume_ratio2'] = (df['x'] * df['z']) / (df['y'] * df['y'])\n",
    "    df['volume_ratio3'] = (df['y'] * df['z']) / (df['x'] * df['x'])\n",
    "    df['volume_ratio4'] = (df['x'] * df['y']) / (df['z'] * df['y'])\n",
    "    df['volume_ratio5'] = (df['x'] * df['y']) / (df['z'] * df['x'])\n",
    "    df['volume_ratio6'] = (df['x'] * df['z']) / (df['y'] * df['x'])\n",
    "    df['volume_ratio7'] = (df['x'] * df['z']) / (df['y'] * df['z'])\n",
    "    df['volume_ratio8'] = (df['y'] * df['z']) / (df['x'] * df['y'])\n",
    "    df['volume_ratio9'] = (df['y'] * df['z']) / (df['x'] * df['z'])\n",
    "    df['volume_ratio10'] = (df['x'] * df['y']) / (df['y'] * df['z'])\n",
    "    df['volume_ratio11'] = (df['x'] * df['z']) / (df['x'] * df['y'])\n",
    "    df['volume_ratio12'] = (df['x'] * df['y']) / (df['x'] * df['z'])\n",
    "    return df\n",
    "\n",
    "def add_surface_area(df):\n",
    "    df['surface_area'] = 2 * (df['x'] * df['y'] + df['x'] * df['z'] + df['y'] * df['z'])\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # df[\"volume\"] = df[\"x\"] * df[\"y\"] * df[\"z\"]\n",
    "    # df[\"surface_area\"] = 2 * (df[\"x\"] * df[\"y\"] + df[\"y\"] * df[\"z\"] + df[\"z\"] * df[\"x\"])\n",
    "    df[\"aspect_ratio_xy\"] = df[\"x\"] / df[\"y\"]\n",
    "    df[\"aspect_ratio_yz\"] = df[\"y\"] / df[\"z\"]\n",
    "    df[\"aspect_ratio_zx\"] = df[\"z\"] / df[\"x\"]\n",
    "    df[\"diagonal_distance\"] = np.sqrt(df[\"x\"] ** 2 + df[\"y\"] ** 2 + df[\"z\"] ** 2)\n",
    "    df[\"relative_height\"] = (df[\"z\"] - df[\"z\"].min()) / (df[\"z\"].max() - df[\"z\"].min())\n",
    "    df[\"relative_position\"] = (df[\"x\"] + df[\"y\"] + df[\"z\"]) / (df[\"x\"] + df[\"y\"] + df[\"z\"]).sum()\n",
    "    df[\"volume_ratio\"] = df[\"x\"] * df[\"y\"] * df[\"z\"] / (df[\"x\"].mean() * df[\"y\"].mean() * df[\"z\"].mean())\n",
    "    df[\"length_ratio\"] = df[\"x\"] / df[\"x\"].mean()\n",
    "    df[\"width_ratio\"] = df[\"y\"] / df[\"y\"].mean()\n",
    "    df[\"height_ratio\"] = df[\"z\"] / df[\"z\"].mean()\n",
    "    df[\"sphericity\"] = 1.4641 * (6 * df[\"volume\"])**(2/3) / df[\"surface_area\"]\n",
    "    df[\"compactness\"] = df[\"volume\"]**(1/3) / df[\"x\"]\n",
    "    return df\n",
    "\n",
    "def depth_to_table_ratio(df):\n",
    "    df['depth_to_table_ratio'] = df['depth'] / df['table']\n",
    "    return df\n",
    "\n",
    "def new_depth(df):\n",
    "    df['new_depth'] = df['z'] / (df['x'] + df['y'])\n",
    "    return df\n",
    "\n",
    "def target_transform(serie):\n",
    "    serie = np.log1p(serie)\n",
    "    return serie\n",
    "\n",
    "def inverse_target_transform(serie):\n",
    "    serie = np.expm1(serie)\n",
    "    return serie\n",
    "    \n",
    "# Make data preparation pipeline\n",
    "def data_prepation(X_train, X_test):\n",
    "    \n",
    "    for df in [X_train, X_test]:\n",
    "        for col in ['x', 'y', 'z']:\n",
    "            df[col].replace(0, np.nan, inplace=True)\n",
    "        df = transform_categorical(df)\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    \n",
    "    # for df in [X_train, X_test]:\n",
    "        # df = add_volume(df)\n",
    "        # df = add_volume_ratio(df)\n",
    "        # df = add_surface_area(df)\n",
    "        # df = feature_engineering(df)\n",
    "        # df = depth_to_table_ratio(df)\n",
    "        # df = new_depth(df)\n",
    "        \n",
    "    # Scaling\n",
    "    scaler = PowerTransformer()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "        \n",
    "    return X_train, X_test\n",
    "            \n",
    "X_train, X_test = data_prepation(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor1: -598.5343 ± 11.8246, Time: 6.15 seconds, RMSE: 582.7651\n",
      "XGBRegressor1: -619.3136 ± 13.1754, Time: 10.15 seconds, RMSE: 602.7441\n",
      "MLPRegressor1: -712.3590 ± 21.7901, Time: 51.56 seconds, RMSE: 690.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rzfxxf\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor2: -652.6601 ± 21.2668, Time: 320.14 seconds, RMSE: 630.4257\n",
      "MLPRegressor3: -737.3074 ± 20.3224, Time: 20.89 seconds, RMSE: 738.6174\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "regressors = {\n",
    "    'LGBMRegressor1': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt'),\n",
    "    # 'LGBMRegressor2': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart'),\n",
    "    # 'LGBMRegressor3': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='goss'),\n",
    "    # 'LGBMRegressor4': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1),\n",
    "    # 'LGBMRegressor5': LGBMRegressor(random_state=SEED, n_jobs=-1, class_weight='balanced'),\n",
    "    # 'LGBMRegressor6': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'LGBMRegressor7': LGBMRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor8': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor9': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart', colsample_bytree=0.7),\n",
    "    'XGBRegressor1': XGBRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'XGBRegressor2': XGBRegressor(random_state=SEED, n_jobs=-1, booster='dart'),\n",
    "    # 'XGBRegressor3': XGBRegressor(random_state=SEED, n_jobs=-1, booster='gblinear'),\n",
    "    # 'XGBRegressor4': XGBRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'XGBRegressor5': XGBRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'XGBRandomForestRegressor': XGBRFRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'CatBoostRegressor': CatBoostRegressor(random_state=SEED, silent=True), # Promising but fails on the cv\n",
    "    # 'RandomForestRegressor': RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'ExtraTreesRegressor': ExtraTreesRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'AdaBoostRegressor': AdaBoostRegressor(random_state=SEED),\n",
    "    # 'GradientBoostingRegressor': GradientBoostingRegressor(random_state=SEED),\n",
    "    # 'BaggingRegressor': BaggingRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    # 'DecisionTreeRegressor': DecisionTreeRegressor(random_state=SEED),\n",
    "    # 'GaussianProcessRegressor': GaussianProcessRegressor(random_state=SEED),\n",
    "    'MLPRegressor1': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='adam'),\n",
    "    'MLPRegressor2': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='lbfgs'),\n",
    "    'MLPRegressor3': MLPRegressor(random_state=SEED, max_iter=5000, activation='tanh', solver='adam'),\n",
    "    'MLPRegressor4': MLPRegressor(random_state=SEED, max_iter=1000, activation='tanh', solver='lbfgs'),\n",
    "    'MLPRegressor5': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='adam'),\n",
    "    'MLPRegressor6': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='lbfgs'),\n",
    "    'MLPRegressor7': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='adam'),\n",
    "    'MLPRegressor8': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='lbfgs'),\n",
    "    'Ridge': Ridge(random_state=SEED),\n",
    "    # 'SGDRegressor': SGDRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'PassiveAggressiveRegressor': PassiveAggressiveRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'Perceptron': Perceptron(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Lasso': Lasso(random_state=SEED),\n",
    "    # 'ElasticNet': ElasticNet(random_state=SEED, max_iter=1e6),\n",
    "    # 'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    # 'BayesianRidge': BayesianRidge(),\n",
    "    # 'ARDRegression': ARDRegression(),\n",
    "    # 'TheilSenRegressor': TheilSenRegressor(random_state=SEED),\n",
    "    # 'RANSACRegressor': RANSACRegressor(random_state=SEED),\n",
    "    # 'OrthogonalMatchingPursuit': OrthogonalMatchingPursuit(normalize=False),\n",
    "    # 'Lars': Lars(),\n",
    "    # 'LassoLars': LassoLars(),\n",
    "    # 'LassoLarsIC': LassoLarsIC(normalize=False),\n",
    "    # 'StackingRegressor': StackingRegressor(\n",
    "    #         estimators=[\n",
    "    #             ('LGBMRandomForestRegressor', LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1)),\n",
    "    #             ('XGBRandomForestRegressor', XGBRFRegressor(random_state=SEED, n_jobs=-1)),\n",
    "    #             ('RandomForestRegressor', RandomForestRegressor(random_state=SEED, n_jobs=-1)),\n",
    "    #             # ('ExtraTreesRegressor', ExtraTreesRegressor(random_state=SEED, n_jobs=-1))\n",
    "    #             ], \n",
    "    #         final_estimator=Ridge(random_state=SEED),\n",
    "    #         # cv=cv,\n",
    "    #         # n_jobs=-1,\n",
    "    #         )\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for model_name, model in regressors.items():\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # model = TransformedTargetRegressor(regressor=model, transformer=QuantileTransformer(output_distribution='normal', random_state=SEED))\n",
    "    model = TransformedTargetRegressor(regressor=model, func=target_transform, inverse_func=inverse_target_transform, check_inverse=False)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(f'{model_name}: {scores.mean():.4f} ± {scores.std():.4f}, Time: {time.time() - t0:.2f} seconds, RMSE: {score_eval:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = test_predictions.mean(axis=1).round().astype(int)\n",
    "\n",
    "sub = pd.read_csv('submissions/sample_submission.csv')\n",
    "sub['quality'] = y_pred_test\n",
    "now = time.strftime(\"%Y-%m-%d %H_%M_%S\")\n",
    "sub.to_csv(f'submissions/submission{now}.csv', index=False)\n",
    "# Copy the leaked values from the original dataset before submitting\n",
    "# Transform the price column back to the original scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f90af0c099b2a3322334c0593a59f872710278483b6e7b3217af559be1bbf34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
