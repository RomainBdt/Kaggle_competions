{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, Birch, MeanShift, SpectralClustering, AffinityPropagation, FeatureAgglomeration\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn import config_context\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import KMeans, Birch, MiniBatchKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# import regressors\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, StackingRegressor, HistGradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, PassiveAggressiveRegressor, Perceptron, RidgeClassifier, LogisticRegression\n",
    "from sklearn.linear_model import Lasso, ElasticNet, Lars, BayesianRidge, ARDRegression, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor, RANSACRegressor\n",
    "from sklearn.linear_model import LassoLars, LassoLarsIC\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV, LarsCV, OrthogonalMatchingPursuitCV, LassoLarsCV, BayesianRidge, LinearRegression\n",
    "\n",
    "# pandas deactivate future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "SUBMIT = False\n",
    "USE_ORIGINAL = True\n",
    "SEED = 15\n",
    "SAMPLE = 1\n",
    "TARGET = 'Strength'\n",
    "TARGET_TRANSFORM = True\n",
    "SCALING_MODELS = ['Ridge', 'RidgeCV', 'SGDRegressor', 'PassiveAggressiveRegressor', 'LinearRegression', 'Lasso', 'ElasticNet', 'ElasticNetCV', 'HuberRegressor', \n",
    "                 'BayesianRidge', 'ARDRegression', 'TheilSenRegressor', 'RANSACRegressor', 'OrthogonalMatchingPursuit', 'Lars', 'LassoLars', 'LassoLarsIC']\n",
    "CLUSTER_RANGE = range(2, 11)\n",
    "CLUSTER_FEATURES = False\n",
    "DIMENSIONALITY_REDUCTION = False\n",
    "PCA_N_COMPONENTS = 0.9\n",
    "\n",
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')\n",
    "orig = pd.read_csv('datasets/ConcreteStrengthData.csv')\n",
    "\n",
    "for i, X in enumerate([train, test, orig]):\n",
    "    if 'id' in X.columns:\n",
    "        X.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# Define test set with original data\n",
    "if not SUBMIT:\n",
    "    train, test = train_test_split(train, test_size=0.2, random_state=SEED) \n",
    "\n",
    "# Add original data to training set\n",
    "if USE_ORIGINAL:\n",
    "    train = pd.concat([train, orig], axis=0)\n",
    "    train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "del orig\n",
    "\n",
    "# Sampling for faster training\n",
    "if SAMPLE < 1:\n",
    "    train = train.sample(frac=SAMPLE, random_state=SEED)\n",
    "\n",
    "# set training data\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop(TARGET)\n",
    "X_test = test.copy()\n",
    "\n",
    "if not SUBMIT:\n",
    "    y_test = X_test.pop(TARGET)\n",
    "else:\n",
    "    y_test = None\n",
    "\n",
    "# Feature engineering\n",
    "class FeatureEngineering(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\" Add new features to the dataset\"\"\"\n",
    "        X = X.copy()\n",
    "\n",
    "        # # Sum of features\n",
    "        # X['cement_slag'] = X['CementComponent'] + X['BlastFurnaceSlag']\n",
    "        # X['cement_flyash'] = X['CementComponent'] + X['FlyAshComponent']\n",
    "        # X['cement_water'] = X['CementComponent'] + X['WaterComponent']\n",
    "        # X['cement_superplasticizer'] = X['CementComponent'] + X['SuperplasticizerComponent']\n",
    "        # X['cement_coarseaggregate'] = X['CementComponent'] + X['CoarseAggregateComponent']\n",
    "        # X['cement_fineaggregate'] = X['CementComponent'] + X['FineAggregateComponent']\n",
    "        # X['cement_age'] = X['CementComponent'] + X['AgeInDays']\n",
    "        # X['slag_flyash'] = X['BlastFurnaceSlag'] + X['FlyAshComponent']\n",
    "        # X['slag_water'] = X['BlastFurnaceSlag'] + X['WaterComponent']\n",
    "        # X['slag_superplasticizer'] = X['BlastFurnaceSlag'] + X['SuperplasticizerComponent']\n",
    "        # X['slag_coarseaggregate'] = X['BlastFurnaceSlag'] + X['CoarseAggregateComponent']\n",
    "        # X['slag_fineaggregate'] = X['BlastFurnaceSlag'] + X['FineAggregateComponent']\n",
    "        # X['slag_age'] = X['BlastFurnaceSlag'] + X['AgeInDays']\n",
    "        # X['flyash_water'] = X['FlyAshComponent'] + X['WaterComponent']\n",
    "        # X['flyash_superplasticizer'] = X['FlyAshComponent'] + X['SuperplasticizerComponent']\n",
    "        # X['flyash_coarseaggregate'] = X['FlyAshComponent'] + X['CoarseAggregateComponent']\n",
    "\n",
    "        # # Ratio of features\n",
    "        # # X['cement_slag_ratio'] = X['CementComponent'] / X['BlastFurnaceSlag']  # Inf values\n",
    "        # # X['cement_flyash_ratio'] = X['CementComponent'] / X['FlyAshComponent']  # Inf values\n",
    "        # X['cement_water_ratio'] = X['CementComponent'] / X['WaterComponent']\n",
    "        # # X['cement_superplasticizer_ratio'] = X['CementComponent'] / X['SuperplasticizerComponent']  # Inf values\n",
    "        # X['cement_coarseaggregate_ratio'] = X['CementComponent'] / X['CoarseAggregateComponent']\n",
    "        # X['cement_fineaggregate_ratio'] = X['CementComponent'] / X['FineAggregateComponent']\n",
    "        # X['cement_age_ratio'] = X['CementComponent'] / X['AgeInDays']\n",
    "        # # X['slag_flyash_ratio'] = X['BlastFurnaceSlag'] / X['FlyAshComponent'] # Divide by zero\n",
    "        # X['slag_water_ratio'] = X['BlastFurnaceSlag'] / X['WaterComponent']\n",
    "        # # X['slag_superplasticizer_ratio'] = X['BlastFurnaceSlag'] / X['SuperplasticizerComponent']  # Divide by zero\n",
    "        # X['slag_coarseaggregate_ratio'] = X['BlastFurnaceSlag'] / X['CoarseAggregateComponent']\n",
    "        # X['slag_fineaggregate_ratio'] = X['BlastFurnaceSlag'] / X['FineAggregateComponent']\n",
    "        # X['slag_age_ratio'] = X['BlastFurnaceSlag'] / X['AgeInDays']\n",
    "        # X['flyash_water_ratio'] = X['FlyAshComponent'] / X['WaterComponent']\n",
    "        # # X['flyash_superplasticizer_ratio'] = X['FlyAshComponent'] / X['SuperplasticizerComponent'] # Divide by zero\n",
    "        # X['flyash_coarseaggregate_ratio'] = X['FlyAshComponent'] / X['CoarseAggregateComponent']\n",
    "\n",
    "        # Other features\n",
    "        X['water_age_cement_ratio'] = (X['WaterComponent'] * X['AgeInDays']) / X['CementComponent']\n",
    "        # X['has_superplasticizer'] = X['SuperplasticizerComponent'].apply(lambda x: 1 if x > 0 else 0)\n",
    "        # X['has_flyash'] = X['FlyAshComponent'].apply(lambda x: 1 if x > 0 else 0)\n",
    "        # X['has_slag'] = X['BlastFurnaceSlag'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "        # Taken from PHONG NGUYEN: Detailed feature description and feature engineering by ChatGPT\n",
    "        # Calculate TotalComponentWeight\n",
    "        # X['TotalComponentWeight'] = X['CementComponent'] + X['BlastFurnaceSlag'] + X['FlyAshComponent'] + X['WaterComponent'] + X['SuperplasticizerComponent'] + X['CoarseAggregateComponent'] + X['FineAggregateComponent']\n",
    "        # # Calculate Water-Cement-Ratio (WCR)\n",
    "        # X['WCR'] = X['WaterComponent'] / X['CementComponent']\n",
    "        # # Calculate Aggregate-Ratio (AR)\n",
    "        # X['AR'] = (X['CoarseAggregateComponent'] + X['FineAggregateComponent']) / X['CementComponent']\n",
    "        # # Calculate Water-Cement-Plus-Pozzolan-Ratio (WCPR)\n",
    "        # X['WCPR'] = X['WaterComponent'] / (X['CementComponent'] + X['BlastFurnaceSlag'] + X['FlyAshComponent'])\n",
    "        # # Calculate Cement-Age\n",
    "        # X['Cement-Age'] = X['CementComponent'] * X['AgeInDays']\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_features(X_train, X_test):\n",
    "    for n_clusters in CLUSTER_RANGE:\n",
    "        cluster_models = {\n",
    "            'KMeans': KMeans(n_clusters=n_clusters, random_state=SEED),\n",
    "            'Birch': Birch(n_clusters=n_clusters),\n",
    "            # 'MiniBatchKMeans': MiniBatchKMeans(n_clusters=n_clusters, random_state=SEED),\n",
    "            'GaussianMixture': GaussianMixture(n_components=n_clusters, random_state=SEED),\n",
    "        }\n",
    "        for model_name, model in cluster_models.items():\n",
    "            X_train[f'{model_name}_{n_clusters}'] = model.fit_predict(X_train)\n",
    "            X_test[f'{model_name}_{n_clusters}'] = model.predict(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def data_preparation(X_train, X_test, model_name):\n",
    "    \"\"\"Pipeline for data preparation.\"\"\"    \n",
    "    set_config(transform_output=\"pandas\")  # set to pandas\n",
    "\n",
    "    clf = make_pipeline(\n",
    "        PolynomialFeatures(2, include_bias=False, interaction_only=True),\n",
    "        FeatureEngineering(),\n",
    "        )\n",
    "    \n",
    "    # Add power transformer to the pipeline for linear models\n",
    "    if model_name in SCALING_MODELS:\n",
    "        clf = make_pipeline(\n",
    "            clf,\n",
    "            PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "            )\n",
    "        \n",
    "    y = X_train.iloc[:, 0] # get any column, not used in transformation\n",
    "    clf.fit(X_train, y)\n",
    "\n",
    "    with config_context(transform_output=\"pandas\"):\n",
    "        # the output of transform will be a Pandas DataFrame\n",
    "        X_train= clf.transform(X_train)\n",
    "        X_test = clf.transform(X_test)\n",
    "        if DIMENSIONALITY_REDUCTION:\n",
    "            pca = PCA(n_components=PCA_N_COMPONENTS, random_state=SEED)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "\n",
    "    set_config(transform_output=\"default\")  # reset to default\n",
    "\n",
    "    # Cluster features\n",
    "    if CLUSTER_FEATURES:\n",
    "        X_train, X_test = cluster_features(X_train, X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def model_preparation(regressor, model_name):\n",
    "    if TARGET_TRANSFORM and model_name in SCALING_MODELS:\n",
    "        regressor = TransformedTargetRegressor(regressor=regressor, transformer=QuantileTransformer(output_distribution='normal', random_state=SEED))\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(transform_output=\"pandas\")  # set to pandas\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Set categorical features for catboost\n",
    "cat_features = [col for col in X_train.columns if X_train[col].dtype == 'category']\n",
    "\n",
    "regressors = {\n",
    "    # 'LGBMRegressor1': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt'),\n",
    "    # 'LGBMRegressor2': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart'),\n",
    "    # 'LGBMRegressor3': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='goss'),\n",
    "    'LGBMRegressor4': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='rf', subsample=.632, subsample_freq=1),\n",
    "    # 'LGBMRegressor5': LGBMRegressor(random_state=SEED, n_jobs=-1, class_weight='balanced'),\n",
    "    # 'LGBMRegressor6': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'LGBMRegressor7': LGBMRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor8': LGBMRegressor(random_state=SEED, n_jobs=-1, subsample=0.7, colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor9': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='dart', colsample_bytree=0.7),\n",
    "    # 'LGBMRegressor10': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240),\n",
    "    # 'LGBMRegressor11': LGBMRegressor(random_state=SEED, n_jobs=-1, boosting_type='gbdt', num_leaves=48, max_depth=14, learning_rate=0.08, n_estimators=240, subsample=0.7, colsample_bytree=0.6),\n",
    "    # 'XGBRegressor1': XGBRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'XGBRegressor2': XGBRegressor(random_state=SEED, n_jobs=-1, booster='dart'),\n",
    "    # 'XGBRegressor3': XGBRegressor(random_state=SEED, n_jobs=-1, booster='gblinear'),\n",
    "    # 'XGBRegressor4': XGBRegressor(random_state=SEED, n_jobs=-1, colsample_bytree=0.7),\n",
    "    # 'XGBRegressor5': XGBRegressor(random_state=SEED, n_jobs=-1, subsample=0.7),\n",
    "    # 'XGBRegressor6': XGBRegressor(random_state=SEED, \n",
    "    #                               n_jobs=-1, \n",
    "    #                               learning_rate=0.055, \n",
    "    #                               n_estimators=200, \n",
    "    #                               max_depth=8, \n",
    "    #                               min_child_weight=1, \n",
    "    #                               gamma=0.07, \n",
    "    #                               colsample_bytree=0.67, \n",
    "    #                               colsample_bylevel=0.67, \n",
    "    #                               colsample_bynode=0.8,\n",
    "    #                               subsample=0.7, \n",
    "    #                               objective='reg:squarederror'),\n",
    "    # 'XGBRegressor7': XGBRegressor(random_state=SEED, n_jobs=-1, objective='reg:squarederror'),\n",
    "    # 'XGBRandomForestRegressor': XGBRFRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'CatBoostRegressor': CatBoostRegressor(random_state=SEED, silent=True),\n",
    "    # 'HistGradientBoostingRegressor': HistGradientBoostingRegressor(random_state=SEED),\n",
    "    # 'HistGradientBoostingRegressor2': HistGradientBoostingRegressor(random_state=SEED, max_iter=200, \n",
    "    #                                                                 max_depth=6, learning_rate=0.1, \n",
    "    #                                                                 l2_regularization=0.0006754828207682132, max_leaf_nodes=42, \n",
    "    #                                                                 min_samples_leaf=9, max_bins=255),\n",
    "    # 'HistGradientBoostingRegressor3': HistGradientBoostingRegressor(random_state=SEED, max_iter=1000, \n",
    "    #                                                                 max_depth=10, learning_rate=0.1, \n",
    "    #                                                                 l2_regularization=0.1, max_leaf_nodes=100, \n",
    "    #                                                                 min_samples_leaf=20, max_bins=255),\n",
    "    # 'RandomForestRegressor': RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'ExtraTreesRegressor': ExtraTreesRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'AdaBoostRegressor': AdaBoostRegressor(random_state=SEED),\n",
    "    # 'GradientBoostingRegressor': GradientBoostingRegressor(random_state=SEED),\n",
    "    # 'BaggingRegressor': BaggingRegressor(random_state=SEED, n_jobs=-1),\n",
    "    # 'KNeighborsRegressor': KNeighborsRegressor(n_jobs=-1),\n",
    "    # 'DecisionTreeRegressor': DecisionTreeRegressor(random_state=SEED),\n",
    "    # 'GaussianProcessRegressor': GaussianProcessRegressor(random_state=SEED),\n",
    "    # 'MLPRegressor1': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='adam'),\n",
    "    # 'MLPRegressor2': MLPRegressor(random_state=SEED, max_iter=1000, activation='relu', solver='lbfgs'),\n",
    "    # 'MLPRegressor3': MLPRegressor(random_state=SEED, max_iter=5000, activation='tanh', solver='adam'),\n",
    "    # 'MLPRegressor4': MLPRegressor(random_state=SEED, max_iter=1000, activation='tanh', solver='lbfgs'),\n",
    "    # 'MLPRegressor5': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='adam'),\n",
    "    # 'MLPRegressor6': MLPRegressor(random_state=SEED, max_iter=1000, activation='logistic', solver='lbfgs'),\n",
    "    # 'MLPRegressor7': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='adam'),\n",
    "    # 'MLPRegressor8': MLPRegressor(random_state=SEED, max_iter=1000, activation='identity', solver='lbfgs'),\n",
    "    # 'Ridge': Ridge(random_state=SEED),\n",
    "    # 'RidgeCV': RidgeCV(alphas=np.logspace(-3, 3, 7), cv=cv),\n",
    "    # 'SGDRegressor': SGDRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'PassiveAggressiveRegressor': PassiveAggressiveRegressor(random_state=SEED, max_iter=1000, tol=1e-3),\n",
    "    # 'LinearRegression': LinearRegression(n_jobs=-1),\n",
    "    # 'Lasso': Lasso(random_state=SEED),\n",
    "    # 'ElasticNet': ElasticNet(random_state=SEED, max_iter=1000000),\n",
    "    # 'ElasticNetCV': ElasticNetCV(alphas=np.logspace(-3, 3, 7), cv=cv, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], max_iter=1000000),\n",
    "    # 'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    # 'ARDRegression': ARDRegression(),\n",
    "    # 'TheilSenRegressor': TheilSenRegressor(random_state=SEED),\n",
    "    # 'RANSACRegressor': RANSACRegressor(random_state=SEED),\n",
    "    # 'OrthogonalMatchingPursuit': OrthogonalMatchingPursuit(normalize=False),\n",
    "    # 'Lars': Lars(),\n",
    "    # 'LassoLars': LassoLars(),\n",
    "    # 'LassoLarsIC': LassoLarsIC(normalize=False),\n",
    "    # 'VotingRegressor': VotingRegressor(\n",
    "    #         estimators=[\n",
    "    #             ('Ridge', Ridge(random_state=SEED)),\n",
    "    #             ('LGBMRegressor4', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='rf', subsample=.632, subsample_freq=1)),\n",
    "    #             ('XGBRegressor1', XGBRegressor(random_state=SEED, n_jobs=1)),\n",
    "    #             ], \n",
    "    #         n_jobs=-1,\n",
    "    #         verbose=0,\n",
    "    #         ),\n",
    "    # 'BaggingRegressor2': BaggingRegressor(base_estimator=LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='rf', subsample=.632, subsample_freq=1),\n",
    "    #                                       random_state=SEED, \n",
    "    #                                       n_jobs=-1,\n",
    "    #                                       max_samples=0.632,\n",
    "    #                                       max_features=0.632,\n",
    "    #                                       bootstrap=True,\n",
    "    #                                       ),\n",
    "    # 'StackingRegressor': StackingRegressor(\n",
    "    #         estimators=[\n",
    "    #             ('Ridge', Ridge(random_state=SEED)),\n",
    "    #             ('LGBMRegressor4', LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='rf', subsample=.632, subsample_freq=1)),\n",
    "    #             ('XGBRegressor1', XGBRegressor(random_state=SEED, n_jobs=1)),\n",
    "    #             ], \n",
    "    #         final_estimator=Ridge(random_state=SEED),\n",
    "    #         cv=cv,\n",
    "    #         n_jobs=-1,\n",
    "    #         verbose=0,\n",
    "    #         )\n",
    "}\n",
    "\n",
    "# for PCA_N_COMPONENTS in [None, 0.2, 0.5, 0.9, 0.95, 0.99]:\n",
    "for model_name, regressor in regressors.items():\n",
    "    t0 = time.time()\n",
    "    scores = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(cv.split(X_train))):\n",
    "        \n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index].copy(), X_train.iloc[test_index].copy()\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "\n",
    "        X_train_cv, X_test_cv = data_preparation(X_train_cv, X_test_cv, model_name)\n",
    "        regressor_ = model_preparation(regressor, model_name)\n",
    "\n",
    "        regressor_.fit(X_train_cv, y_train_cv)        \n",
    "        y_pred = regressor_.predict(X_test_cv)\n",
    "\n",
    "        score_eval = mean_squared_error(y_test_cv, y_pred, squared=False)\n",
    "        scores.append(score_eval)\n",
    "        \n",
    "        if isinstance(regressor_, TransformedTargetRegressor):\n",
    "            regressor_ = regressor_.regressor_\n",
    "\n",
    "        try:\n",
    "            feature_importance = pd.Series(regressor_.feature_importances_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "        except:\n",
    "            try:\n",
    "                feature_importance = pd.Series(regressor_.coef_, index=X_train_cv.columns, name=f'fold{i}')\n",
    "            except:\n",
    "                feature_importance = pd.Series(np.zeros(X_train_cv.shape[1]), index=X_train_cv.columns, name=f'fold{i}')\n",
    "        feature_importances = pd.concat([feature_importances, feature_importance], axis=1)\n",
    "    \n",
    "    feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "    \n",
    "    X_train_, X_test_ = data_preparation(X_train, X_test, model_name)\n",
    "    regressor_ = model_preparation(regressor, model_name)\n",
    "    regressor_.fit(X_train_, y_train)\n",
    "    y_pred = regressor_.predict(X_test_)\n",
    "    \n",
    "    if not SUBMIT:\n",
    "        score_eval = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    \n",
    "    print(f'{model_name}: {np.mean(scores):.4f} ± {np.std(scores):.4f}, Time: {time.time() - t0:.2f} seconds, RMSE: {score_eval:.4f}')\n",
    "    # print(feature_importances.sort_values('mean', ascending=False))\n",
    "set_config(transform_output=\"default\")  # reset to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBMRegressor4\n",
    "# 12.1279 ± 0.2388, Time: 0.54 seconds, RMSE: 12.2312 Benchmark\n",
    "# 12.1491 ± 0.2170, Time: 0.61 seconds, RMSE: 12.2304 Benchmark + PHONG NGUYEN features\n",
    "# 12.1461 ± 0.2530, Time: 0.75 seconds, RMSE: 12.2729 Benchmark + other features\n",
    "# 12.1461 ± 0.2530, Time: 0.54 seconds, RMSE: 12.2729 Benchmark + water_age_cement_ratio\n",
    "# 11.7352 ± 0.2269, Time: 0.56 seconds, RMSE: 11.9373 Benchmark + water_age_cement_ratio + origin data\n",
    "# 11.7387 ± 0.2246, Time: 0.63 seconds, RMSE: 11.9454 Benchmark + origin data\n",
    "# 11.7221 ± 0.2309, Time: 1.24 seconds, RMSE: 11.9058 Benchmark + water_age_cement_ratio + origin data + PolynomialFeatures\n",
    "# 11.7941 ± 0.2258, Time: 0.73 seconds, RMSE: 11.9935 Benchmark + water_age_cement_ratio + origin data + mean of duplicated rows\n",
    "# 11.7221 ± 0.2309, Time: 1.66 seconds, RMSE: 11.9058 Benchmark + water_age_cement_ratio + origin data + PolynomialFeatures + water_age_cement_ratio\n",
    "# 11.6205 ± 0.2677, Time: 3.17 seconds, RMSE: 12.3050 Benchmark + water_age_cement_ratio + origin data + PolynomialFeatures + water_age_cement_ratio + drop ratio with inf of div0 error ← BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Drop one feature at a time and evaluate\n",
    "# from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = LGBMRegressor(random_state=SEED, n_jobs=1, boosting_type='rf', subsample=.632, subsample_freq=1)\n",
    "X_train_prep, X_test_prep = data_preparation(X_train, X_test, 'LGBMRegressor4')\n",
    "\n",
    "# drop one feature at a time and evaluate\n",
    "cols = X_train.columns\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "sfs = SFS(model, \n",
    "          # k_features=(5,20),\n",
    "          forward=False, \n",
    "          floating=False, \n",
    "          scoring='neg_root_mean_squared_error',\n",
    "          cv=cv,\n",
    "          verbose=2,\n",
    "          n_jobs=-1,\n",
    "          )\n",
    "\n",
    "sfs = sfs.fit(X_train_prep, y_train)\n",
    "fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print('best combination (ACC: %.3f): %s\\n' % (sfs.k_score_, sfs.k_feature_idx_))\n",
    "print('all subsets:\\n', sfs.subsets_)\n",
    "plot_sfs(sfs.get_metric_dict(), kind='std_err')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sfs = sfs.transform(X_train)\n",
    "X_test_sfs = sfs.transform(X_test)\n",
    "\n",
    "model.fit(X_train_sfs, y_train)\n",
    "y_pred = model.predict(X_test_sfs)\n",
    "\n",
    "mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make cluster\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, OPTICS, Birch, MeanShift, SpectralClustering, AffinityPropagation\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Evaluation metrics for clustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep, X_test_prep = data_preparation(X_train, X_test, 'LinearRegression')\n",
    "\n",
    "y_train_preds = {}\n",
    "y_test_preds = {}\n",
    "scores = {}\n",
    "\n",
    "for n_clusters in range(3,21):\n",
    "    cluster_models = {\n",
    "        'KMeans': KMeans(n_clusters=n_clusters, random_state=SEED),\n",
    "        'Birch': Birch(n_clusters=n_clusters),\n",
    "        # 'MeanShift': MeanShift(n_jobs=-1),\n",
    "        # 'AffinityPropagation': AffinityPropagation(max_iter=1000, random_state=SEED),\n",
    "        'GaussianMixture': GaussianMixture(n_components=n_clusters, random_state=SEED),\n",
    "    }\n",
    "\n",
    "    for model_name, model in cluster_models.items():\n",
    "        name = f'{model_name}_{n_clusters}'\n",
    "        model.fit(X_train_prep)\n",
    "        y_pred = model.predict(X_train_prep)\n",
    "        y_train_preds[name] = y_pred\n",
    "        y_test_preds[name] = model.predict(X_test_prep)\n",
    "\n",
    "        if y_pred.max() != y_pred.min():\n",
    "            scores[name] = [silhouette_score(X_train_prep, y_pred, random_state=SEED), calinski_harabasz_score(X_train_prep, y_pred), davies_bouldin_score(X_train_prep, y_pred)]\n",
    "            print(f'{model_name} with {n_clusters} clusters: {scores[name]}')\n",
    "        else:\n",
    "            print('All data in one cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b19c57337e6f35fb2bbb187e6b167dc26fa5c786ecde0a595b1abdf014c95f55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
